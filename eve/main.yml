---
version: "0.2"

branches:
  # yamllint disable rule:line-length
  user/*, feature/*, improvement/*, bugfix/*, w/*, q/*, hotfix/*, dependabot/*, documentation/*, release/*:
    stage: pre-merge

models:
  # --- Re-usable steps (no parameters) ---
  - Git: &git_pull
      name: git pull
      repourl: "%(prop:git_reference)s"
      method: clobber
      retryFetch: true
      haltOnFailure: true
  - ShellCommand: &git_pull_terraform
      name: Git pull terraform snapshot
      command: >
        git clone --depth 1 "git@github.com:scality/terraform-snapshot.git"
        --branch "0.3.0"
      haltOnFailure: true
  - ShellCommand: &git_pull_ssh
      name: Git pull on bastion
      command: >
        ssh -F ssh_config bastion --
        git clone "https://github.com/scality/metalk8s" --branch "%(prop:branch)s"
      workdir: &terraform_workdir build/terraform-snapshot/terraform/
      haltOnFailure: true
  - ShellCommand: &yum_clean_all
      # Eve cache the workers, so time to time yum repositories definition are
      # really outdated, to avoid this, just do a `yum clean all` when starting a
      # worker
      name: Clean local yum cache
      command: sudo yum clean all && sudo rm -rf /var/cache/yum
      haltOnFailure: true
  - SetProperty: &set_premerge_url
      name: Set premerge artifacts private_url as Property
      property: premerge_artifacts_private_url
      value: "http://artifacts/builds/%(prop:premerge_artifacts_name)s"
  - ShellCommand: &wait_for_docker
      name: Wait for Docker daemon to be ready
      command: |
        bash -c '
        for i in {1..150}
        do
          docker info &> /dev/null && exit
          sleep 2
        done
        echo "Could not reach Docker daemon from buildbot worker" >&2
        exit 1'
      haltOnFailure: true
  - ShellCommand: &wait_pods_stable
      name: Wait for pods to stabilize
      env: &_env_wait_pods_stable
        RETRY: "60"
        SLEEP_TIME: "5"
        STABILIZATION_TIME: "30"
      command: >
        git checkout "%(prop:branch)s" &&
        sudo eve/wait_pods_stable.sh
        --sleep-time "$SLEEP_TIME" --stabilization-time "$STABILIZATION_TIME"
        --retry "$RETRY"
      usePTY: true
      haltOnFailure: true
  - ShellCommand: &wait_pods_stable_ssh
      name: Wait for pods to stabilize
      env: &_env_wait_pods_stable_ssh
        <<: *_env_wait_pods_stable
        SSH_CONFIG: ssh_config
        SSH_HOST: bootstrap
      command: >
        pushd "%(prop:builddir)s/build" && git checkout "%(prop:branch)s" && popd &&
        scp -F "$SSH_CONFIG" %(prop:builddir)s/build/eve/wait_pods_stable.sh "$SSH_HOST":/tmp/ &&
        ssh -F "$SSH_CONFIG" "$SSH_HOST" sudo /tmp/wait_pods_stable.sh
        --sleep-time "$SLEEP_TIME" --stabilization-time "$STABILIZATION_TIME"
        --retry "$RETRY"
      usePTY: true
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &build_all
      name: Build everything
      env:
        PYTHON_SYS: python3.6
      # There are 3 CPUs available for Docker, and 1 for `doit` in the Pod.
      # Given the network IO-bound nature of some of the build steps, and
      # most build steps running in Docker, set concurrency to 4.
      command: >
        source /etc/profile &&
        ./doit.sh -n 4 --verbosity 2 --failure-verbosity 2
      usePTY: true
      haltOnFailure: true
  - ShellCommand: &ssh_ip_setup
      name: Install SSH keys and report connection info
      command: |
        mkdir -p ~/.ssh
        echo "%(secret:ssh_pub_keys)s" >> ~/.ssh/authorized_keys
        IP=$(
          ip -f inet addr show eth0 | sed -En 's/^.*inet ([0-9.]+).*$/\1/p'
        )
        cat << END
        Connect to this worker using:
            ssh eve@$IP
        END
  - ShellCommand: &check_iso_checksum
      name: Check image with checksums
      command: |
        sha256sum -c SHA256SUM
        checkisomd5 metalk8s.iso
      haltOnFailure: true
  - Upload: &upload_artifacts
      name: Upload artifacts
      source: artifacts
      alwaysRun: true
  - Upload: &upload_report_artifacts
      name: Upload sosreport logs to artifacts
      source: sosreport
      alwaysRun: true
  - Upload: &upload_cypress_artifacts
      name: Upload Cypress and UI Junit folder
      source: ui-tests
      alwaysRun: true
  - ShellCommand: &collect_sosreport
      name: Collect logs using sosreport
      command: >
        sudo sosreport --all-logs
        -o metalk8s -kmetalk8s.podlogs=True
        -o containerd -kcontainerd.all=True -kcontainerd.logs=True
        --batch --tmp-dir /var/tmp &&
        sudo chown eve:eve /var/tmp/sosreport*
      alwaysRun: true
  - ShellCommand: &wait_debug
      name: Debug step - wait before allowing resource destruction
      env:
        # Use this step with STEP_NAME and DURATION environment values to
        # customize the command behaviour. DURATION default value is 3600
        # (wait duration, in seconds).
        STEP_NAME: default
      timeout: 3600
      command: |
        bash -c '
        DEBUG_STEPS="%(prop:debug)s"
        DURATION="${DURATION:-3600}"
        RUN_STEP=0
        REASON=""
        if [ -z "$DEBUG_STEPS" ]; then
          REASON="\"debug\" build property not set"
        elif [ "$DEBUG_STEPS" = all ]; then
          RUN_STEP=1
          REASON="\"debug\" property set to \"all\""
        elif [[ "$DEBUG_STEPS" =~ ^[a-z0-9\-]+(~[a-z0-9\-]+)*$ ]]; then
          IFS="~" read -ra SELECTED <<< "$DEBUG_STEPS"
          for selected in "${SELECTED[@]}"; do
            if [ "$selected" = "$STEP_NAME" ]; then
              RUN_STEP=1
              REASON="step selected in \"$DEBUG_STEPS\""
              break
            fi
          done
          if [ "$RUN_STEP" -eq 0 ]; then
            REASON="step not in \"$DEBUG_STEPS\""
          fi
        else
          REASON="invalid \"debug\" property value"
          cat >&2 << EOF
        Invalid "debug" build property value "$DEBUG_STEPS".
        Must use either:
          - "all", to select all debug steps
          - a single step name
          - a list of step names, separated by tilde signs "~",
            e.g. "single-node~multiple-nodes".
        EOF
        fi
        if [ "$RUN_STEP" -eq 1 ]; then
          echo "Step $STEP_NAME - wait $DURATION seconds"
          echo "Reason: $REASON"
          sleep "$DURATION"
        else
          echo "Step $STEP_NAME - skip debug"
          echo "Reason: $REASON"
        fi'
      alwaysRun: true

  # --- Re-usable steps (with parameters) ---
  - SetPropertyFromCommand: &set_version_prop
      name: Set version as property from built artifacts
      property: metalk8s_version
      env:
        BASE_URL: "%(prop:premerge_artifacts_private_url)s"
      command: >
        bash -c '
        . <(curl -s "${BASE_URL}/product.txt") &&
        echo $VERSION'
  - SetPropertyFromCommand: &set_short_version_prop
      name: Set short version as property from built artifacts
      property: metalk8s_short_version
      env:
        BASE_URL: "%(prop:premerge_artifacts_private_url)s"
      command: >
        bash -c '
          . <(curl -s "${BASE_URL}/product.txt") &&
          echo $SHORT_VERSION'
  - ShellCommand: &copy_artifacts
      name: Put the artifacts to upload in a separate directory
      env: &_env_copy_artifacts
        DEST_DIR: "artifacts"
        ARTIFACTS: >-
          _build/metalk8s.iso
          _build/SHA256SUM
          _build/root/product.txt
      command: |
        bash -c '
        mkdir "${DEST_DIR}" -p
        for artifact in ${ARTIFACTS}; do
          cp -r "$artifact" "${DEST_DIR}"
        done'
      haltOnFailure: true
      alwaysRun: true
  - ShellCommand: &copy_report_artifacts
      name: Put the sosreport logs to upload in a separate directory
      env:
        DEST_DIR: sosreport/sosreport
      command: mkdir -p "${DEST_DIR}" && cp /var/tmp/sosreport* "${DEST_DIR}"
      alwaysRun: true
  - ShellCommand: &retrieve_iso_checksum
      name: Retrieve ISO image checksum
      env: &_env_retrieve_artifact
        BASE_URL: "%(prop:premerge_artifacts_private_url)s"
        DEST_DIR: "."
      command: >
        curl -L -s -XGET -o "${DEST_DIR}/SHA256SUM" "${BASE_URL}/SHA256SUM"
      # Three minutes should be enough for this small file
      timeout: 180
  - ShellCommand: &retrieve_iso
      name: Retrieve ISO image
      env: &_env_retrieve_artifact_retry
        <<: *_env_retrieve_artifact
        MAX_ATTEMPTS: "300"  # retry every 2 seconds for 10 minutes total
        FILE_SOURCE: 'metalk8s.iso'
        FILE_DEST: ''
      command: |
        bash -c '
        in_url="${BASE_URL}/$FILE_SOURCE"
        out_path="${DEST_DIR}/${FILE_DEST:-$FILE_SOURCE}"
        for ((i=1;i<=MAX_ATTEMPTS;i++)); do
          if [ $(( $i % 10 )) -eq 1 ]; then
            echo "Attempt $i out of ${MAX_ATTEMPTS}"
          fi
          status_code=$(curl -L -s -XGET -o "${out_path}" "${in_url}" --write-out "%{http_code}")
          case "$status_code" in
              200)
                  exit
                  ;;
              404)
                  echo "Could not retrieve $FILE_SOURCE: $status_code" >&2
                  exit 1
                  ;;
          esac
          sleep 2
        done
        echo "Could not retrieve $FILE_SOURCE after $MAX_ATTEMPTS attempts" >&2
        exit 1'
      haltOnFailure: true
      # Increase default timeout for ISOs, as artifacts may be too slow
      timeout: 2400
  - ShellCommand: &copy_iso_bootstrap_ssh
      name: Copy archive to bootstrap node
      env: &_env_copy_iso_bootstrap_ssh
        ARCHIVE_DIRECTORY: "%(prop:builddir)s/build"
        ARCHIVE: "metalk8s.iso"
        DEST: ''
        NODE: bootstrap
      command: >
        scp -F ssh_config "$ARCHIVE_DIRECTORY/$ARCHIVE"
        $NODE:"$DEST"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &create_mountpoint
      name: Create mountpoint
      env:
        PRODUCT_VERSION: "%(prop:metalk8s_version)s"
      command: sudo mkdir -p "/srv/scality/metalk8s-${PRODUCT_VERSION}"
  - ShellCommand: &create_mountpoint_ssh
      name: Create mountpoint in bootstrap node
      command: >
        ssh -F ssh_config bootstrap
        sudo mkdir -p /var/tmp/metalk8s
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &mount_iso
      name: Mount ISO image
      env:
        ISO_PATH: metalk8s.iso
        PRODUCT_VERSION: "%(prop:metalk8s_version)s"
      command: >
        sudo mount -o loop "${ISO_PATH}"
        "/srv/scality/metalk8s-${PRODUCT_VERSION}"
  - ShellCommand: &mount_iso_ssh
      name: Mount ISO image in bootstrap node
      env: &_env_mount_iso_ssh
        ARCHIVE: metalk8s.iso
        MOUNTPOINT: /var/tmp/metalk8s
      command: >
        ssh -F ssh_config bootstrap
        sudo mount -o loop ${ARCHIVE} ${MOUNTPOINT}
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &set_bootstrap_minion_id_ssh
      name: Set the Salt minion ID on boostrap node
      env:
        MINION_ID: "bootstrap"
      command: |
        ssh -F ssh_config bootstrap "
        sudo bash << EOF
        mkdir -p /etc/salt
        echo \"$MINION_ID\" > /etc/salt/minion_id
        EOF"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &bootstrap_config
      name: Create bootstrap configuration file
      env:
        PRODUCT_VERSION: "%(prop:metalk8s_version)s"
        DEBUG: "%(prop:metalk8s_debug:-false)s"
      command: |
        sudo bash << EOF
        mkdir -p /etc/metalk8s
        cat > /etc/metalk8s/bootstrap.yaml << END
        apiVersion: metalk8s.scality.com/v1alpha2
        kind: BootstrapConfiguration
        networks:
          controlPlane: 10.100.0.0/16
          workloadPlane: 10.100.0.0/16
        ca:
          minion: "bootstrap"
        apiServer:
          host: $(ip route get 10.100.0.0 | awk '/10.100.0.0/{ print $6 }')
        archives:
          - "/srv/scality/metalk8s-${PRODUCT_VERSION}"
        debug: ${DEBUG}
        END
        EOF
      haltOnFailure: true
  - ShellCommand: &bootstrap_config_ssh
      name: Create bootstrap configuration file on bootstrap
      env: &_env_bootstrap_config_ssh
        DEBUG: "%(prop:metalk8s_debug:-false)s"
        ARCHIVE: metalk8s.iso
        COREDNS_ANTI_AFFINITY: "{}"
      command: |
        ssh -F ssh_config bootstrap "
        sudo bash << EOF
        mkdir -p /etc/metalk8s
        cat > /etc/metalk8s/bootstrap.yaml << END
        apiVersion: metalk8s.scality.com/v1alpha3
        kind: BootstrapConfiguration
        networks:
          controlPlane:
            cidr: 192.168.1.0/24
            metalLB:
              enabled: true
            ingress:
              ip: 192.168.1.254
          workloadPlane:
            cidr: 192.168.2.0/24
        ca:
          minion: \"bootstrap\"
        archives:
          - \"\$(readlink -f \"${ARCHIVE}\")\"
        debug: ${DEBUG}
        kubernetes:
          coreDNS:
            podAntiAffinity: ${COREDNS_ANTI_AFFINITY}
        END
        EOF"
      haltOnFailure: true
      workdir: *terraform_workdir
  - ShellCommand: &run_bootstrap
      name: Start the bootstrap process
      env:
        PRODUCT_VERSION: "%(prop:metalk8s_version)s"
      command: >
        sudo bash
        "/srv/scality/metalk8s-${PRODUCT_VERSION}/bootstrap.sh"
        --verbose
      haltOnFailure: true
  - ShellCommand: &run_bootstrap_ssh
      name: Start the bootstrap process in bootstrap node
      command: >
        ssh -F ssh_config bootstrap
        sudo bash
        /var/tmp/metalk8s/bootstrap.sh --verbose
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &provision_volumes_ssh
      name: Provision MetalK8s Volumes
      env: &_env_provision_volumes
        SSH_CONFIG: ssh_config
        HOST: bootstrap
        NODE_NAME: bootstrap
        PRODUCT_MOUNT: "/var/tmp/metalk8s"
        PRODUCT_TXT: "/var/tmp/metalk8s/product.txt"
      command: >
        scp -F "$SSH_CONFIG" "%(prop:builddir)s/build/eve/create-volumes.sh" "$HOST":/tmp/create-volumes.sh &&
        ssh -F "$SSH_CONFIG" "$HOST"
        sudo env
        PRODUCT_MOUNT="$PRODUCT_MOUNT"
        PRODUCT_TXT="$PRODUCT_TXT"
        NODE_NAME=$NODE_NAME
        /tmp/create-volumes.sh
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &add_archive
      name: Add ISO to cluster
      env: &_env_add_archive
        ISO_PATH: metalk8s.iso
        PRODUCT_VERSION: "%(prop:metalk8s_version)s"
      command: >
        sudo bash
        "/srv/scality/metalk8s-${PRODUCT_VERSION}/iso-manager.sh"
        --archive "$(readlink -f "${ISO_PATH}")" --verbose
      haltOnFailure: true
  - ShellCommand: &add_archive_ssh
      name: Add ISO to cluster from Bootstrap
      env: &_env_add_archive_ssh
        ISO_PATH: metalk8s.iso
        PRODUCT_VERSION: "%(prop:metalk8s_version)s"
      command: >
        ssh -F ssh_config bootstrap "
        sudo bash
        /srv/scality/metalk8s-${PRODUCT_VERSION}/iso-manager.sh
        --archive \"\$(readlink -f \"${ISO_PATH}\")\" --verbose"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &local_tests
      name: Run tests locally
      env: &_env_local_tests
        BRANCH: "%(prop:branch)s"
        ISO_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
        PYTEST_FILTERS: "post and ci and not multinodes"
      command: >
        git checkout "${BRANCH}" &&
        tox -e tests-local -- -m "${PYTEST_FILTERS}"
      haltOnFailure: true
  - ShellCommand: &bastion_tests
      name: Run tests on Bastion
      env: &_env_bastion_tests
        SSH_CONFIG_FILE: "/home/centos/ssh_config"
        ISO_MOUNTPOINT: "/var/tmp/metalk8s"
        TEST_HOSTS_LIST: "bootstrap"
        PYTEST_FILTERS: "post and ci"
        BOOTSTRAP_BACKUP_ARCHIVE: ""
        CONTROL_PLANE_INGRESS_VIP: "192.168.1.253"
      command: >
        ssh -F ssh_config bastion --
        "cd metalk8s &&
        export SSH_CONFIG_FILE=\"${SSH_CONFIG_FILE}\" &&
        export ISO_MOUNTPOINT=\"${ISO_MOUNTPOINT}\" &&
        export TEST_HOSTS_LIST=\"${TEST_HOSTS_LIST}\" &&
        export BOOTSTRAP_BACKUP_ARCHIVE=\"${BOOTSTRAP_BACKUP_ARCHIVE}\" &&
        export CONTROL_PLANE_INGRESS_VIP=\"${CONTROL_PLANE_INGRESS_VIP}\" &&
        tox -e tests -- ${PYTEST_ARGS:-""} -m \"${PYTEST_FILTERS}\""
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &bastion_fast_tests
      <<: *bastion_tests
      name: Run fast tests on Bastion
      env: &_env_bastion_fast_tests
        <<: *_env_bastion_tests
        PYTEST_FILTERS: "post and ci and not multinode and not slow and not registry_ha"
  - SetPropertyFromCommand: &set_bootstrap_cp_ip_ssh
      name: Set the bootstrap node control plane IP as a property
      property: bootstrap_control_plane_ip
      command: >
        ssh -F ssh_config bootstrap "
            sudo salt-call grains.get metalk8s:control_plane_ip --out txt |
            sed -rn 's/^local: (.*)$/\1/p'
        "
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &install_cypress_ssh
      name: Install Cypress on Bastion
      command: >
        ssh -F ssh_config bastion --
        "cd metalk8s &&
        bash ui/cypress/requirements.sh"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &bastion_ui_tests
      name: Run UI tests on Bastion
      env: &_env_bastion_ui_tests
        TEST_FILTER: "e2e"
        TARGET_URL: "https://192.168.1.254:8443"
      command: >
        ssh -F ssh_config bastion --
        "cd metalk8s/ui &&
        CYPRESS_BASE_URL=$TARGET_URL npm run test:$TEST_FILTER"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &add_final_status_artifact
      name: Add final status to artifacts
      command: |-
        bash -c '
          declare BUILD_STATUS_DIR=build_status
          [[ ${STEP_NAME:-} ]] && BUILD_STATUS_DIR+="/build_status/$STEP_NAME"
          mkdir -p "$BUILD_STATUS_DIR"
          echo -n "$FINAL_STATUS" > "$BUILD_STATUS_DIR/.final_status"
          TEST_NAME="${TEST_NAME:-$STEP_NAME}"
          if [[ ${TEST_NAME:-} && ${TEST_SUITE:-} ]]; then
              git checkout "%(prop:branch)s" && \
              eve/generate_junit_result.sh \
                  > "$BUILD_STATUS_DIR/junit_status.xml"
          fi
        '
      env: &_env_final_status_artifact
        STEP_NAME: ''
        FINAL_STATUS: ''
        TEXT: |-
          Build Url: %(prop:buildurl)s
          Artifact Url: %(prop:artifacts_public_url)s
          Branch: %(prop:branch)s
          Commit: %(prop:revision)s
      haltOnFailure: true
  - ShellCommand: &add_final_status_artifact_success
      <<: *add_final_status_artifact
      name: Add successful status to artifacts
      env: &_env_final_status_artifact_success
        <<: *_env_final_status_artifact
        FINAL_STATUS: "SUCCESSFUL"
  - ShellCommand: &add_final_status_artifact_failed
      <<: *add_final_status_artifact
      name: Add failed status to artifacts
      env: &_env_final_status_artifact_failed
        <<: *_env_final_status_artifact
        FINAL_STATUS: "FAILED"
  - Upload: &upload_final_status_artifact
      name: Upload final status to artifacts
      source: build_status
      alwaysRun: true
  - ShellCommand: &image_registry_login
      name: Login to Harbor image registry
      command: |
        if [ "$USE_PRODUCTION_REGISTRY" = true ]; then
            HARBOR_LOGIN='%(secret:harbor_prod_login)s'
            HARBOR_PASSWORD='%(secret:harbor_prod_token)s'
        else
            HARBOR_LOGIN='%(secret:harbor_dev_login)s'
            HARBOR_PASSWORD='%(secret:harbor_dev_token)s'
        fi
        docker login --username "$HARBOR_LOGIN" --password "$HARBOR_PASSWORD" "$REGISTRY_HOST"
      env: &_env_image_registry
        USE_PRODUCTION_REGISTRY: "%(prop:use_production_registry:-false)s"
        REGISTRY_HOST: registry.scality.com
      haltOnFailure: true

  # --- Re-usable steps related to terraform actions ---
  - ShellCommand: &terraform_install
      name: Install terraform and requirements
      command: bash ../requirements.sh
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &terraform_install_check
      name: Check that terraform was installed
      command: |-
        if ! terraform --version 2&> /dev/null; then
          echo "aborting - terraform not installed and required" >&2
          exit 1
        fi
      haltOnFailure: true
  - ShellCommand: &terraform_init
      name: Init terraform
      command: |-
        for try in $(seq 1 $MAX_RETRIES); do
          if terraform init; then
            break
          elif [ $try -lt $MAX_RETRIES ]; then
            rm -rf .terraform/
            sleep 5
          else
            echo "Error: unable to initialize terraform after" \
                 "$MAX_RETRIES tries" >&2
            exit 1
          fi
        done
      env:
        MAX_RETRIES: "12"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &terraform_validate
      name: Validate terraform definition
      command: terraform validate
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &terraform_apply
      name: Spawn openstack virtual infra
      command: |-
        for try in $(seq 1 $MAX_RETRIES); do
          if terraform apply -auto-approve; then
            break
          elif [ $try -lt $MAX_RETRIES ]; then
            sleep 300
          else
            echo "Error: unable to spawn openstack virtual infra after" \
                 "$MAX_RETRIES tries" >&2
            exit 1
          fi
        done
      env: &_env_terraform
        OS_AUTH_URL: "%(secret:scality_cloud_auth_url)s"
        OS_REGION_NAME: "%(secret:scality_cloud_region)s"
        OS_USERNAME: "%(secret:scality_cloud_username)s"
        OS_PASSWORD: "%(secret:scality_cloud_password)s"
        OS_TENANT_NAME: "%(secret:scality_cloud_tenant_name)s"
        OS_PROJECT_DOMAIN_ID: "default"
        OS_USER_DOMAIN_ID: "default"
        TF_VAR_prefix: "%(prop:buildnumber)s-%(prop:stage_name)s"
        TF_VAR_os: "%(prop:os:-centos-7)s"
        TF_VAR_nodes_count: "%(prop:nodes_count:-0)s"
        TF_VAR_rhsm_username: "%(secret:rhel_ci_login)s"
        TF_VAR_rhsm_password: "%(secret:rhel_ci_password)s"
        TF_VAR_debug: "%(prop:metalk8s_debug:-false)s"
        TF_VAR_offline: "%(prop:offline:-true)s"
        TF_VAR_use_proxy: "%(prop:use_proxy:-true)s"
        MAX_RETRIES: "3"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &terraform_gen_snapshot
      name: Generate snapshot from terraform environment
      doStepIf: "%(prop:generate_snapshot:-false)s"
      env:
        <<: *_env_terraform
        SNAPSHOT_NAME: "metalk8s-%(prop:metalk8s_version)s-%(prop:os:-centos-7)s-%(prop:environment_type)s-%(prop:environment_name)s"
      command: ../generate_snapshot.sh
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &terraform_destroy
      name: Destroy openstack virtual infra
      command: |-
        for try in $(seq 1 $MAX_RETRIES); do
          if terraform destroy -auto-approve; then
            break
          elif [ $try -lt $MAX_RETRIES ]; then
            sleep 300
          else
            echo "Error: unable to destroy openstack virtual infra after" \
                 "$MAX_RETRIES tries" >&2
            exit 1
          fi
        done
      env: *_env_terraform
      workdir: *terraform_workdir
      alwaysRun: true
      sigtermTime: 600

  # --- Solutions management ---
  - ShellCommand: &retrieve_iso_solution
      # NOTE: Retrieve solution from property
      # "<solution_base_url>/<solution_archive>"
      # OR if solution_base_url not set
      # "<solution_repo>/<solution_version>/<solution_archive>"
      # default to:
      # "https://github.com/scality/metalk8s-solution-example/releases/download/1.0.0/example-solution-1.0.2.iso"
      <<: *retrieve_iso
      doStepIf: "%(prop:install_solution:-false)s"
      name: Retrieve Solution archive
      env:
        <<: *_env_retrieve_artifact_retry
        BASE_URL: >-
          %(prop:solution_base_url:-%(prop:solution_repo:-https://github.com/scality/metalk8s-solution-example/releases/download)s/%(prop:solution_version:-1.0.2)s)s
        FILE_SOURCE: "%(prop:solution_archive:-example-solution-1.0.2.iso)s"
        FILE_DEST: "%(prop:solution_archive:-example-solution-1.0.2.iso)s"
  - ShellCommand: &copy_solution_archive_bootstrap_ssh
      doStepIf: "%(prop:install_solution:-false)s"
      name: Copy Solution archive to bootstrap
      <<: *copy_iso_bootstrap_ssh
      env: &_env_copy_solution_archive_bootstrap_ssh
        <<: *_env_copy_iso_bootstrap_ssh
        ARCHIVE: "%(prop:solution_archive:-example-solution-1.0.2.iso)s"
  - ShellCommand: &import_solution
      doStepIf: "%(prop:install_solution:-false)s"
      name: Import Solution archive
      env:
        SOLUTION_ARCHIVE: "%(prop:solution_archive:-example-solution-1.0.2.iso)s"
        METALK8S_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
      command: >
        ssh -F ssh_config bootstrap
        sudo "$METALK8S_MOUNTPOINT/solutions.sh"
        import --archive "\$(readlink -f \"$SOLUTION_ARCHIVE\")"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &activate_solution
      doStepIf: "%(prop:install_solution:-false)s"
      name: Activate Solution
      env:
        SOLUTION_NAME: "%(prop:solution_name:-example-solution)s"
        SOLUTION_VERSION: "%(prop:solution_version:-1.0.2)s"
        METALK8S_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
      command: >
        ssh -F ssh_config bootstrap
        sudo "$METALK8S_MOUNTPOINT/solutions.sh"
        activate --name "$SOLUTION_NAME" --version "$SOLUTION_VERSION"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &create_solution_env
      doStepIf: "%(prop:install_solution:-false)s"
      name: Create Solution environment
      env:
        ENVIRONMENT_NAME: "%(prop:solution_env_name:-example-environment)s"
        METALK8S_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
      command: >
        ssh -F ssh_config bootstrap
        sudo "$METALK8S_MOUNTPOINT/solutions.sh"
        create-env --name "$ENVIRONMENT_NAME"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &untaint_bootstrap_ssh
      doStepIf: "%(prop:install_solution:-false)s"
      name: Untaint Bootstrap before deploying a Solution
      env:
        NODE_NAME: "bootstrap"
      command: |
        ssh -F ssh_config bootstrap <<ENDSSH
        sudo kubectl --kubeconfig /etc/kubernetes/admin.conf \
        patch node "$NODE_NAME" --patch '{
            "metadata": {"labels": {"node-role.kubernetes.io/node": ""}},
            "spec": {"taints": []}
        }'
        ENDSSH
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &deploy_solution
      doStepIf: "%(prop:install_solution:-false)s"
      name: Deploy Solution
      env:
        ENVIRONMENT_NAME: "%(prop:solution_env_name:-example-environment)s"
        SOLUTION_NAME: "%(prop:solution_name:-example-solution)s"
        SOLUTION_VERSION: "%(prop:solution_version:-1.0.2)s"
        METALK8S_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
      command: >
        ssh -F ssh_config bootstrap
        sudo "$METALK8S_MOUNTPOINT/solutions.sh"
        add-solution --name "$ENVIRONMENT_NAME"
        --solution "$SOLUTION_NAME" --version "$SOLUTION_VERSION"
      workdir: *terraform_workdir
      haltOnFailure: true
  - ShellCommand: &wait_for_solution_operator
      doStepIf: "%(prop:install_solution:-false)s"
      name: Wait for Solution Operator to become ready
      env:
        ENVIRONMENT_NAME: "%(prop:solution_env_name:-example-environment)s"
        SOLUTION_NAME: "%(prop:solution_name:-example-solution)s"
      command: >
        ssh -F ssh_config bootstrap
        sudo kubectl --kubeconfig /etc/kubernetes/admin.conf
        wait pods --namespace "$ENVIRONMENT_NAME"
        --selector "app.kubernetes.io/name=$SOLUTION_NAME-operator"
        --for condition=Ready
      workdir: *terraform_workdir
      haltOnFailure: true

  # --- Previous version (for Upgrade/Downgrade) ---
  - Git: &git_pull_prev
      name: clone previous version branch
      command: >
          git clone "%(prop:repository)s"
          --branch "development/%(prop:product_version_prev)s"
          metalk8s-"%(prop:product_version_prev)s"
      haltOnFailure: true

  - ShellCommand: &generate_report_over_ssh
      name: Generate sosreport on every machine
      env: &_env_generate_report_over_ssh
        # NOTE: If HOSTS_LIST is empty auto detect nodes number from
        # nodes_count property
        HOSTS_LIST: ""
        SSH_CONFIG: ssh_config
        # NOTE: If REPORT_OWNER or REPORT_GROUP is empty auto detect from
        # os property (which default to centos-7)
        REPORT_OWNER: ""
        REPORT_GROUP: ""
      command: |-
        HOSTS_LIST=${HOSTS_LIST:-bootstrap$(seq -s '' --format ' node-%%g' 1 %(prop:nodes_count:-0)s)}
        case "%(prop:os:-centos-7)s" in
          centos-*) REPORT_OWNER=${REPORT_OWNER:-centos} REPORT_GROUP=${REPORT_GROUP:-centos};;
          rhel-*) REPORT_OWNER=${REPORT_OWNER:-cloud-user} REPORT_GROUP=${REPORT_GROUP:-cloud-user};;
        esac
        for host in $HOSTS_LIST; do
          ssh -F "$SSH_CONFIG" $host \
          "sudo sosreport --all-logs -o metalk8s -kmetalk8s.podlogs=True\
          -o containerd -kcontainerd.all=True -kcontainerd.logs=True\
          --batch --tmp-dir /var/tmp && \
          sudo chown '$REPORT_OWNER:$REPORT_GROUP' /var/tmp/sosreport*"
        done
      workdir: *terraform_workdir
      alwaysRun: true
  - ShellCommand: &collect_report_over_ssh
      name: Download every sosreports on worker
      env: &_env_collect_report_over_ssh
        # NOTE: If HOSTS_LIST is empty auto detect nodes number from
        # nodes_count property
        HOSTS_LIST: ""
        SSH_CONFIG: ssh_config
        DEST_DIR: "%(prop:builddir)s/build/sosreport/sosreport"
        STEP_NAME: ''
      command: >
        HOSTS_LIST=${HOSTS_LIST:-bootstrap$(seq -s '' --format ' node-%%g' 1 %(prop:nodes_count:-0)s)};
        mkdir -p "$DEST_DIR/$STEP_NAME" &&
        for host in $HOSTS_LIST; do
          scp -F "$SSH_CONFIG" \
          "$host:/var/tmp/sosreport-*.tar.xz" \
          "$DEST_DIR/$STEP_NAME/$host-sosreport.tar.xz"
        done
      workdir: *terraform_workdir
      alwaysRun: true
  - ShellCommand: &collect_cypress_result_ssh
      name: Download Cypress result from Bastion
      env:
        SSH_HOST: bastion
        SSH_CONFIG: ssh_config
        DEST_DIR: "%(prop:builddir)s/build/ui-tests"
      command: |-
        mkdir -p "$DEST_DIR/cypress"
        scp -r -F "$SSH_CONFIG" "$SSH_HOST":metalk8s/ui/cypress/screenshots "$DEST_DIR/cypress/"
        scp -r -F "$SSH_CONFIG" "$SSH_HOST":metalk8s/ui/cypress/videos "$DEST_DIR/cypress/"
        scp -r -F "$SSH_CONFIG" "$SSH_HOST":metalk8s/ui/junit "$DEST_DIR"
      workdir: *terraform_workdir
      alwaysRun: true

stages:
  pre-merge:
    worker:
      type: local
    steps:
      - ShellCommand: *add_final_status_artifact_failed
      - SetProperty: &set_premerge_artifacts_name
          # Set premerge_artifacts_name to current artifacts name
          # as we are in premerge so we can use same step to retrieve ISO
          # in pre-merge and post-merge
          name: Set premerge artifacts name to current artifacts_name
          property: premerge_artifacts_name
          value: "%(prop:artifacts_name)s"
      - SetProperty: *set_premerge_url
      - TriggerStages:
          name: Trigger build, docs, test and lint stages simultaneously
          stage_names:
            - build
            - build-shell-ui
            - docs
            - lint
            - unit_tests
            - integration_tests_ui
          haltOnFailure: true
      - SetPropertyFromCommand: *set_version_prop
      - SetPropertyFromCommand: *set_short_version_prop
      - TriggerStages:
          name: Trigger single and multiple nodes install stages with built ISO
          stage_names:
            - single-node-install-rhel
            - multiple-nodes
      - ShellCommand: *add_final_status_artifact_success
      - Upload: *upload_final_status_artifact

  post-merge:
    # Only let 2 post merge run simultaneously
    simultaneous_builds: 2
    worker:
      type: local
    steps:
      - ShellCommand: *add_final_status_artifact_failed
      - GetArtifactsFromStage: &get_premerge_name
          name: Get pre-merge artifacts_name
          stage: pre-merge
          property: premerge_artifacts_name
          haltOnFailure: true
      - SetProperty: *set_premerge_url
      - ShellCommand:
          name: Save the pre-merge artifacts reference
          command: >
            mkdir -p build_status/.related_artifacts
            && touch
            "build_status/.related_artifacts/%(prop:premerge_artifacts_name)s"
          haltOnFailure: true
      - SetPropertyFromCommand: *set_version_prop
      - SetPropertyFromCommand: *set_short_version_prop
      - TriggerStages:
          name: Trigger post-merge lifecycle, solutions and restore stages simultaneously
          stage_names:
            - post-merge-solutions
            - lifecycle-dev-branch
            - lifecycle-patch-version
            - lifecycle-minor-version
            - bootstrap-restore
          haltOnFailure: true
      - TriggerStages:
          name: Trigger publish stage
          stage_names:
            - publish
          haltOnFailure: true
          doStepIf: "%(prop:publish_images:-false)s"
      - ShellCommand: *add_final_status_artifact_success
      - Upload: *upload_final_status_artifact
      - TriggerStages:
          name: Trigger TestRail objects creation and results upload
          stage_names:
            - create-upload-testrail-objects
          alwaysRun: true

  post-merge-solutions:
    worker:
      type: local
    steps:
      - SetProperty:
          name: Set Example Solution version property
          property: example_solution_version
          value: 1.0.2
      - SetProperty:
          name: Set Example Solution version next property
          property: example_solution_version_next
          value: 1.0.3
      - TriggerStages:
          name: Trigger Solutions framework tests
          stage_names:
            - single-node-solutions
          haltOnFailure: true

  lifecycle-dev-branch:
    worker:
      type: local
    steps:
      - SetPropertyFromCommand:
          name: Set previous version to upgrade from and downgrade to
          property: product_version_prev
          command: >
              major=$(echo "%(prop:product_version)s" | cut -d'.' -f1) &&
              minor=$(echo "%(prop:product_version)s" | cut -d'.' -f2) &&
              echo "$major.$(( $minor-1 ))"
      - TriggerStages:
          name: Trigger build of previous version
          stage_names:
            - buildprev
          haltOnFailure: true
      - SetPropertyFromCommand:
          <<: *set_version_prop
          name: Set previous version as property from built artifacts
          property: metalk8s_version_prev
          env:
            BASE_URL: "%(prop:artifacts_private_url)s/pre"
      - TriggerStages:
          name: Trigger upgrade and downgrade test stages simultaneously
          stage_names:
            - single-node-upgrade-centos
            # NOTE: Deactivate minor downgrade on this branch
            # See: https://github.com/scality/metalk8s/issues/1750
            #- single-node-downgrade-centos
          waitForFinish: true

  create-upload-testrail-objects:
    worker:
      type: kube_pod
      path: eve/workers/pod-basic/pod.yaml
      images:
        docker-basic: eve/workers/docker-centos7
      vars:
        name: "metalk8s-test-tools-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          name: Install test tools requirements
          command: >
            git clone git@github.com:scality/test_tools.git &&
            sudo yum install -y epel-release &&
            sudo yum install -y wget python3.6 python3-pip &&
            pip3 install --user -r test_tools/testrail/requirements.txt
      - ShellCommand:
          name: Create TestRail objects
          env: &env_testrail
            TESTRAIL_LOGIN: "%(secret:testrail_login)s"
            TESTRAIL_KEY: "%(secret:testrail_key)s"
            TESTRAIL_PROJECT: "MetalK8s"
            TESTRAIL_MILESTONE: "%(prop:metalk8s_short_version)s"
            TESTRAIL_SUITE: "%(prop:metalk8s_short_version)s"
            TESTRAIL_PLAN: "%(prop:metalk8s_version)s"
            DESCRIPTION_FILE: "eve/testrail_description_file.yaml"
          command: >
            python3 test_tools/testrail/testrail_create.py
            --user "$TESTRAIL_LOGIN"
            --password "$TESTRAIL_KEY"
            --project "$TESTRAIL_PROJECT"
            --milestone "$TESTRAIL_MILESTONE"
            --test-suite "$TESTRAIL_SUITE"
            --test-plan "$TESTRAIL_PLAN"
            "$DESCRIPTION_FILE"
          haltOnFailure: false
      - ShellCommand:
          name: Upload TestRail results
          env:
            <<: *env_testrail
            BASE_URL: "%(prop:artifacts_private_url)s/build_status/"
            PATTERN: "*.xml"
          # This step will upload TestRail result from all xml in
          # `build_status` directory
          # These xml should be JUnit file
          # NOTE: `--tolerate` allow to continue even if one XML upload failed
          #       BUT script will exit with 1 if at least one XML file failed
          command: >
            TEMPDIR="$(mktemp -d)" &&
            wget --tries=10 --recursive --level=10 --no-parent
            --directory-prefix="$TEMPDIR" --accept="$PATTERN"
            "$BASE_URL" &&
            find "$TEMPDIR" -type f -name "$PATTERN" -exec
            python3 test_tools/testrail/testrail_upload_result.py
            --user "$TESTRAIL_LOGIN"
            --password "$TESTRAIL_KEY"
            --project "$TESTRAIL_PROJECT"
            --test-suite "$TESTRAIL_SUITE"
            --test-plan "$TESTRAIL_PLAN"
            --tolerate
            '{}' +

  build:
    _metalk8s_internal_info:
      junit_info: &_build_metalk8s_junit_info
        TEST_SUITE: build
        TEST_NAME: metalk8s
    worker: &build_worker
      type: kube_pod
      path: eve/workers/pod-builder/pod.yaml
      images:
        docker-builder: eve/workers/pod-builder
    steps:
      - ShellCommand: *wait_for_docker
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_build_metalk8s_junit_info
            STEP_NAME: build
      - ShellCommand: *yum_clean_all
      - ShellCommand: *build_all
      - ShellCommand:
          <<: *copy_artifacts
          env:
            <<: *_env_copy_artifacts
            ARTIFACTS: >-
              build.log
              _build/metalk8s.iso
              _build/SHA256SUM
              _build/root/product.txt
      - Upload: *upload_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_build_metalk8s_junit_info
            STEP_NAME: build
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          name: Cleanup build tree
          env:
            PYTHON_SYS: python3.6
          command: ./doit.sh clean && test ! -d _build
          usePTY: true
          haltOnFailure: true

  buildprev:
    _metalk8s_internal_info:
      junit_info: &_build_metalk8s-previous_junit_info
        TEST_SUITE: build
        TEST_NAME: metalk8s previous
    worker: *build_worker
    steps:
      - ShellCommand: *wait_for_docker
      - ShellCommand: *yum_clean_all
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_build_metalk8s-previous_junit_info
            STEP_NAME: buildprev
      - ShellCommand: *git_pull_prev
      - ShellCommand:
          <<: *build_all
          workdir: "build/metalk8s-%(prop:product_version_prev)s"
      - ShellCommand:
          <<: *copy_artifacts
          workdir: "build/metalk8s-%(prop:product_version_prev)s"
          env:
            <<: *_env_copy_artifacts
            DEST_DIR: "../artifacts/pre"
      - Upload: *upload_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_build_metalk8s-previous_junit_info
            STEP_NAME: buildprev
      - Upload: *upload_final_status_artifact

  docs:
    _metalk8s_internal_info:
      junit_info: &_build_docs_junit_info
        TEST_SUITE: build
        TEST_NAME: docs
    worker:
      type: kube_pod
      path: eve/workers/pod-docs-builder/pod.yaml
      images:
        doc-builder:
          context: '.'
          dockerfile: docs/Dockerfile
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_build_docs_junit_info
            STEP_NAME: docs
      - ShellCommand:
          name: Build documentation for ReadTheDocs
          env:
            # Fake that we are building in a ReadTheDocs environment
            READTHEDOCS: 'True'
          command: tox --workdir /tmp/tox -e docs -- html
          haltOnFailure: true
      - ShellCommand:
          <<: *copy_artifacts
          name: Copy generated docs for ReadTheDocs
          env:
            <<: *_env_copy_artifacts
            DEST_DIR: "artifacts/docs/readthedocs"
            ARTIFACTS: >-
              docs/_build/*
      - ShellCommand:
          name: Build documentation with Scality theme
          command: rm -rf docs/_build && ./docs/entrypoint.sh html
          env:
            BUILD_DIR: "%(prop:builddir)s/build/docs/_build"
          haltOnFailure: true
      - ShellCommand:
          <<: *copy_artifacts
          name: Copy generated docs with Scality theme
          env:
            <<: *_env_copy_artifacts
            DEST_DIR: "artifacts/docs"
            ARTIFACTS: >-
              docs/_build/*
              CHANGELOG.md
      - Upload:
          <<: *upload_artifacts
          urls:
            - docs/html/index.html
            - docs/CHANGELOG.md
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_build_docs_junit_info
            STEP_NAME: docs
      - Upload: *upload_final_status_artifact

  lint:
    _metalk8s_internal_info:
      junit_info: &_lint_junit_info
        TEST_SUITE: lint
        # This one should be split in different test case for each linting
        TEST_NAME: full
    worker:
      type: kube_pod
      path: eve/workers/pod-linter/pod.yaml
      images:
        docker-linter:
          context: 'storage-operator'
          dockerfile: eve/workers/pod-linter/Dockerfile
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_lint_junit_info
            STEP_NAME: lint
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Run all linting targets
          command: source /etc/profile && ./doit.sh lint
          usePTY: true
          haltOnFailure: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_lint_junit_info
            STEP_NAME: lint
      - Upload: *upload_final_status_artifact

  integration_tests_ui:
    _metalk8s_internal_info:
      junit_info: &_integration_tests_ui_junit_info
        TEST_SUITE: integration tests
        TEST_NAME: ui
    worker:
      type: kube_pod
      path: eve/workers/pod-integration-tests/ui/pod.yaml
      images:
        worker:
          context: 'ui'
          dockerfile: eve/workers/pod-integration-tests/ui/worker.Dockerfile
        application:
          context: 'ui'
          dockerfile: eve/workers/pod-integration-tests/ui/app.Dockerfile
        shell-ui:
          context: 'shell-ui'
          dockerfile: eve/workers/pod-integration-tests/ui/shell-ui.Dockerfile
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_integration_tests_ui_junit_info
            STEP_NAME: integration_tests_ui
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Install Cypress and its dependencies
          workdir: build/ui
          command: |
            PKGS="har-validator cypress cypress-cucumber-preprocessor cypress-wait-until @testing-library/cypress"
            for pkg in $PKGS; do
              npm install --no-save --no-package-lock $pkg@$(node -p \
                -e "require('./package-lock.json').dependencies['$pkg'].version" \
              ) || exit 1
            done
          haltOnFailure: true
      - ShellCommand:
          name: Wait for application to be available
          command: |
            bash -c '
              attempts=0
              until curl -Isfo /dev/null http://localhost:80/; do
                (( attempts++ ))
                if [ $attempts -gt 100 ]; then
                  >&2 echo "Failed to reach application after 5 minutes"
                  exit 1
                fi
                sleep 3
              done
            '
          haltOnFailure: true
      - ShellCommand:
          name: Run all UI integration tests
          workdir: build/ui
          env:
            CYPRESS_BASE_URL: http://localhost:80
          command: >
            npm run test:integration --no-update-notifier
          haltOnFailure: true
      - ShellCommand:
          name: Prepare upload folder
          command: |
            mkdir -p upload/ui/cypress
            mv junit upload/ui
            mv cypress/screenshots upload/ui/cypress/screenshots
            mv cypress/videos upload/ui/cypress/videos
          workdir: build/ui
          alwaysRun: true
      - Upload:
          name: Upload Cypress and JUnit artifacts
          source: ui/upload
          urls:
            - ui/junit/*.xml
            - ui/cypress/videos/*
          alwaysRun: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_integration_tests_ui_junit_info
            STEP_NAME: integration_tests_ui
      - Upload: *upload_final_status_artifact

  unit_tests:
    worker:
      type: local
    steps:
      - TriggerStages:
          name: Trigger all unit tests simultaneously
          stage_names:
            - unit_tests_ui
            - unit_tests_shell_ui
            - unit_tests_crd_client_generator
            - unit_tests_storage_operator
            - unit_tests_salt
            - unit_tests_lib_alert_tree

  unit_tests_ui:
    _metalk8s_internal_info:
      junit_info: &_unit-test_ui_junit_info
        TEST_SUITE: unit test
        TEST_NAME: ui
    worker:
      type: kube_pod
      path: eve/workers/pod-unit-tests/pod.yaml
      images:
        docker-unit-tests:
          context: 'ui'
          dockerfile: eve/workers/pod-unit-tests/ui/Dockerfile
      vars:
        name: "metalk8s-unit-tests-ui-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_unit-test_ui_junit_info
            STEP_NAME: unit_tests_ui
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Install UI dependencies
          workdir: build/ui
          command: >
            npm ci --prefer-offline --no-audit
          haltOnFailure: true
      - ShellCommand:
          name: Run all UI unit tests
          workdir: build/ui
          command: >
            npm run test:nowatch --no-update-notifier
          haltOnFailure: true
      - ShellCommand:
          name: Prepare upload folder
          command: |
            mkdir -p upload/ui
            mv junit upload/ui
          workdir: build/ui
          alwaysRun: true
      - Upload:
          name: Upload Cypress and JUnit artifacts
          source: ui/upload
          urls:
            - ui/junit/*.xml
          alwaysRun: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_unit-test_ui_junit_info
            STEP_NAME: unit_tests_ui
      - Upload: *upload_final_status_artifact

  unit_tests_shell_ui:
    _metalk8s_internal_info:
      junit_info: &_unit-test_shell_ui_junit_info
        TEST_SUITE: unit test
        TEST_NAME: shell-ui
    worker:
      type: kube_pod
      path: eve/workers/pod-unit-tests/pod.yaml
      images:
        docker-unit-tests:
          context: 'shell-ui'
          dockerfile: eve/workers/pod-unit-tests/ui/Dockerfile
      vars:
        name: "metalk8s-unit-tests-shell-ui-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_unit-test_shell_ui_junit_info
            STEP_NAME: unit_tests_shell_ui
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Install UI dependencies
          workdir: build/shell-ui
          command: >
            npm ci --prefer-offline --no-audit
          haltOnFailure: true
      - ShellCommand:
          name: Run all Shell UI unit tests
          workdir: build/shell-ui
          command: >
            npm run test --no-update-notifier
          haltOnFailure: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_unit-test_shell_ui_junit_info
            STEP_NAME: unit_tests_sehll_ui
      - Upload: *upload_final_status_artifact

  unit_tests_crd_client_generator:
    _metalk8s_internal_info:
      junit_info: &_unit-test_crd_client_generator_junit_info
        TEST_SUITE: unit test
        TEST_NAME: crd client generator
    worker:
      type: kube_pod
      path: eve/workers/pod-unit-tests/pod.yaml
      images:
        docker-unit-tests:
          context: 'tools/crd-client-generator-js'
          dockerfile: eve/workers/pod-unit-tests/ui/Dockerfile
      vars:
        name: "metalk8s-unit-tests-crd-client-generator-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_unit-test_crd_client_generator_junit_info
            STEP_NAME: unit_tests_crd_client_generator
      - ShellCommand:
          name: Install CRD client generator dependencies
          workdir: build/tools/crd-client-generator-js
          command: >
            npm ci --prefer-offline --no-audit
          haltOnFailure: true
      - ShellCommand:
          name: Run all CRD client generator unit tests
          workdir: build/tools/crd-client-generator-js
          command: >
            npm run test --no-update-notifier
          haltOnFailure: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_unit-test_crd_client_generator_junit_info
            STEP_NAME: unit_tests_crd_client_generator
      - Upload: *upload_final_status_artifact

  unit_tests_storage_operator:
    _metalk8s_internal_info:
      junit_info: &_unit-test_storage_operator_junit_info
        TEST_SUITE: unit test
        TEST_NAME: storage operator
    worker:
      type: kube_pod
      path: eve/workers/pod-unit-tests/pod.yaml
      images:
        docker-unit-tests:
          context: 'storage-operator'
          dockerfile: eve/workers/pod-unit-tests/storage-operator/Dockerfile
      vars:
        name: "metalk8s-unit-tests-storage-operator-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_unit-test_storage_operator_junit_info
            STEP_NAME: unit_tests_storage_operator
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Run all storage-operator unit tests
          workdir: build/storage-operator
          command: go test -cover -v ./...
          haltOnFailure: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_unit-test_storage_operator_junit_info
            STEP_NAME: unit_tests_storage_operator
      - Upload: *upload_final_status_artifact

  unit_tests_salt:
    _metalk8s_internal_info:
      junit_info: &_unit-test_salt_junit_info
        TEST_SUITE: unit test
        TEST_NAME: salt
    worker:
      type: kube_pod
      path: eve/workers/pod-unit-tests/pod.yaml
      images:
        docker-unit-tests:
          context: 'salt'
          dockerfile: eve/workers/pod-unit-tests/salt/Dockerfile
      vars:
        name: "metalk8s-unit-tests-salt-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_unit-test_salt_junit_info
            STEP_NAME: unit_tests_salt
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Run all salt unit tests
          command: tox -e unit-tests
          haltOnFailure: true
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_unit-test_salt_junit_info
            STEP_NAME: unit_tests_salt
      - Upload: *upload_final_status_artifact

  unit_tests_lib_alert_tree:
    _metalk8s_internal_info:
      junit_info: &_unit_test_lib_alert_tree_junit_info
        TEST_SUITE: unit test
        TEST_NAME: lib-alert-tree
    worker:
      type: kube_pod
      path: eve/workers/pod-unit-tests/pod.yaml
      images:
        docker-unit-tests:
          context: 'tools/lib-alert-tree'
          dockerfile: eve/workers/pod-unit-tests/lib-alert-tree/Dockerfile
      vars:
        name: "metalk8s-unit-tests-lib-alert-tree-worker"
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_unit-test_salt_junit_info
            STEP_NAME: unit_tests_lib_alert_tree
      - ShellCommand: *yum_clean_all
      - ShellCommand:
          name: Install lib_alert_tree
          command: poetry install -E cli
          haltOnFailure: true
          workdir: &_workdir_lib_alert_tree >-
            %(prop:builddir)s/build/tools/lib-alert-tree
      - ShellCommand:
          name: Run all lib_alert_tree unit tests
          command: poetry run pytest -vv
          haltOnFailure: true
          workdir: *_workdir_lib_alert_tree
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_unit-test_salt_junit_info
            STEP_NAME: unit_tests_lib_alert_tree
      - Upload: *upload_final_status_artifact

  single-node-downgrade-centos:
    _metalk8s_internal_info:
      junit_info: &_downgrade_minor_single-node_junit_info
        TEST_SUITE: downgrade
        CLASS_NAME: minor dev.single node.centos7
        TEST_NAME: simple environment
    simultaneous_builds: 20
    worker: &single_node_worker
      type: openstack
      image: CentOS-7-x86_64-GenericCloud-1809.qcow2
      flavor: m1.large
      path: eve/workers/openstack-single-node
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_downgrade_minor_single-node_junit_info
            STEP_NAME: single-node-downgrade-centos
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      # --- Get ISO for version N ---
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *retrieve_iso
      - ShellCommand: *check_iso_checksum
      # --- Get ISO for version N-1 ---
      - ShellCommand: &retrieve_prev_iso_checksum
          <<: *retrieve_iso_checksum
          env: &_env_previous_artifact
            DEST_DIR: "/tmp"
            BASE_URL: "%(prop:artifacts_private_url)s/pre"
          name: Retrieve previous ISO image checksum
      - ShellCommand: &retrieve_prev_iso
          <<: *retrieve_iso
          name: Retrieve previous ISO image
          env:
            <<: *_env_retrieve_artifact_retry
            <<: *_env_previous_artifact
      - ShellCommand: &check_prev_iso_checksum
          <<: *check_iso_checksum
          name: Check previous ISO image with checksum
          workdir: "/tmp"
      # --- Prepare for Bootstrap in version N ---
      - ShellCommand: *create_mountpoint
      - ShellCommand: *mount_iso
      - ShellCommand: *bootstrap_config
      # --- Install version N ---
      - ShellCommand: *run_bootstrap
      - ShellCommand: &provision_prometheus_volumes
          name: Provision Prometheus and AlertManager storage
          env:
            BRANCH: "%(prop:branch)s"
            PRODUCT_TXT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s/product.txt"
            PRODUCT_MOUNT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
          command: >
            git checkout "${BRANCH}" --quiet &&
            sudo -E eve/create-volumes.sh
          haltOnFailure: true
      - ShellCommand: &untaint_bootstrap
          name: Untaint Bootstrap node
          env:
            NODE_NAME: "bootstrap"
          command: |
            sudo kubectl --kubeconfig /etc/kubernetes/admin.conf \
            patch node "$NODE_NAME" --patch '{
                "metadata": {"labels": {"node-role.kubernetes.io/node": ""}},
                "spec": {"taints": []}
            }'
          haltOnFailure: true
      # --- Wait for the cluster to be stabilized ---
      - ShellCommand: *wait_pods_stable
      # --- Test version N ---
      - ShellCommand: &fast_tests
          <<: *local_tests
          name: Run fast tests locally
          env: &_env_fast_tests
            <<: *_env_local_tests
            PYTEST_FILTERS: "post and ci and not multinode and not slow"
      - ShellCommand:
          <<: *local_tests
          name: Run slow tests locally
          env:
            <<: *_env_local_tests
            PYTEST_FILTERS: >-
              post and ci and not multinode and slow and not restore
      # --- Downgrade to version N-1 ---
      - ShellCommand:
          <<: *add_archive
          name: Add previous ISO to cluster
          env:
            <<: *_env_add_archive
            ISO_PATH: /tmp/metalk8s.iso
      - ShellCommand:
          name: Run downgrade to previous version
          command: >
            sudo bash
            /srv/scality/metalk8s-%(prop:metalk8s_version)s/downgrade.sh
            --destination-version "%(prop:metalk8s_version_prev)s" --verbose
          haltOnFailure: true
      - ShellCommand: *wait_pods_stable
      # --- Test version N-1 ---
      - ShellCommand: &fast_tests_prev
          <<: *local_tests
          name: Run fast tests locally for previous version
          env:
            <<: *_env_fast_tests
            BRANCH: "development/%(prop:product_version_prev)s"
            ISO_MOUNTPOINT: >
              /srv/scality/metalk8s-%(prop:metalk8s_version_prev)s
      # --- Collect logs ---
      - ShellCommand: *collect_sosreport
      - ShellCommand:
          <<: *copy_report_artifacts
          env:
            DEST_DIR: sosreport/sosreport/single-node-downgrade-centos
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_downgrade_minor_single-node_junit_info
            STEP_NAME: single-node-downgrade-centos
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          <<: *wait_debug
          env:
            STEP_NAME: single-node-downgrade-centos

  single-node-upgrade-centos:
    _metalk8s_internal_info:
      junit_info: &_upgrade_minor_single-node_junit_info
        TEST_SUITE: upgrade
        CLASS_NAME: minor dev.single node.centos7
        TEST_NAME: simple environment
    simultaneous_builds: 20
    worker: *single_node_worker
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_upgrade_minor_single-node_junit_info
            STEP_NAME: single-node-upgrade-centos
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      # --- Get ISO for version N ---
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *retrieve_iso
      - ShellCommand: *check_iso_checksum
      # --- Get ISO for version N-1 ---
      - ShellCommand: *retrieve_prev_iso_checksum
      - ShellCommand: *retrieve_prev_iso
      - ShellCommand: *check_prev_iso_checksum
      # --- Prepare for Bootstrap in version N-1 ---
      - ShellCommand:
          <<: *create_mountpoint
          env: &_env_version_prev
            PRODUCT_VERSION: "%(prop:metalk8s_version_prev)s"
      - ShellCommand:
          <<: *mount_iso
          env:
            <<: *_env_version_prev
            ISO_PATH: /tmp/metalk8s.iso
      - ShellCommand:
          <<: *bootstrap_config
          env:
            <<: *_env_version_prev
            DEBUG: "%(prop:metalk8s_debug:-false)s"
      # --- Install version N-1 ---
      - ShellCommand:
          <<: *run_bootstrap
          env: *_env_version_prev
      - ShellCommand:
          <<: *provision_prometheus_volumes
          env:
            BRANCH: "development/%(prop:product_version_prev)s"
            PRODUCT_TXT: "/srv/scality/metalk8s-%(prop:metalk8s_version_prev)s/product.txt"
            PRODUCT_MOUNT: "/srv/scality/metalk8s-%(prop:metalk8s_version_prev)s"
      - ShellCommand: *untaint_bootstrap
      - ShellCommand: *wait_pods_stable
      # --- Test version N-1 ---
      - ShellCommand: *fast_tests_prev
      # --- Upgrade to version N ---
      - ShellCommand:
          <<: *add_archive
          name: Add current ISO to cluster
          env:
            <<: *_env_add_archive
            PRODUCT_VERSION: "%(prop:metalk8s_version_prev)s"
      - ShellCommand:
          name: Run upgrade from previous version
          command: >
            sudo bash
            /srv/scality/metalk8s-%(prop:metalk8s_version)s/upgrade.sh
            --verbose
          haltOnFailure: true
      - ShellCommand: *provision_prometheus_volumes
      - ShellCommand: *wait_pods_stable
      # --- Test version N ---
      - ShellCommand: *fast_tests
      # --- Collect logs ---
      - ShellCommand: *collect_sosreport
      - ShellCommand:
          <<: *copy_report_artifacts
          env:
            DEST_DIR: sosreport/sosreport/single-node-upgrade-centos
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_upgrade_minor_single-node_junit_info
            STEP_NAME: single-node-upgrade-centos
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          <<: *wait_debug
          env:
            STEP_NAME: single-node-upgrade-centos

  single-node-install-rhel:
    worker:
      type: local
    steps:
      - SetProperty:
          name: Set OS property to rhel-8
          property: os
          value: "rhel-8"
      # FIXME: We disable offline mode for RHEL as it does not work for now,
      # there are some certificates issues when trying to reach RedHat
      # repositories through the company proxy.
      - SetProperty:
          name: Set offline property to false
          property: offline
          value: "false"
      - SetProperty:
          name: Set use_proxy property to false
          property: use_proxy
          value: "false"
      - TriggerStages:
          name: Trigger single-node-install with os set to rhel-8
          stage_names:
            - single-node-install

  single-node-install:
    _metalk8s_internal_info:
      junit_info: &_install_single-node_junit_info
        TEST_SUITE: install
        CLASS_NAME: "single node.%(prop:os:-centos-7)s"
        TEST_NAME: simple environment
    simultaneous_build: 20
    worker: &openstack_worker
      type: openstack
      image: CentOS-7-x86_64-GenericCloud-1809.qcow2
      flavor: m1.medium
      path: eve/workers/openstack-basic
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_install_single-node_junit_info
            STEP_NAME: "single-node-install-%(prop:os:-centos-7)s"
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      - ShellCommand: *git_pull_terraform
      - ShellCommand: *retrieve_iso
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *check_iso_checksum
      - ShellCommand: *terraform_install
      - ShellCommand: *terraform_install_check
      - ShellCommand: *terraform_init
      - ShellCommand: *terraform_validate
      - ShellCommand: *terraform_apply
      - ShellCommand: *set_bootstrap_minion_id_ssh
      - ShellCommand: *bootstrap_config_ssh
      - ShellCommand: *copy_iso_bootstrap_ssh
      - ShellCommand: *create_mountpoint_ssh
      - ShellCommand: *mount_iso_ssh
      - ShellCommand: *run_bootstrap_ssh
      - ShellCommand: *provision_volumes_ssh
      - ShellCommand:
          <<: *untaint_bootstrap_ssh
          doStepIf: true
      # NOTE: This section only run if property "install_solution" is True
      # {{{
      - ShellCommand: *retrieve_iso_solution
      - ShellCommand: *copy_solution_archive_bootstrap_ssh
      - ShellCommand: *import_solution
      - ShellCommand: *activate_solution
      - ShellCommand: *create_solution_env
      - ShellCommand: *deploy_solution
      - ShellCommand: *wait_for_solution_operator
      # }}}
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand: *git_pull_ssh
      - ShellCommand: *bastion_fast_tests
      - SetPropertyFromCommand: *set_bootstrap_cp_ip_ssh
      - ShellCommand: *install_cypress_ssh
      - ShellCommand: *bastion_ui_tests
      - ShellCommand: *collect_cypress_result_ssh
      - Upload: *upload_cypress_artifacts
      - ShellCommand: *generate_report_over_ssh
      - ShellCommand:
          <<: *collect_report_over_ssh
          env:
            <<: *_env_collect_report_over_ssh
            STEP_NAME: "single-node-install-%(prop:os:-centos-7)s"
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_install_single-node_junit_info
            STEP_NAME: single-node-install-rhel
      - Upload: *upload_final_status_artifact
      # NOTE: This section only run if property "generate_snapshot" is True
      # {{{
      - ShellCommand: *terraform_gen_snapshot
      # }}}
      - ShellCommand:
          <<: *wait_debug
          env:
            STEP_NAME: "single-node-install-%(prop:os:-centos-7)s"
      - ShellCommand: *terraform_destroy

  lifecycle-patch-version:
    worker:
      type: local
    steps:
      - SetProperty:
          name: Set promoted version type to patch
          property: promoted_version_type
          value: patch
      - SetPropertyFromCommand:
          name: Set previous patch version to upgrade from and downgrade to
          property: product_promoted_version
          command: >
              major=$(echo "%(prop:product_version)s" | cut -d'.' -f1) &&
              minor=$(echo "%(prop:product_version)s" | cut -d'.' -f2) &&
              patch=$(echo "%(prop:product_version)s" | cut -d'.' -f3) &&
              echo "$major.$minor.$(( patch>0 ? patch-1 : 0 ))"
      - SetPropertyFromCommand:
          name: Set the patch version flag
          property: patch_present
          env:
            PRODUCT_VERSION: "%(prop:product_version)s"
            PRODUCT_VERSION_PREV_patch: "%(prop:product_promoted_version)s"
          command: |-
              if [ "$PRODUCT_VERSION" != "$PRODUCT_VERSION_PREV_patch" ]; then
                echo "true"
              else
                echo "false"
              fi
      - TriggerStages:
          name: Trigger upgrade stages with patch Artifact ISO
          doStepIf: "%(prop:patch_present)s"
          stage_names:
            - snapshot-single-node-upgrades
            - snapshot-multi-node-upgrades
            - single-node-downgrade-promoted-centos

  lifecycle-minor-version:
    worker:
      type: local
    steps:
      - SetProperty:
          name: Set promoted version type to minor
          property: promoted_version_type
          value: minor
      - Git: *git_pull
      - SetPropertyFromCommand:
          name: Set previous minor version to upgrade from and downgrade to
          property: product_promoted_version
          command: >
              major=$(echo "%(prop:product_version)s" | cut -d'.' -f1) &&
              minor=$(echo "%(prop:product_version)s" | cut -d'.' -f2) &&
              git tag --sort=taggerdate --list "$major.$(( $minor-1 )).*" | tail -1
      - SetPropertyFromCommand:
          name: Set the minor version flag
          property: minor_present
          env:
            PRODUCT_VERSION_PREV_patch: "%(prop:product_promoted_version)s"
          command: |-
              if [ -z "$PRODUCT_VERSION_PREV_patch" ]; then
                echo "false"
              else
                echo "true"
              fi
      - TriggerStages:
          name: Trigger upgrade stages with minor Artifact ISO
          doStepIf: "%(prop:minor_present)s"
          stage_names:
            - snapshot-single-node-upgrades
            - snapshot-multi-node-upgrades
            # NOTE: Deactivate minor downgrade on this branch
            # See: https://github.com/scality/metalk8s/issues/1750
            #- single-node-downgrade-promoted-centos

  snapshot-single-node-upgrades:
    # NOTE: This stage just set `environment_type` property to
    # `single node` and trigger all upgrade single nodes stages
    worker:
      type: local
    steps:
      - SetProperty: &prop_single_node_env_type
          name: Set environment type to single node
          property: environment_type
          value: single node
      - TriggerStages:
          name: Trigger single-node upgrade stages
          stage_names:
            - snapshot-simple-environment-upgrade

  snapshot-multi-node-upgrades:
    # NOTE: This stage just set `environment_type` property to
    # `multi node` and trigger all upgrade multi nodes stages
    worker:
      type: local
    steps:
      - SetProperty: &prop_multi_node_env_type
          name: Set environment type to multi node
          property: environment_type
          value: multi node
      - TriggerStages:
          name: Trigger multi-node upgrade stages
          stage_names:
            - snapshot-3-nodes-upgrade


  snapshot-simple-environment-upgrade:
    # NOTE: This stage just set `environment_name` property to
    # `simple environment` and trigger the real upgrade tests
    worker:
      type: local
    steps:
      - SetProperty: &prop_simple_env_name
          name: Set environment name to simple environment
          property: environment_name
          value: simple environment
      - TriggerStages:
          name: Trigger simple environment upgrade stage
          stage_names:
            - snapshot-upgrade

  snapshot-3-nodes-upgrade:
    # NOTE: This stage just set `environment_name` property to
    # `simple environment` and trigger the real upgrade tests
    worker:
      type: local
    steps:
      - SetProperty: &prop_3nodes_count
          name: Set nodes count to 2 (bootstrap + 2 nodes)
          property: nodes_count
          value: "2"
      - SetProperty: &prop_3nodes_env_name
          name: Set environment name to 3 nodes
          property: environment_name
          value: "3 nodes"
      - SetProperty:
          name: Enable certificates renewal test
          property: check_certs_renewal
          value: "true"
      - TriggerStages:
          name: Trigger 3 node upgrade stage
          stage_names:
            - snapshot-upgrade

  # --- Upgrade from snapshot ---
  # NOTE: This stage do every kind of upgrade from snapshot
  # argument come from property
  # Mandatory:
  # - product_promoted_version  (MetalK8s version to start upgrade)
  # - environment_type          (Type of environment e.g.: single node)
  # - environment_name          (Name of environment e.g.: simple environment)
  # Optional:
  # - os                        (Operating system, default: centos-7)
  # - promoted_version_type     ("patch" or "minor", default: unknown)
  # - nodes_count               (Number of nodes to spawn, default: 0)
  # - check_certs_renewal       (Whether to test certificate renewal, default: false)
  # Snapshot need to be named:
  # - "metalk8s-<product_promoted_version>-<os>-<environement_type>-<environment_name>-bootstrap"
  # - "metalk8s-<product_promoted_version>-<os>-<environement_type>-<environment_name>-node-X"
  snapshot-upgrade:
    _metalk8s_internal_info:
      junit_info: &_snapshot_upgrade_junit_info
        TEST_SUITE: upgrade
        # NOTE: promoted_version_type should be "patch" or "minor"
        CLASS_NAME: "%(prop:promoted_version_type:-unknown)s.%(prop:environment_type)s.%(prop:os:-centos-7)s"
        TEST_NAME: "%(prop:environment_name)s"
    simultaneous_build: 20
    worker: *openstack_worker
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_snapshot_upgrade_junit_info
            STEP_NAME: "%(prop:environment_type)s-%(prop:environment_name)s-upgrade-%(prop:promoted_version_type:-unknown)s-%(prop:os:-centos-7)s"
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      - ShellCommand: *git_pull_terraform
      # --- Get ISO for version N ---
      - ShellCommand: *retrieve_iso
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *check_iso_checksum
      # --- Terraform stuff ---
      - ShellCommand: *terraform_install
      - ShellCommand: *terraform_install_check
      - ShellCommand: *terraform_init
      - ShellCommand: *terraform_validate
      - ShellCommand:
          <<: *terraform_apply
          env: &_env_terraform_upgrade
            <<: *_env_terraform
            TF_VAR_restore_env: "%(prop:product_promoted_version)s-%(prop:os:-centos-7)s-%(prop:environment_type)s-%(prop:environment_name)s"
      # --- Wait for all pods to be running ---
      - ShellCommand:
          name: Ensure all Salt minion running
          command: |
            ssh -F ssh_config bootstrap <<ENDSSH
            for _ in \$(seq 10); do
                sudo crictl exec \$(sudo crictl ps --label="io.kubernetes.container.name=salt-master" -q) salt '*' test.ping && exit 0
                sleep 5
            done
            ENDSSH
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand:
          # NOTE: We remove all "NodeAffinity" pods as, when restoring an environment
          # from time to time we get some pods stuck in "NodeAffinity"
          name: Remove pods stuck in NodeAffinity
          command: |
            ssh -F ssh_config bootstrap <<ENDSSH
            for ns in \$(sudo kubectl get ns --kubeconfig=/etc/kubernetes/admin.conf -o jsonpath='{.items[*].metadata.name}'); do
                node_aff_pods=\$(sudo kubectl get pods --kubeconfig=/etc/kubernetes/admin.conf -n "\$ns" | grep NodeAffinity | awk '{print \$1}' | tr \\\n ' ')
                if [ "\$node_aff_pods" ]; then
                    sudo kubectl delete pods --kubeconfig=/etc/kubernetes/admin.conf -n "\$ns" \$node_aff_pods
                fi
            done
            ENDSSH
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          # NOTE: we ignore Succeeded pods which were scheduled by Jobs, since
          # their Ready condition is False
          name: Ensure all pods are running
          command: >
            ssh -F ssh_config bootstrap 'sudo bash -c "
            kubectl --kubeconfig=/etc/kubernetes/admin.conf
            wait pods --for=condition=Ready --all --all-namespaces
            --field-selector=status.phase!=Succeeded"'
          workdir: *terraform_workdir
          haltOnFailure: true
      # --- We do not need to Test promoted version ---
      # --- Upgrade to version N ---
      - ShellCommand:
          <<: *copy_iso_bootstrap_ssh
          env:
            <<: *_env_copy_iso_bootstrap_ssh
            DEST: "metalk8s-%(prop:metalk8s_version)s.iso"
      - ShellCommand:
          <<: *add_archive_ssh
          env:
            <<: *_env_add_archive_ssh
            ISO_PATH: "metalk8s-%(prop:metalk8s_version)s.iso"
            PRODUCT_VERSION: "%(prop:product_promoted_version)s"
      - ShellCommand:
          name: Run upgrade from promoted version
          command: >
            ssh -F ssh_config bootstrap "
            sudo bash
            /srv/scality/metalk8s-%(prop:metalk8s_version)s/upgrade.sh
            --verbose"
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          <<: *provision_volumes_ssh
          env:
            <<: *_env_provision_volumes
            PRODUCT_MOUNT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
            PRODUCT_TXT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s/product.txt"
      - ShellCommand:
          <<: *wait_pods_stable_ssh
          env:
            <<: *_env_wait_pods_stable_ssh
            # NOTE: We increase stabilization time after upgrade it may take some
            # time to stabilize (e.g.: Rolling update of some DaemonSet that may take
            # some times, especially in multi node context)
            STABILIZATION_TIME: "240"
      # --- Test version N ---
      - ShellCommand: *git_pull_ssh
      - ShellCommand:
          <<: *bastion_fast_tests
          env:
            <<: *_env_bastion_fast_tests
            ISO_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
      # --- Check certs renewal in version N ---
      - ShellCommand:
          name: Certificates expiration beacons test
          command: |
              scp -F ssh_config %(prop:builddir)s/build/tests/test-certificates-beacon.sh bootstrap:
              ssh -F ssh_config bootstrap "sudo ./test-certificates-beacon.sh /srv/scality/metalk8s-%(prop:metalk8s_version)s"
          workdir: *terraform_workdir
          haltOnFailure: true
          doStepIf: "%(prop:check_certs_renewal:-false)s"
      - ShellCommand:
          <<: *wait_pods_stable_ssh
          name: Wait for pods after certificates renewal
          doStepIf: "%(prop:check_certs_renewal:-false)s"
      - ShellCommand:
          <<: *bastion_fast_tests
          name: Run fast tests on Bastion after certificates renewal
          env:
            <<: *_env_bastion_fast_tests
            ISO_MOUNTPOINT: "/srv/scality/metalk8s-%(prop:metalk8s_version)s"
          doStepIf: "%(prop:check_certs_renewal:-false)s"
      # --- Collect logs ---
      - ShellCommand: *generate_report_over_ssh
      - ShellCommand:
          <<: *collect_report_over_ssh
          env:
            <<: *_env_collect_report_over_ssh
            STEP_NAME: "%(prop:environment_type)s-%(prop:environment_name)s-upgrade-%(prop:promoted_version_type:-unknown)s-%(prop:os:-centos-7)s"
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_snapshot_upgrade_junit_info
            STEP_NAME: "%(prop:environment_type)s-%(prop:environment_name)s-upgrade-%(prop:promoted_version_type:-unknown)s-%(prop:os:-centos-7)s"
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          <<: *wait_debug
          env:
            STEP_NAME: "%(prop:environment_type)s-%(prop:environment_name)s-upgrade-%(prop:promoted_version_type:-unknown)s-%(prop:os:-centos-7)s"
      - ShellCommand:
          <<: *terraform_destroy
          env: *_env_terraform_upgrade

  # --- Downgrade for promoted versions ---
  single-node-downgrade-promoted-centos:
    _metalk8s_internal_info:
      junit_info: &_downgrade_promoted_single-node_junit_info
        TEST_SUITE: downgrade
        CLASS_NAME: "%(prop:promoted_version_type)s.single node.centos7"
        TEST_NAME: simple environment
    simultaneous_builds: 20
    worker: *single_node_worker
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_downgrade_promoted_single-node_junit_info
            STEP_NAME: "single-node-downgrade-%(prop:promoted_version_type)s-centos"
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      # --- Get ISO for version N ---
      - ShellCommand: *retrieve_iso
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *check_iso_checksum
      # --- Get ISO for promoted version from Artefacts ---
      - ShellCommand:
          <<: *retrieve_iso_checksum
          name: Retrieve promoted ISO image checksum
          env: &_env_promoted_artifacts
            DEST_DIR: "/tmp"
            BASE_URL: "http://artifacts/builds/github:scality:metalk8s:\
                      promoted-%(prop:product_promoted_version)s"
      - ShellCommand:
          <<: *retrieve_iso
          name: Retrieve promoted ISO image
          env:
            <<: *_env_retrieve_artifact_retry
            <<: *_env_promoted_artifacts
      - ShellCommand:
          <<: *check_iso_checksum
          name: Check promoted ISO image with checksum
          workdir: "/tmp"
      # --- Prepare for Bootstrap with version N ---
      - ShellCommand: *create_mountpoint
      - ShellCommand: *mount_iso
      - ShellCommand: *bootstrap_config
      # --- Install version N ---
      - ShellCommand: *run_bootstrap
      - ShellCommand: *provision_prometheus_volumes
      - ShellCommand: *untaint_bootstrap
      - ShellCommand: *wait_pods_stable
      # --- Test version N ---
      - ShellCommand: *fast_tests
      # --- Downgrade to promoted version ---
      - ShellCommand:
          <<: *add_archive
          name: Add previous ISO to cluster
          env:
            <<: *_env_add_archive
            ISO_PATH: /tmp/metalk8s.iso
      - ShellCommand:
          name: Run downgrade to promoted version
          command: >
            sudo bash
            /srv/scality/metalk8s-%(prop:metalk8s_version)s/downgrade.sh
            --destination-version %(prop:product_promoted_version)s --verbose
          haltOnFailure: true
      - ShellCommand: *wait_pods_stable
      # --- Test promoted version ---
      - ShellCommand:
          <<: *local_tests
          name: Run fast tests locally for promoted version
          env:
            <<: *_env_fast_tests
            BRANCH: "%(prop:product_promoted_version)s"
            ISO_MOUNTPOINT: >
              /srv/scality/metalk8s-%(prop:product_promoted_version)s
      # --- Collect logs ---
      - ShellCommand: *collect_sosreport
      - ShellCommand:
          <<: *copy_report_artifacts
          env:
            DEST_DIR: sosreport/sosreport/single-node-downgrade-%(prop:promoted_version_type)s-centos
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_downgrade_promoted_single-node_junit_info
            STEP_NAME: "single-node-downgrade-%(prop:promoted_version_type)s-centos"
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          <<: *wait_debug
          env:
            STEP_NAME: "single-node-downgrade-%(prop:promoted_version_type)s-centos"

  multiple-nodes:
    _metalk8s_internal_info:
      junit_info: &_install_multi-node_junit_info
        TEST_SUITE: install
        CLASS_NAME: "multi node.%(prop:os:-centos-7)s"
        TEST_NAME: "1 bootstrap %(prop:nodes_count:-1)s master,etcd"
    simultaneous_builds: 20
    worker: *openstack_worker
    steps:
      - Git: *git_pull
      - SetProperty:
          # NOTE: Set property will be skipped if the value is already set,
          # so this step just set default value to 1
          name: Set nodes count default to 1
          property: nodes_count
          value: "%(prop:nodes_count:-1)s"
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_install_multi-node_junit_info
            STEP_NAME: "multiple-nodes-%(prop:os:-centos-7)s"
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      - ShellCommand: *git_pull_terraform
      - ShellCommand: *retrieve_iso
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *check_iso_checksum
      - ShellCommand: *terraform_install
      - ShellCommand: *terraform_install_check
      - ShellCommand: *terraform_init
      - ShellCommand: *terraform_validate
      - ShellCommand: *terraform_apply
      - ShellCommand: &check_ssh_config_bootstrap
          name: Check SSH config for bootstrap node
          command: |-
            if [ ! -f ssh_config ]; then
              echo "Missing SSH config file" >&2
              exit 1
            fi
            for _ in $(seq 1 12); do
              sleep 5
              ssh -F ssh_config bootstrap id && break
            done;
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand: &copy_bastion_priv_key_to_bootstrap
          # FIXME: find a cleaner way with Terraform.
          name: Send bastion private key to bootstrap
          command: >
              ssh -F ssh_config bootstrap "sudo mkdir -p /etc/metalk8s/pki/" &&
              scp -F ssh_config -3 bastion:.ssh/bastion bootstrap:./         &&
              ssh -F ssh_config bootstrap "sudo cp bastion /etc/metalk8s/pki/"
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand: *set_bootstrap_minion_id_ssh
      - ShellCommand: &create_archive_directory
          name: Create /archives directory
          env:
            NODE: "bootstrap"
          command: >
            ssh -F ssh_config $NODE '
              sudo mkdir /archives && sudo chown $USER: /archives
            '
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          <<: *bootstrap_config_ssh
          env:
            <<: *_env_bootstrap_config_ssh
            ARCHIVE: /archives/metalk8s.iso
            COREDNS_ANTI_AFFINITY: >-
              {"hard": [{"topologyKey": "kubernetes.io/hostname"}]}
      - ShellCommand:
          <<: *copy_iso_bootstrap_ssh
          env:
            <<: *_env_copy_iso_bootstrap_ssh
            DEST: /archives/metalk8s.iso
      - ShellCommand: *create_mountpoint_ssh
      - ShellCommand:
          <<: *mount_iso_ssh
          env:
            <<: *_env_mount_iso_ssh
            ARCHIVE: /archives/metalk8s.iso
      - ShellCommand: *run_bootstrap_ssh
      - ShellCommand: *git_pull_ssh
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand:
          <<: *bastion_tests
          name: Run installation scenarii on the bastion
          env:
            <<: *_env_bastion_tests
            PYTEST_FILTERS: "install and ci and multinodes and not node$((%(prop:nodes_count)s + 1))"
      - ShellCommand: &provision_volumes_on_node1
          <<: *provision_volumes_ssh
          env:
            <<: *_env_provision_volumes
            NODE_NAME: node-1
      # NOTE: This section only run if property "install_solution" is True
      # {{{
      - ShellCommand: *retrieve_iso_solution
      - ShellCommand: *copy_solution_archive_bootstrap_ssh
      - ShellCommand: *import_solution
      - ShellCommand: *activate_solution
      - ShellCommand: *create_solution_env
      - ShellCommand: *untaint_bootstrap_ssh
      - ShellCommand: *deploy_solution
      - ShellCommand: *wait_for_solution_operator
      # }}}
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand:
          <<: *create_archive_directory
          env:
            NODE: node-1
      - ShellCommand:
          <<: *copy_iso_bootstrap_ssh
          name: Copy archive to node-1
          env:
            <<: *_env_copy_iso_bootstrap_ssh
            NODE: node-1
            DEST: /archives/metalk8s.iso
      - ShellCommand:
          <<: *copy_solution_archive_bootstrap_ssh
          name: Copy Solution archive to node-1
          env:
            <<: *_env_copy_solution_archive_bootstrap_ssh
            NODE: node-1
      - ShellCommand: &multi_node_fast_tests
          <<: *bastion_tests
          name: Run fast tests on Bastion
          env:
            <<: *_env_bastion_tests
            PYTEST_FILTERS: "post and ci and not slow"
      - ShellCommand: &multi_node_slow_tests
          <<: *bastion_tests
          name: Run slow tests on Bastion
          env:
            <<: *_env_bastion_tests
            PYTEST_ARGS: "--suppress-no-test-exit-code"
            PYTEST_FILTERS: >
              post and ci and slow and not bootstrap and not restore
      - ShellCommand: *generate_report_over_ssh
      - ShellCommand:
          <<: *collect_report_over_ssh
          env:
            <<: *_env_collect_report_over_ssh
            STEP_NAME: "multiple-nodes-%(prop:os:-centos-7)s"
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_install_multi-node_junit_info
            STEP_NAME: "multiple-nodes-%(prop:os:-centos-7)s"
      - Upload: *upload_final_status_artifact
      # NOTE: This section only run if property "generate_snapshot" is True
      # {{{
      - ShellCommand: *terraform_gen_snapshot
      # }}}
      - ShellCommand:
          <<: *wait_debug
          timeout: 14400
          env:
            STEP_NAME: "multiple-nodes-%(prop:os:-centos-7)s"
            DURATION: "14400"
      - ShellCommand: *terraform_destroy

  bootstrap-restore:
    _metalk8s_internal_info:
      junit_info: &_bootstrap_restore_junit_info
        TEST_SUITE: install
        CLASS_NAME: "multi node.%(prop:os:-centos-7)s"
        TEST_NAME: bootstrap restore
      simultaneous_builds: 20
    worker: *openstack_worker
    steps:
      - Git: *git_pull
      - SetProperty:
          # NOTE: Set property will be skipped if the value is already set,
          # so this step just set default value to 2
          name: Set nodes count default to 2
          property: nodes_count
          value: "%(prop:nodes_count:-2)s"
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_bootstrap_restore_junit_info
            STEP_NAME: bootstrap-restore
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      - ShellCommand: *git_pull_terraform
      - ShellCommand: *retrieve_iso
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *check_iso_checksum
      - ShellCommand: *terraform_install
      - ShellCommand: *terraform_install_check
      - ShellCommand: *terraform_init
      - ShellCommand: *terraform_validate
      - ShellCommand: *terraform_apply
      - ShellCommand: *check_ssh_config_bootstrap
      - ShellCommand: *copy_bastion_priv_key_to_bootstrap
      - ShellCommand: *set_bootstrap_minion_id_ssh
      - ShellCommand: *bootstrap_config_ssh
      - ShellCommand: *copy_iso_bootstrap_ssh
      - ShellCommand: *create_mountpoint_ssh
      - ShellCommand: *mount_iso_ssh
      - ShellCommand: *run_bootstrap_ssh
      - ShellCommand: *git_pull_ssh
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand:
          <<: *bastion_tests
          name: Run installation scenario on the bastion
          env:
            <<: *_env_bastion_tests
            PYTEST_FILTERS: "install and ci and multinodes"
      - ShellCommand: *provision_volumes_on_node1
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand:
          <<: *copy_iso_bootstrap_ssh
          name: Copy archive to node-1
          env:
            <<: *_env_copy_iso_bootstrap_ssh
            NODE: node-1
      - ShellCommand: *multi_node_fast_tests
      - ShellCommand: *multi_node_slow_tests
      - SetPropertyFromCommand:
          name: Set bootstrap backup archive property
          property: bootstrap_backup_archive
          command: >
            ssh -F ssh_config bootstrap
            "sudo find /var/lib/metalk8s/backups -name '*.tar.gz'
            -printf '%f\n' | sort | tail -n1"
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          name: Retrieve the backup archive from the bootstrap node
          command: >
            scp -F ssh_config
            bootstrap:"/var/lib/metalk8s/backups/%(prop:bootstrap_backup_archive)s"
            /tmp
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          name: Destroy the bootstrap node
          env: *_env_terraform
          command: >
            terraform destroy -auto-approve
            -target openstack_compute_instance_v2.bootstrap
          workdir: *terraform_workdir
          haltOnFailure: true
      - SetPropertyFromCommand:
          name: Set node-1 etcd container id
          property: node-1_etcd_container_id
          command: >
            ssh -F ssh_config node-1
            sudo crictl ps -q --label io.kubernetes.pod.namespace=kube-system
            --label io.kubernetes.container.name=etcd --state Running
          workdir: *terraform_workdir
          haltOnFailure: true
      - SetPropertyFromCommand:
          name: Set boostrap node etcd member id
          property: bootstrap_etcd_member_id
          command: >
            ssh -F ssh_config node-1
            sudo crictl exec -i "%(prop:node-1_etcd_container_id)s" sh -c \"
            ETCDCTL_API=3 etcdctl --endpoints https://127.0.0.1:2379
            --cert /etc/kubernetes/pki/etcd/server.crt
            --key /etc/kubernetes/pki/etcd/server.key
            --cacert /etc/kubernetes/pki/etcd/ca.crt
            member list\" | awk -F ', ' '$3 ~ "bootstrap" { print $1 }'
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          name: Remove bootstrap node from etcd members
          command: >
            ssh -F ssh_config node-1
            sudo crictl exec -i "%(prop:node-1_etcd_container_id)s" sh -c \"
            ETCDCTL_API=3 etcdctl --endpoints https://127.0.0.1:2379
            --cert /etc/kubernetes/pki/etcd/server.crt
            --key /etc/kubernetes/pki/etcd/server.key
            --cacert /etc/kubernetes/pki/etcd/ca.crt
            member remove %(prop:bootstrap_etcd_member_id)s\"
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          name: Remove bootstrap node object
          command: |-
            for _ in $(seq 5); do
                ssh -F ssh_config node-1 \
                sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf delete \
                node --selector="node-role.kubernetes.io/bootstrap" && exit 0
                sleep 5
            done
            exit 1
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          name: Create a new bootstrap node
          env: *_env_terraform
          command:
            terraform apply -auto-approve -refresh
            -target openstack_compute_instance_v2.bootstrap
            -target null_resource.copy_bastion_pub_to_bootstrap
            -target null_resource.ssh_config
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          name: Copy the backup archive to the new bootstrap node
          command: >
            scp -F ssh_config
            "/tmp/%(prop:bootstrap_backup_archive)s" bootstrap:/tmp
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand:
          <<: *copy_iso_bootstrap_ssh
          name: Copy ISO to the new bootstrap node
      - ShellCommand:
          <<: *create_mountpoint_ssh
          name: Create mountpoint on the new bootstrap node
      - ShellCommand:
          <<: *mount_iso_ssh
          name: Mount ISO image on the new bootstrap node
      - ShellCommand:
          <<: *bastion_tests
          name: Run restore tests on the bastion
          # NOTE: Increase timeout as restore may take some time
          # Since we use pytest we do not have any output until restore totaly
          # finished
          timeout: 2700
          env:
            <<: *_env_bastion_tests
            PYTEST_FILTERS: "restore"
            BOOTSTRAP_BACKUP_ARCHIVE: "/tmp/%(prop:bootstrap_backup_archive)s"
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand: *multi_node_fast_tests
      - ShellCommand: *multi_node_slow_tests
      - ShellCommand: *generate_report_over_ssh
      - ShellCommand:
          <<: *collect_report_over_ssh
          env:
            <<: *_env_collect_report_over_ssh
            STEP_NAME: bootstrap-restore
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_bootstrap_restore_junit_info
            STEP_NAME: bootstrap-restore
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          <<: *wait_debug
          timeout: 14400
          env:
            STEP_NAME: bootstrap-restore
            DURATION: "14400"
      - ShellCommand: *terraform_destroy

  single-node-solutions:
    _metalk8s_internal_info:
      junit_info: &_solutions_single-node_junit_info
        TEST_SUITE: install
        CLASS_NAME: "single node.%(prop:os:-centos-7)s"
        TEST_NAME: solutions
    simultaneous_builds: 20
    worker: *openstack_worker
    steps:
      - Git: *git_pull
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            <<: *_solutions_single-node_junit_info
            STEP_NAME: single-node-solutions
      - ShellCommand: *yum_clean_all
      - ShellCommand: *ssh_ip_setup
      - ShellCommand: *git_pull_terraform
      - ShellCommand: *retrieve_iso
      - ShellCommand: *retrieve_iso_checksum
      - ShellCommand: *check_iso_checksum
      - ShellCommand: *terraform_install
      - ShellCommand: *terraform_install_check
      - ShellCommand: *terraform_init
      - ShellCommand: *terraform_validate
      - ShellCommand: *terraform_apply
      - ShellCommand: *set_bootstrap_minion_id_ssh
      - ShellCommand: *bootstrap_config_ssh
      - ShellCommand: *copy_iso_bootstrap_ssh
      - ShellCommand: *create_mountpoint_ssh
      - ShellCommand: *mount_iso_ssh
      - ShellCommand: *run_bootstrap_ssh
      - ShellCommand: *provision_volumes_ssh
      - ShellCommand: *untaint_bootstrap_ssh
      - ShellCommand: *wait_pods_stable_ssh
      - ShellCommand:
          <<: *retrieve_iso
          name: Retrieve Solution example-solution ISO image
          env:
            <<: *_env_retrieve_artifact_retry
            FILE_SOURCE: >-
              example-solution-%(prop:example_solution_version)s.iso
            FILE_DEST: example-solution.iso
            BASE_URL: "http://artifacts/builds/github:\
                       scality:metalk8s-solution-example:\
                       promoted-%(prop:example_solution_version)s"
      - ShellCommand:
          <<: *retrieve_iso
          name: Retrieve Solution next example-solution ISO image
          env:
            <<: *_env_retrieve_artifact_retry
            FILE_SOURCE: >-
              example-solution-%(prop:example_solution_version_next)s.iso
            FILE_DEST: example-solution-next.iso
            BASE_URL: "http://artifacts/builds/github:\
                       scality:metalk8s-solution-example:\
                       promoted-%(prop:example_solution_version_next)s"
      - ShellCommand:
          name: Copy Solutions ISOs to bootstrap node
          command: >
            scp -F ssh_config
            "%(prop:builddir)s/build/example-solution.iso"
            "%(prop:builddir)s/build/example-solution-next.iso"
            "bootstrap:/var/tmp"
          workdir: *terraform_workdir
          haltOnFailure: true
      - ShellCommand: *git_pull_ssh
      - ShellCommand:
          <<: *bastion_tests
          name: Run Solutions CLI tests on Bastion
          env:
            <<: *_env_bastion_tests
            PYTEST_FILTERS: "solution"
      - ShellCommand: *generate_report_over_ssh
      - ShellCommand:
          <<: *collect_report_over_ssh
          env:
            <<: *_env_collect_report_over_ssh
            STEP_NAME: single-node-solutions
      - Upload: *upload_report_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            <<: *_solutions_single-node_junit_info
            STEP_NAME: single-node-solutions
      - Upload: *upload_final_status_artifact
      - ShellCommand:
          <<: *wait_debug
          env:
            STEP_NAME: single-node-solutions
      - ShellCommand: *terraform_destroy

  generate-snapshot:
    worker:
      type: local
    steps:
      - GetArtifactsFromStage:
          name: Get pre-merge artifacts_name
          stage: pre-merge
          property: premerge_artifacts_name
          haltOnFailure: true
      - SetProperty: *set_premerge_url
      - TriggerStages:
          name: Trigger snapshot generations stages simultaneously
          stage_names:
            - gen-single-node-snapshot
            - gen-3-nodes-snapshot

  gen-single-node-snapshot:
    worker:
      type: local
    steps:
      - SetPropertyFromCommand: *set_version_prop
      - SetPropertyFromCommand: *set_short_version_prop
      - SetProperty: *prop_single_node_env_type
      - SetProperty: *prop_simple_env_name
      - SetProperty: &prop_enable_solution_install
          name: Enable solution install
          property: install_solution
          value: "true"
      - SetProperty: &prop_enable_snapshot_generation
          name: Enable snapshot mode
          property: generate_snapshot
          value: "true"
      - TriggerStages:
          name: Trigger single-node step with snapshot enabled
          stage_names:
            - single-node-install

  gen-3-nodes-snapshot:
    worker:
      type: local
    steps:
      - SetPropertyFromCommand: *set_version_prop
      - SetPropertyFromCommand: *set_short_version_prop
      - SetProperty: *prop_multi_node_env_type
      - SetProperty: *prop_3nodes_count
      - SetProperty: *prop_3nodes_env_name
      - SetProperty: *prop_enable_solution_install
      - SetProperty: *prop_enable_snapshot_generation
      - TriggerStages:
          name: Trigger multiple-nodes step with snapshot enabled
          stage_names:
            - multiple-nodes

  build-shell-ui:
    worker: *build_worker
    steps:
      - ShellCommand:
          <<: *add_final_status_artifact_failed
          env:
            <<: *_env_final_status_artifact_failed
            STEP_NAME: build-shell-ui
      - ShellCommand: *wait_for_docker
      - Git: *git_pull
      - SetPropertyFromCommand:
          name: Set shell-ui version as a property
          property: shell_ui_version
          command: |
            . ./VERSION
            echo "$VERSION_MAJOR.$VERSION_MINOR.$VERSION_PATCH$VERSION_SUFFIX"
      - ShellCommand:
          name: Build shell-ui container image
          command: docker build . --tag shell-ui:v%(prop:shell_ui_version)s
          workdir: build/shell-ui
          haltOnFailure: true
      - ShellCommand:
          name: Save shell-ui container image
          command: >
            docker save shell-ui:v%(prop:shell_ui_version)s |
            gzip > shell-ui.tar.gz
      - ShellCommand:
          <<: *copy_artifacts
          env:
            DEST_DIR: "artifacts/images"
            ARTIFACTS: >-
              shell-ui.tar.gz
      - Upload: *upload_artifacts
      - ShellCommand:
          <<: *add_final_status_artifact_success
          env:
            <<: *_env_final_status_artifact_success
            STEP_NAME: build-shell-ui
      - Upload: *upload_final_status_artifact

  publish:
    worker:
      type: local
    steps:
      - ShellCommand: *add_final_status_artifact_failed
      - GetArtifactsFromStage: *get_premerge_name
      - SetProperty: *set_premerge_url
      - TriggerStages:
          stage_names:
            - publish-shell-ui
          haltOnFailure: true
      - ShellCommand: *add_final_status_artifact_success
      - Upload: *upload_final_status_artifact

  publish-shell-ui:
    worker: *build_worker
    steps:
      - ShellCommand: *wait_for_docker
      - ShellCommand: *image_registry_login
      - SetPropertyFromCommand: *set_version_prop
      - SetPropertyFromCommand:
          name: Set is_latest_dev property
          property: is_latest_dev
          command: |
            if [ "$IS_LATEST_DEV" != unset ]; then
                echo "$IS_LATEST_DEV"
            elif [ "$IS_LATEST_STABLE" = true ]; then
                echo false
            else
                LATEST_DEV_BRANCH=$(
                    git ls-remote --heads "$GIT_REFERENCE" | \
                    awk -F/ '$3 == "development" { print $4 }' | sort -V | tail -n 1
                )
                [ "$GIT_BRANCH" = "development/$LATEST_DEV_BRANCH" ] && echo true || echo false
            fi
          env:
            GIT_REFERENCE: '%(prop:git_reference)s'
            GIT_BRANCH: '%(prop:branch)s'
            IS_LATEST_DEV: '%(prop:is_latest_dev:-unset)s'
            IS_LATEST_STABLE: '%(prop:is_latest_stable:-unset)s'
      - SetPropertyFromCommand:
          name: Set is_stable_version property
          property: is_latest_stable
          command: |
            if [ "$IS_LATEST_STABLE" != unset ]; then
                echo "$IS_LATEST_STABLE"
            elif [ "$IS_LATEST_DEV" = true ]; then
                echo false
            else
                LATEST_STABLE_RELEASE=$(
                    git ls-remote --tags --refs "$GIT_REFERENCE" | \
                    awk -F/ '$3 ~ /^[0-9]+\.[0-9]+\.[0-9]+$/ { print $3 }' | sort -V | tail -n 1
                )
                [ "$VERSION" = "$LATEST_STABLE_RELEASE" ] && echo true || echo false
            fi
          env:
            GIT_REFERENCE: '%(prop:git_reference)s'
            IS_LATEST_DEV: '%(prop:is_latest_dev)s'
            IS_LATEST_STABLE: '%(prop:is_latest_stable:-unset)s'
            VERSION: '%(prop:metalk8s_version)s'
      - SetProperty:
          name: Set is_dev property
          property: is_dev
          value: false
          doStepIf: '%(prop:use_production_registry:-false)s'
      - ShellCommand:
          name: Retrieve shell-ui image from pre-merge artifacts
          command: >
            curl -L -s -XGET -O '%(prop:premerge_artifacts_private_url)s/images/shell-ui.tar.gz'
          haltOnFailure: true
      - ShellCommand:
          name: Load shell-ui image
          command: docker load < shell-ui.tar.gz
          haltOnFailure: true
      - ShellCommand: &image_registry_tag_shell_ui
          name: Tag shell-ui image with current version
          command: |
            if [ "$USE_PRODUCTION_REGISTRY" = true ]; then
                PROJECT='%(secret:harbor_prod_project)s'
            else
                PROJECT='%(secret:harbor_dev_project)s'
            fi
            docker tag "$IMAGE_SRC" "$REGISTRY_HOST/$PROJECT/$IMAGE_DST"
          env: &_env_image_registry_tag_shell_ui
            <<: *_env_image_registry
            IMAGE_SRC: 'shell-ui:v%(prop:metalk8s_version)s'
            IMAGE_DST: 'shell-ui:v%(prop:metalk8s_version)s'
      - ShellCommand:
          <<: *image_registry_tag_shell_ui
          name: Tag shell-ui image with commit short revision
          env:
            <<: *_env_image_registry_tag_shell_ui
            IMAGE_DST: 'shell-ui:v%(prop:metalk8s_version)s-%(prop:commit_short_revision)s'
          doStepIf: '%(prop:is_dev:-true)s'
      - ShellCommand:
          <<: *image_registry_tag_shell_ui
          name: Tag shell-ui image as stable
          env:
            <<: *_env_image_registry_tag_shell_ui
            IMAGE_DST: 'shell-ui:stable'
          doStepIf: '%(prop:is_latest_stable)s'
      - ShellCommand:
          <<: *image_registry_tag_shell_ui
          name: Tag shell-ui image as latest
          env:
            <<: *_env_image_registry_tag_shell_ui
            IMAGE_DST: 'shell-ui:latest'
          doStepIf: '%(prop:is_latest_dev)s'
      - ShellCommand:
          name: Push shell-ui image to the remote registry
          command: |
            if [ "$USE_PRODUCTION_REGISTRY" = true ]; then
                PROJECT='%(secret:harbor_prod_project)s'
            else
                PROJECT='%(secret:harbor_dev_project)s'
            fi
            echo "The following $IMAGE_NAME tags will be pushed:"
            docker images "$REGISTRY_HOST/$PROJECT/$IMAGE_NAME"
            docker push "$REGISTRY_HOST/$PROJECT/$IMAGE_NAME"
          env:
            <<: *_env_image_registry
            IMAGE_NAME: shell-ui
