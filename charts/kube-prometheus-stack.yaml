nameOverride: prometheus-operator

kubeTargetVersionOverride: 1.21.0

commonLabels:
  # This needs to remain 'Tiller' for the render script to work properly
  #heritage: 'metalk8s'
  app.kubernetes.io/part-of: 'metalk8s'
  app.kubernetes.io/managed-by: 'metalk8s'
  metalk8s.scality.com/monitor: ''

alertmanager:
  alertmanagerSpec:
    image:
      repository: '__image__(alertmanager)'

    nodeSelector:
      node-role.kubernetes.io/infra: ''

    podAntiAffinity: 'soft'

    tolerations:
      - key: 'node-role.kubernetes.io/bootstrap'
        operator: 'Exists'
        effect: 'NoSchedule'
      - key: 'node-role.kubernetes.io/infra'
        operator: 'Exists'
        effect: 'NoSchedule'

    podMetadata:
      annotations:
        # Override default checksum as we want to manage it with salt
        checksum/config: '__slot__:salt:metalk8s_kubernetes.get_object_digest(kind="Secret", apiVersion="v1", namespace="metalk8s-monitoring", name="alertmanager-prometheus-operator-alertmanager", path="data:alertmanager.yaml")'

    replicas: '__var__(alertmanager.spec.deployment.replicas)'

    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: metalk8s
          accessModes: ['ReadWriteOnce']
          resources:
            requests:
              storage: 1Gi
          selector:
            matchLabels:
              app.kubernetes.io/name: prometheus-operator-alertmanager

  ## Passing the below configuration directives through the helm template
  ## engine leads to no substitution since the variables passed will be base64
  ## encoded directly hence we end up with `alertmanager.yaml` being invalid

  ## So let us disable the creation of this secret from the default charts
  ## and then create a new secret that replaces the
  ## alertmanager-prometheus-operator-alertmanager secret with new config
  ## read from a ConfigMap (metalk8s-alertmanager-config)

  # config: '__var_tojson__(alertmanager.spec.notification.config)'

    useExistingSecret: true
    secrets:
      - alertmanager-prometheus-operator-alertmanager

prometheusOperator:
  thanosImage:
    repository: '__image__(thanos)'

  prometheusConfigReloader:
    image:
      repository: '__image__(prometheus-config-reloader)'

  tls:
    enabled: false

  admissionWebhooks:
    enabled: false

  nodeSelector:
    node-role.kubernetes.io/infra: ''

  tolerations:
    - key: 'node-role.kubernetes.io/bootstrap'
      operator: 'Exists'
      effect: 'NoSchedule'
    - key: 'node-role.kubernetes.io/infra'
      operator: 'Exists'
      effect: 'NoSchedule'

  image:
    repository: '__image__(prometheus-operator)'

prometheus:
  thanosService:
    enabled: true

  prometheusSpec:
    thanos:
      image: '__full_image__(thanos)'

    image:
      repository: '__image__(prometheus)'

    tolerations:
      - key: 'node-role.kubernetes.io/bootstrap'
        operator: 'Exists'
        effect: 'NoSchedule'
      - key: 'node-role.kubernetes.io/infra'
        operator: 'Exists'
        effect: 'NoSchedule'

    replicas: '__var__(prometheus.spec.deployment.replicas)'

    nodeSelector:
      node-role.kubernetes.io/infra: ''

    serviceMonitorSelector:
      matchLabels:
        metalk8s.scality.com/monitor: ''

    probeSelector:
      matchLabels:
        metalk8s.scality.com/monitor: ''

    ruleSelector:
      matchLabels:
        metalk8s.scality.com/monitor: ''

    podAntiAffinity: 'soft'

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: metalk8s
          accessModes: ['ReadWriteOnce']
          resources:
            requests:
              storage: 10Gi
          selector:
            matchLabels:
              app.kubernetes.io/name: prometheus-operator-prometheus

    ## How long to retain metrics
    retention: '__var__(prometheus.spec.config.retention_time)'

    ## Maximum size of metrics
    retentionSize: '__escape__({{ prometheus.spec.config.retention_size | string }})'

    enableAdminAPI: '__var__(prometheus.spec.config.enable_admin_api)'

grafana:
  adminPassword: admin

  image:
    repository: '__image__(grafana)'
    tag: '8.3.1-ubuntu'

  sidecar:
    image:
      repository: '__image__(k8s-sidecar)'

    dashboards:
      searchNamespace: ALL
      folderAnnotation: metalk8s.scality.com/grafana-folder-name
      provider:
        foldersFromFilesStructure: true

    datasources:
      # Service deployed by Thanos
      url: http://thanos-query-http:10902/

  nodeSelector:
    node-role.kubernetes.io/infra: ''

  tolerations:
    - key: 'node-role.kubernetes.io/bootstrap'
      operator: 'Exists'
      effect: 'NoSchedule'
    - key: 'node-role.kubernetes.io/infra'
      operator: 'Exists'
      effect: 'NoSchedule'

  podAnnotations:
    # Override default checksum as we want to manage it with salt
    checksum/config: '__slot__:salt:metalk8s_kubernetes.get_object_digest(kind="ConfigMap", apiVersion="v1", namespace="metalk8s-monitoring", name="prometheus-operator-grafana", path="data:grafana.ini")'

  replicas: '__var__(grafana.spec.deployment.replicas)'

  ingress:
    enabled: true
    ingressClassName: nginx-control-plane
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /$2
    path: /grafana(/|$)(.*)
    hosts:
      - 'null'

  grafana.ini:
    server:
      root_url: '__escape__({{ "{{ salt.metalk8s_network.get_control_plane_ingress_endpoint() }}" }}/grafana)'
    analytics:
      reporting_enabled: false
      check_for_updates: false
    auth:
      oauth_auto_login: true
    auth.generic_oauth:
      enabled: true
      tls_skip_verify_insecure: true
      scopes: "openid profile email groups"
      client_id: "grafana-ui"
      client_secret: "4lqK98NcsWG5qBRHJUqYM1"
      auth_url: '__escape__({{ "{{ salt.metalk8s_network.get_control_plane_ingress_endpoint() }}" }}/oidc/auth)'
      token_url:  '__escape__({{ "{{ salt.metalk8s_network.get_control_plane_ingress_endpoint() }}" }}/oidc/token)'
      api_url: '__escape__({{ "{{ salt.metalk8s_network.get_control_plane_ingress_endpoint() }}" }}/oidc/userinfo)'
      role_attribute_path: >-
        contains(`{% endraw %}{{ "{{ dex.spec.config.staticPasswords | map(attribute='email') | list | tojson }}" }}{% raw %}`, email) && 'Admin'

  testFramework:
    enabled: false


kube-state-metrics:
  image:
    repository: '__image__(kube-state-metrics)'

  nodeSelector:
    node-role.kubernetes.io/infra: ''

  tolerations:
    - key: 'node-role.kubernetes.io/bootstrap'
      operator: 'Exists'
      effect: 'NoSchedule'
    - key: 'node-role.kubernetes.io/infra'
      operator: 'Exists'
      effect: 'NoSchedule'


prometheus-node-exporter:
  image:
    repository: '__image__(node-exporter)'
  extraArgs:
    - --collector.diskstats.ignored-devices=^(ram|fd)\\d+$
    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$


kubeEtcd:
  service:
    port: 2381
    targetPort: 2381

kubeScheduler:
  service:
    port: 10259
    targetPort: 10259
  serviceMonitor:
    https: true
    insecureSkipVerify: true

kubeControllerManager:
  service:
    port: 10257
    targetPort: 10257
  serviceMonitor:
    https: true
    insecureSkipVerify: true

kubelet:
  serviceMonitor:
    scrapeTimeout: "__var__(prometheus.spec.config.serviceMonitor.kubelet.scrapeTimeout)"
