{
    "data": {
        "groups": [
            {
                "evaluationTime": 0.002333024,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-alert-tree.rules-1a524e92-50f7-4d60-a402-ca39fe3e5c8b.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:05.391941769Z",
                "limit": 0,
                "name": "cluster-at-risk.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "AlertmanagerClusterCrashlooping{severity='critical'}, AlertmanagerClusterDown{severity='critical'}, AlertmanagerClusterFailedToSendAlerts{severity='critical'}, AlertmanagerConfigInconsistent{severity='critical'}, AlertmanagerMembersInconsistent{severity='critical'}, AlertmanagerFailedReload{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'AlertmanagerClusterCrashlooping' && @.labels.severity === 'critical') || (@.labels.alertname === 'AlertmanagerClusterDown' && @.labels.severity === 'critical') || (@.labels.alertname === 'AlertmanagerClusterFailedToSendAlerts' && @.labels.severity === 'critical') || (@.labels.alertname === 'AlertmanagerConfigInconsistent' && @.labels.severity === 'critical') || (@.labels.alertname === 'AlertmanagerMembersInconsistent' && @.labels.severity === 'critical') || (@.labels.alertname === 'AlertmanagerFailedReload' && @.labels.severity === 'critical'))]",
                            "summary": "The alerting service is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000186231,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.393940063Z",
                        "name": "AlertingServiceAtRisk",
                        "query": "sum(ALERTS{alertname=\"AlertmanagerClusterCrashlooping\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"AlertmanagerClusterDown\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"AlertmanagerClusterFailedToSendAlerts\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"AlertmanagerConfigInconsistent\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"AlertmanagerMembersInconsistent\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"AlertmanagerFailedReload\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "NodeAtRisk{severity='critical'}, PlatformServicesAtRisk{severity='critical'}, VolumeAtRisk{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'NodeAtRisk' && @.labels.severity === 'critical') || (@.labels.alertname === 'PlatformServicesAtRisk' && @.labels.severity === 'critical') || (@.labels.alertname === 'VolumeAtRisk' && @.labels.severity === 'critical'))]",
                            "summary": "The cluster is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000319636,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.391962832Z",
                        "name": "ClusterAtRisk",
                        "query": "sum(ALERTS{alertname=\"NodeAtRisk\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PlatformServicesAtRisk\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"VolumeAtRisk\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubernetesControlPlaneAtRisk{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubernetesControlPlaneAtRisk' && @.labels.severity === 'critical'))]",
                            "summary": "The Core services are at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 8.0796e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.392930509Z",
                        "name": "CoreServicesAtRisk",
                        "query": "sum(ALERTS{alertname=\"KubernetesControlPlaneAtRisk\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeAPIErrorBudgetBurn{severity='critical'}, etcdHighNumberOfFailedGRPCRequests{severity='critical'}, etcdGRPCRequestsSlow{severity='critical'}, etcdInsufficientMembers{severity='critical'}, etcdMembersDown{severity='critical'}, etcdNoLeader{severity='critical'}, etcdDatabaseQuotaLowSpace{severity='critical'}, etcdHighFsyncDurations{severity='critical'}, KubeStateMetricsListErrors{severity='critical'}, KubeStateMetricsWatchErrors{severity='critical'}, KubeAPIDown{severity='critical'}, KubeClientCertificateExpiration{severity='critical'}, KubeControllerManagerDown{severity='critical'}, KubeletDown{severity='critical'}, KubeSchedulerDown{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeAPIErrorBudgetBurn' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdHighNumberOfFailedGRPCRequests' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdGRPCRequestsSlow' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdInsufficientMembers' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdMembersDown' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdNoLeader' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdDatabaseQuotaLowSpace' && @.labels.severity === 'critical') || (@.labels.alertname === 'etcdHighFsyncDurations' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeStateMetricsListErrors' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeStateMetricsWatchErrors' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeAPIDown' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeClientCertificateExpiration' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeControllerManagerDown' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeletDown' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeSchedulerDown' && @.labels.severity === 'critical'))]",
                            "summary": "The Kubernetes control plane is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000582765,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.393013445Z",
                        "name": "KubernetesControlPlaneAtRisk",
                        "query": "sum(ALERTS{alertname=\"KubeAPIErrorBudgetBurn\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdHighNumberOfFailedGRPCRequests\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdGRPCRequestsSlow\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdInsufficientMembers\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdMembersDown\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdNoLeader\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdDatabaseQuotaLowSpace\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"etcdHighFsyncDurations\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeStateMetricsListErrors\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeStateMetricsWatchErrors\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeAPIDown\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeClientCertificateExpiration\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeControllerManagerDown\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeletDown\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeSchedulerDown\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeStateMetricsShardingMismatch{severity='critical'}, KubeStateMetricsShardsMissing{severity='critical'}, PrometheusRuleFailures{severity='critical'}, PrometheusRemoteWriteBehind{severity='critical'}, PrometheusRemoteStorageFailures{severity='critical'}, PrometheusErrorSendingAlertsToAnyAlertmanager{severity='critical'}, PrometheusBadConfig{severity='critical'}, PrometheusTargetSyncFailure{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeStateMetricsShardingMismatch' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubeStateMetricsShardsMissing' && @.labels.severity === 'critical') || (@.labels.alertname === 'PrometheusRuleFailures' && @.labels.severity === 'critical') || (@.labels.alertname === 'PrometheusRemoteWriteBehind' && @.labels.severity === 'critical') || (@.labels.alertname === 'PrometheusRemoteStorageFailures' && @.labels.severity === 'critical') || (@.labels.alertname === 'PrometheusErrorSendingAlertsToAnyAlertmanager' && @.labels.severity === 'critical') || (@.labels.alertname === 'PrometheusBadConfig' && @.labels.severity === 'critical') || (@.labels.alertname === 'PrometheusTargetSyncFailure' && @.labels.severity === 'critical'))]",
                            "summary": "The monitoring service is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000238666,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.393698639Z",
                        "name": "MonitoringServiceAtRisk",
                        "query": "sum(ALERTS{alertname=\"KubeStateMetricsShardingMismatch\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubeStateMetricsShardsMissing\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PrometheusRuleFailures\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PrometheusRemoteWriteBehind\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PrometheusRemoteStorageFailures\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PrometheusErrorSendingAlertsToAnyAlertmanager\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PrometheusBadConfig\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"PrometheusTargetSyncFailure\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeletClientCertificateExpiration{severity='critical'}, NodeRAIDDegraded{severity='critical'}, SystemPartitionAtRisk{severity='critical'}",
                            "childrenJsonPath": "$[?(((@.labels.alertname === 'KubeletClientCertificateExpiration' && @.labels.severity === 'critical') || (@.labels.alertname === 'NodeRAIDDegraded' && @.labels.severity === 'critical') || (@.labels.alertname === 'SystemPartitionAtRisk' && @.labels.severity === 'critical')) && (@.labels.instance === '{{ $labels.instance }}'))]",
                            "summary": "The node {{ $labels.instance }} is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.00019549,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.392286519Z",
                        "name": "NodeAtRisk",
                        "query": "sum by (instance) (ALERTS{alertname=\"KubeletClientCertificateExpiration\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"NodeRAIDDegraded\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"SystemPartitionAtRisk\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "MonitoringServiceAtRisk{severity='critical'}, AlertingServiceAtRisk{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'MonitoringServiceAtRisk' && @.labels.severity === 'critical') || (@.labels.alertname === 'AlertingServiceAtRisk' && @.labels.severity === 'critical'))]",
                            "summary": "The observability services are at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 9.6217e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.393599007Z",
                        "name": "ObservabilityServicesAtRisk",
                        "query": "sum(ALERTS{alertname=\"MonitoringServiceAtRisk\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"AlertingServiceAtRisk\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "CoreServicesAtRisk{severity='critical'}, ObservabilityServicesAtRisk{severity='critical'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'CoreServicesAtRisk' && @.labels.severity === 'critical') || (@.labels.alertname === 'ObservabilityServicesAtRisk' && @.labels.severity === 'critical'))]",
                            "summary": "The Platform services are at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000136785,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.39279065Z",
                        "name": "PlatformServicesAtRisk",
                        "query": "sum(ALERTS{alertname=\"CoreServicesAtRisk\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"ObservabilityServicesAtRisk\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "NodeFileDescriptorLimit{severity='critical'}, NodeFilesystemAlmostOutOfSpace{severity='critical'}, NodeFilesystemAlmostOutOfFiles{severity='critical'}, NodeFilesystemFilesFillingUp{severity='critical'}, NodeFilesystemSpaceFillingUp{severity='critical'}",
                            "childrenJsonPath": "$[?(((@.labels.alertname === 'NodeFileDescriptorLimit' && @.labels.severity === 'critical') || (@.labels.alertname === 'NodeFilesystemAlmostOutOfSpace' && @.labels.severity === 'critical') || (@.labels.alertname === 'NodeFilesystemAlmostOutOfFiles' && @.labels.severity === 'critical') || (@.labels.alertname === 'NodeFilesystemFilesFillingUp' && @.labels.severity === 'critical') || (@.labels.alertname === 'NodeFilesystemSpaceFillingUp' && @.labels.severity === 'critical')) && (@.labels.mountpoint === '{{ $labels.mountpoint }}' && @.labels.instance === '{{ $labels.instance }}'))]",
                            "summary": "The system partition {{ $labels.mountpoint }} on node {{ $labels.instance }} is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000303147,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.392484846Z",
                        "name": "SystemPartitionAtRisk",
                        "query": "sum by (mountpoint, instance) (ALERTS{alertname=\"NodeFileDescriptorLimit\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"NodeFilesystemAlmostOutOfSpace\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"NodeFilesystemAlmostOutOfFiles\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"NodeFilesystemFilesFillingUp\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"NodeFilesystemSpaceFillingUp\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubePersistentVolumeFillingUp{severity='critical'}, KubePersistentVolumeErrors{severity='critical'}",
                            "childrenJsonPath": "$[?(((@.labels.alertname === 'KubePersistentVolumeFillingUp' && @.labels.severity === 'critical') || (@.labels.alertname === 'KubePersistentVolumeErrors' && @.labels.severity === 'critical')) && (@.labels.persistentvolumeclaim === '{{ $labels.persistentvolumeclaim }}' && @.labels.namespace === '{{ $labels.namespace }}' && @.labels.instance === '{{ $labels.instance }}'))]",
                            "summary": "The volume {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} on node {{ $labels.instance }} is at risk."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000132155,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.394128507Z",
                        "name": "VolumeAtRisk",
                        "query": "sum by (persistentvolumeclaim, namespace, instance) (ALERTS{alertname=\"KubePersistentVolumeFillingUp\",alertstate=\"firing\",severity=\"critical\"} or ALERTS{alertname=\"KubePersistentVolumeErrors\",alertstate=\"firing\",severity=\"critical\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.006322052,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-alert-tree.rules-1a524e92-50f7-4d60-a402-ca39fe3e5c8b.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:51.855957114Z",
                "limit": 0,
                "name": "cluster-degraded.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "AuthenticationServiceDegraded{severity='warning'}, IngressControllerServicesDegraded{severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'AuthenticationServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'IngressControllerServicesDegraded' && @.labels.severity === 'warning'))]",
                            "summary": "The Access services are degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000100579,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.858056066Z",
                        "name": "AccessServicesDegraded",
                        "query": "sum(ALERTS{alertname=\"AuthenticationServiceDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"IngressControllerServicesDegraded\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "AlertmanagerClusterFailedToSendAlerts{severity='warning'}, AlertmanagerFailedToSendAlerts{severity='warning'}, KubeStatefulSetReplicasMismatch{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'alertmanager-prometheus-operator-alertmanager'}, KubeStatefulSetGenerationMismatch{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'alertmanager-prometheus-operator-alertmanager'}, KubeStatefulSetUpdateNotRolledOut{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'alertmanager-prometheus-operator-alertmanager'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'AlertmanagerClusterFailedToSendAlerts' && @.labels.severity === 'warning') || (@.labels.alertname === 'AlertmanagerFailedToSendAlerts' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeStatefulSetReplicasMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:alertmanager-prometheus-operator-alertmanager)$'))) || (@.labels.alertname === 'KubeStatefulSetGenerationMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:alertmanager-prometheus-operator-alertmanager)$'))) || (@.labels.alertname === 'KubeStatefulSetUpdateNotRolledOut' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:alertmanager-prometheus-operator-alertmanager)$'))))]",
                            "summary": "The alerting service is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000253046,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.861584794Z",
                        "name": "AlertingServiceDegraded",
                        "query": "sum(ALERTS{alertname=\"AlertmanagerClusterFailedToSendAlerts\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"AlertmanagerFailedToSendAlerts\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeStatefulSetReplicasMismatch\",alertstate=\"firing\",namespace=~\"metalk8s-monitoring\",severity=\"warning\",statefulset=~\"alertmanager-prometheus-operator-alertmanager\"} or ALERTS{alertname=\"KubeStatefulSetGenerationMismatch\",alertstate=\"firing\",namespace=~\"metalk8s-monitoring\",severity=\"warning\",statefulset=~\"alertmanager-prometheus-operator-alertmanager\"} or ALERTS{alertname=\"KubeStatefulSetUpdateNotRolledOut\",alertstate=\"firing\",namespace=~\"metalk8s-monitoring\",severity=\"warning\",statefulset=~\"alertmanager-prometheus-operator-alertmanager\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeDeploymentReplicasMismatch{deployment=~'dex', namespace=~'metalk8s-auth', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'dex', namespace=~'metalk8s-auth', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:dex)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-auth)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:dex)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-auth)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The Authentication service for K8S API is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000124361,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.858161006Z",
                        "name": "AuthenticationServiceDegraded",
                        "query": "sum(ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"dex\",namespace=~\"metalk8s-auth\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"dex\",namespace=~\"metalk8s-auth\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubePodNotReady{namespace=~'kube-system', pod=~'repositories-.*', severity='warning'}, KubePodCrashLooping{namespace=~'kube-system', pod=~'repositories-.*', severity='warning'}, KubePodNotReady{namespace=~'kube-system', pod=~'salt-master-.*', severity='warning'}, KubePodCrashLooping{namespace=~'kube-system', pod=~'salt-master-.*', severity='warning'}, KubeDeploymentReplicasMismatch{deployment=~'storage-operator', namespace=~'kube-system', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'storage-operator', namespace=~'kube-system', severity='warning'}, KubeDeploymentReplicasMismatch{deployment=~'metalk8s-ui', namespace=~'metalk8s-ui', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'metalk8s-ui', namespace=~'metalk8s-ui', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubePodNotReady' && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.pod.match(new RegExp('^(?:repositories-.*)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubePodCrashLooping' && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.pod.match(new RegExp('^(?:repositories-.*)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubePodNotReady' && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.pod.match(new RegExp('^(?:salt-master-.*)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubePodCrashLooping' && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.pod.match(new RegExp('^(?:salt-master-.*)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:storage-operator)$')) && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:storage-operator)$')) && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:metalk8s-ui)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ui)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:metalk8s-ui)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ui)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The MetalK8s Bootstrap services are degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000429237,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.859504811Z",
                        "name": "BootstrapServicesDegraded",
                        "query": "sum(ALERTS{alertname=\"KubePodNotReady\",alertstate=\"firing\",namespace=~\"kube-system\",pod=~\"repositories-.*\",severity=\"warning\"} or ALERTS{alertname=\"KubePodCrashLooping\",alertstate=\"firing\",namespace=~\"kube-system\",pod=~\"repositories-.*\",severity=\"warning\"} or ALERTS{alertname=\"KubePodNotReady\",alertstate=\"firing\",namespace=~\"kube-system\",pod=~\"salt-master-.*\",severity=\"warning\"} or ALERTS{alertname=\"KubePodCrashLooping\",alertstate=\"firing\",namespace=~\"kube-system\",pod=~\"salt-master-.*\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"storage-operator\",namespace=~\"kube-system\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"storage-operator\",namespace=~\"kube-system\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"metalk8s-ui\",namespace=~\"metalk8s-ui\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"metalk8s-ui\",namespace=~\"metalk8s-ui\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T08:13:21.854583701Z",
                                "annotations": {
                                    "children": "NetworkDegraded{severity='warning'}, NodeDegraded{severity='warning'}, PlatformServicesDegraded{severity='warning'}, VolumeDegraded{severity='warning'}",
                                    "childrenJsonPath": "$[?((@.labels.alertname === 'NetworkDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'PlatformServicesDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'VolumeDegraded' && @.labels.severity === 'warning'))]",
                                    "summary": "The cluster is degraded."
                                },
                                "labels": {
                                    "alertname": "ClusterDegraded",
                                    "severity": "warning"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "children": "NetworkDegraded{severity='warning'}, NodeDegraded{severity='warning'}, PlatformServicesDegraded{severity='warning'}, VolumeDegraded{severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'NetworkDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'PlatformServicesDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'VolumeDegraded' && @.labels.severity === 'warning'))]",
                            "summary": "The cluster is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000686381,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.855983771Z",
                        "name": "ClusterDegraded",
                        "query": "sum(ALERTS{alertname=\"NetworkDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PlatformServicesDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"VolumeDegraded\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubernetesControlPlaneDegraded{severity='warning'}, BootstrapServicesDegraded{severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubernetesControlPlaneDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'BootstrapServicesDegraded' && @.labels.severity === 'warning'))]",
                            "summary": "The Core services are degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 9.0075e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.858665497Z",
                        "name": "CoreServicesDegraded",
                        "query": "sum(ALERTS{alertname=\"KubernetesControlPlaneDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"BootstrapServicesDegraded\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeDeploymentReplicasMismatch{deployment=~'prometheus-operator-grafana', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'prometheus-operator-grafana', namespace=~'metalk8s-monitoring', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-grafana)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-grafana)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The dashboarding service is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000116433,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.862069153Z",
                        "name": "DashboardingServiceDegraded",
                        "query": "sum(ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"prometheus-operator-grafana\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"prometheus-operator-grafana\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeDeploymentReplicasMismatch{deployment=~'ingress-nginx-defaultbackend', namespace=~'metalk8s-ingress', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'ingress-nginx-defaultbackend', namespace=~'metalk8s-ingress', severity='warning'}, KubeDaemonSetNotScheduled{daemonset=~'ingress-nginx-controller', namespace=~'metalk8s-ingress', severity='warning'}, KubeDaemonSetMisScheduled{daemonset=~'ingress-nginx-controller', namespace=~'metalk8s-ingress', severity='warning'}, KubeDaemonSetRolloutStuck{daemonset=~'ingress-nginx-controller', namespace=~'metalk8s-ingress', severity='warning'}, KubeDaemonSetNotScheduled{daemonset=~'ingress-nginx-control-plane-controller', namespace=~'metalk8s-ingress', severity='warning'}, KubeDaemonSetMisScheduled{daemonset=~'ingress-nginx-control-plane-controller', namespace=~'metalk8s-ingress', severity='warning'}, KubeDaemonSetRolloutStuck{daemonset=~'ingress-nginx-control-plane-controller', namespace=~'metalk8s-ingress', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:ingress-nginx-defaultbackend)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:ingress-nginx-defaultbackend)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetNotScheduled' && @.labels.daemonset.match(new RegExp('^(?:ingress-nginx-controller)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetMisScheduled' && @.labels.daemonset.match(new RegExp('^(?:ingress-nginx-controller)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetRolloutStuck' && @.labels.daemonset.match(new RegExp('^(?:ingress-nginx-controller)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetNotScheduled' && @.labels.daemonset.match(new RegExp('^(?:ingress-nginx-control-plane-controller)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetMisScheduled' && @.labels.daemonset.match(new RegExp('^(?:ingress-nginx-control-plane-controller)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetRolloutStuck' && @.labels.daemonset.match(new RegExp('^(?:ingress-nginx-control-plane-controller)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-ingress)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The Ingress Controllers for control plane and workload plane are degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000375249,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.858287795Z",
                        "name": "IngressControllerServicesDegraded",
                        "query": "sum(ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"ingress-nginx-defaultbackend\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"ingress-nginx-defaultbackend\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetNotScheduled\",alertstate=\"firing\",daemonset=~\"ingress-nginx-controller\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetMisScheduled\",alertstate=\"firing\",daemonset=~\"ingress-nginx-controller\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetRolloutStuck\",alertstate=\"firing\",daemonset=~\"ingress-nginx-controller\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetNotScheduled\",alertstate=\"firing\",daemonset=~\"ingress-nginx-control-plane-controller\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetMisScheduled\",alertstate=\"firing\",daemonset=~\"ingress-nginx-control-plane-controller\",namespace=~\"metalk8s-ingress\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetRolloutStuck\",alertstate=\"firing\",daemonset=~\"ingress-nginx-control-plane-controller\",namespace=~\"metalk8s-ingress\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeAPIErrorBudgetBurn{severity='warning'}, KubeAPITerminatedRequests{severity='warning'}, etcdHighNumberOfFailedGRPCRequests{severity='warning'}, etcdHighCommitDurations{severity='warning'}, etcdHighFsyncDurations{severity='warning'}, etcdHighNumberOfFailedProposals{severity='warning'}, etcdHighNumberOfLeaderChanges{severity='warning'}, etcdMemberCommunicationSlow{severity='warning'}, etcdExcessiveDatabaseGrowth{severity='warning'}, KubeCPUOvercommit{severity='warning'}, KubeCPUQuotaOvercommit{severity='warning'}, KubeMemoryOvercommit{severity='warning'}, KubeMemoryQuotaOvercommit{severity='warning'}, KubeClientCertificateExpiration{severity='warning'}, KubeClientErrors{severity='warning'}, KubeVersionMismatch{severity='warning'}, KubeDeploymentReplicasMismatch{deployment=~'coredns', namespace=~'kube-system', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'coredns', namespace=~'kube-system', severity='warning'}, KubeDeploymentReplicasMismatch{deployment=~'prometheus-adapter', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'prometheus-adapter', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDeploymentReplicasMismatch{deployment=~'prometheus-operator-kube-state-metrics', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'prometheus-operator-kube-state-metrics', namespace=~'metalk8s-monitoring', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeAPIErrorBudgetBurn' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeAPITerminatedRequests' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdHighNumberOfFailedGRPCRequests' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdHighCommitDurations' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdHighFsyncDurations' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdHighNumberOfFailedProposals' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdHighNumberOfLeaderChanges' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdMemberCommunicationSlow' && @.labels.severity === 'warning') || (@.labels.alertname === 'etcdExcessiveDatabaseGrowth' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeCPUOvercommit' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeCPUQuotaOvercommit' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeMemoryOvercommit' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeMemoryQuotaOvercommit' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeClientCertificateExpiration' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeClientErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeVersionMismatch' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:coredns)$')) && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:coredns)$')) && @.labels.namespace.match(new RegExp('^(?:kube-system)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-adapter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-adapter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-kube-state-metrics)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-kube-state-metrics)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The Kubernetes control plane is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000743462,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.858758162Z",
                        "name": "KubernetesControlPlaneDegraded",
                        "query": "sum(ALERTS{alertname=\"KubeAPIErrorBudgetBurn\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeAPITerminatedRequests\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdHighNumberOfFailedGRPCRequests\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdHighCommitDurations\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdHighFsyncDurations\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdHighNumberOfFailedProposals\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdHighNumberOfLeaderChanges\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdMemberCommunicationSlow\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"etcdExcessiveDatabaseGrowth\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeCPUOvercommit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeCPUQuotaOvercommit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeMemoryOvercommit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeMemoryQuotaOvercommit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeClientCertificateExpiration\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeClientErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeVersionMismatch\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"coredns\",namespace=~\"kube-system\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"coredns\",namespace=~\"kube-system\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"prometheus-adapter\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"prometheus-adapter\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"prometheus-operator-kube-state-metrics\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"prometheus-operator-kube-state-metrics\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeStatefulSetReplicasMismatch{namespace=~'metalk8s-logging', severity='warning', statefulset=~'loki'}, KubeStatefulSetGenerationMismatch{namespace=~'metalk8s-logging', severity='warning', statefulset=~'loki'}, KubeStatefulSetUpdateNotRolledOut{namespace=~'metalk8s-logging', severity='warning', statefulset=~'loki'}, KubeDaemonSetNotScheduled{daemonset=~'fluentbit', namespace=~'metalk8s-logging', severity='warning'}, KubeDaemonSetMisScheduled{daemonset=~'fluentbit', namespace=~'metalk8s-logging', severity='warning'}, KubeDaemonSetRolloutStuck{daemonset=~'fluentbit', namespace=~'metalk8s-logging', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'KubeStatefulSetReplicasMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-logging)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:loki)$'))) || (@.labels.alertname === 'KubeStatefulSetGenerationMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-logging)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:loki)$'))) || (@.labels.alertname === 'KubeStatefulSetUpdateNotRolledOut' && @.labels.namespace.match(new RegExp('^(?:metalk8s-logging)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:loki)$'))) || (@.labels.alertname === 'KubeDaemonSetNotScheduled' && @.labels.daemonset.match(new RegExp('^(?:fluentbit)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-logging)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetMisScheduled' && @.labels.daemonset.match(new RegExp('^(?:fluentbit)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-logging)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetRolloutStuck' && @.labels.daemonset.match(new RegExp('^(?:fluentbit)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-logging)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The logging service is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000226303,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.861840397Z",
                        "name": "LoggingServiceDegraded",
                        "query": "sum(ALERTS{alertname=\"KubeStatefulSetReplicasMismatch\",alertstate=\"firing\",namespace=~\"metalk8s-logging\",severity=\"warning\",statefulset=~\"loki\"} or ALERTS{alertname=\"KubeStatefulSetGenerationMismatch\",alertstate=\"firing\",namespace=~\"metalk8s-logging\",severity=\"warning\",statefulset=~\"loki\"} or ALERTS{alertname=\"KubeStatefulSetUpdateNotRolledOut\",alertstate=\"firing\",namespace=~\"metalk8s-logging\",severity=\"warning\",statefulset=~\"loki\"} or ALERTS{alertname=\"KubeDaemonSetNotScheduled\",alertstate=\"firing\",daemonset=~\"fluentbit\",namespace=~\"metalk8s-logging\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetMisScheduled\",alertstate=\"firing\",daemonset=~\"fluentbit\",namespace=~\"metalk8s-logging\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetRolloutStuck\",alertstate=\"firing\",daemonset=~\"fluentbit\",namespace=~\"metalk8s-logging\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T08:08:51.854583701Z",
                                "annotations": {
                                    "children": "PrometheusLabelLimitHit{severity='warning'}, PrometheusTargetLimitHit{severity='warning'}, PrometheusTSDBReloadsFailing{severity='warning'}, PrometheusTSDBCompactionsFailing{severity='warning'}, PrometheusRemoteWriteDesiredShards{severity='warning'}, PrometheusOutOfOrderTimestamps{severity='warning'}, PrometheusNotificationQueueRunningFull{severity='warning'}, PrometheusNotIngestingSamples{severity='warning'}, PrometheusNotConnectedToAlertmanagers{severity='warning'}, PrometheusMissingRuleEvaluations{severity='warning'}, PrometheusErrorSendingAlertsToSomeAlertmanagers{severity='warning'}, PrometheusDuplicateTimestamps{severity='warning'}, PrometheusOperatorWatchErrors{severity='warning'}, PrometheusOperatorSyncFailed{severity='warning'}, PrometheusOperatorRejectedResources{severity='warning'}, PrometheusOperatorReconcileErrors{severity='warning'}, PrometheusOperatorNotReady{severity='warning'}, PrometheusOperatorNodeLookupErrors{severity='warning'}, PrometheusOperatorListErrors{severity='warning'}, PrometheusHighQueryLoad{severity='warning'}, KubeStatefulSetReplicasMismatch{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'prometheus-prometheus-operator-prometheus'}, KubeStatefulSetGenerationMismatch{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'prometheus-prometheus-operator-prometheus'}, KubeStatefulSetUpdateNotRolledOut{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'prometheus-prometheus-operator-prometheus'}, KubeDeploymentReplicasMismatch{deployment=~'prometheus-operator-operator', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'prometheus-operator-operator', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDaemonSetNotScheduled{daemonset=~'prometheus-operator-prometheus-node-exporter', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDaemonSetMisScheduled{daemonset=~'prometheus-operator-prometheus-node-exporter', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDaemonSetRolloutStuck{daemonset=~'prometheus-operator-prometheus-node-exporter', namespace=~'metalk8s-monitoring', severity='warning'}",
                                    "childrenJsonPath": "$[?((@.labels.alertname === 'PrometheusLabelLimitHit' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusTargetLimitHit' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusTSDBReloadsFailing' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusTSDBCompactionsFailing' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusRemoteWriteDesiredShards' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOutOfOrderTimestamps' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusNotificationQueueRunningFull' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusNotIngestingSamples' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusNotConnectedToAlertmanagers' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusMissingRuleEvaluations' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusErrorSendingAlertsToSomeAlertmanagers' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusDuplicateTimestamps' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorWatchErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorSyncFailed' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorRejectedResources' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorReconcileErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorNotReady' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorNodeLookupErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorListErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusHighQueryLoad' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeStatefulSetReplicasMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:prometheus-prometheus-operator-prometheus)$'))) || (@.labels.alertname === 'KubeStatefulSetGenerationMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:prometheus-prometheus-operator-prometheus)$'))) || (@.labels.alertname === 'KubeStatefulSetUpdateNotRolledOut' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:prometheus-prometheus-operator-prometheus)$'))) || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-operator)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-operator)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetNotScheduled' && @.labels.daemonset.match(new RegExp('^(?:prometheus-operator-prometheus-node-exporter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetMisScheduled' && @.labels.daemonset.match(new RegExp('^(?:prometheus-operator-prometheus-node-exporter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetRolloutStuck' && @.labels.daemonset.match(new RegExp('^(?:prometheus-operator-prometheus-node-exporter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning'))]",
                                    "summary": "The monitoring service is degraded."
                                },
                                "labels": {
                                    "alertname": "MonitoringServiceDegraded",
                                    "severity": "warning"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "children": "PrometheusLabelLimitHit{severity='warning'}, PrometheusTargetLimitHit{severity='warning'}, PrometheusTSDBReloadsFailing{severity='warning'}, PrometheusTSDBCompactionsFailing{severity='warning'}, PrometheusRemoteWriteDesiredShards{severity='warning'}, PrometheusOutOfOrderTimestamps{severity='warning'}, PrometheusNotificationQueueRunningFull{severity='warning'}, PrometheusNotIngestingSamples{severity='warning'}, PrometheusNotConnectedToAlertmanagers{severity='warning'}, PrometheusMissingRuleEvaluations{severity='warning'}, PrometheusErrorSendingAlertsToSomeAlertmanagers{severity='warning'}, PrometheusDuplicateTimestamps{severity='warning'}, PrometheusOperatorWatchErrors{severity='warning'}, PrometheusOperatorSyncFailed{severity='warning'}, PrometheusOperatorRejectedResources{severity='warning'}, PrometheusOperatorReconcileErrors{severity='warning'}, PrometheusOperatorNotReady{severity='warning'}, PrometheusOperatorNodeLookupErrors{severity='warning'}, PrometheusOperatorListErrors{severity='warning'}, PrometheusHighQueryLoad{severity='warning'}, KubeStatefulSetReplicasMismatch{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'prometheus-prometheus-operator-prometheus'}, KubeStatefulSetGenerationMismatch{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'prometheus-prometheus-operator-prometheus'}, KubeStatefulSetUpdateNotRolledOut{namespace=~'metalk8s-monitoring', severity='warning', statefulset=~'prometheus-prometheus-operator-prometheus'}, KubeDeploymentReplicasMismatch{deployment=~'prometheus-operator-operator', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDeploymentGenerationMismatch{deployment=~'prometheus-operator-operator', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDaemonSetNotScheduled{daemonset=~'prometheus-operator-prometheus-node-exporter', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDaemonSetMisScheduled{daemonset=~'prometheus-operator-prometheus-node-exporter', namespace=~'metalk8s-monitoring', severity='warning'}, KubeDaemonSetRolloutStuck{daemonset=~'prometheus-operator-prometheus-node-exporter', namespace=~'metalk8s-monitoring', severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'PrometheusLabelLimitHit' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusTargetLimitHit' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusTSDBReloadsFailing' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusTSDBCompactionsFailing' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusRemoteWriteDesiredShards' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOutOfOrderTimestamps' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusNotificationQueueRunningFull' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusNotIngestingSamples' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusNotConnectedToAlertmanagers' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusMissingRuleEvaluations' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusErrorSendingAlertsToSomeAlertmanagers' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusDuplicateTimestamps' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorWatchErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorSyncFailed' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorRejectedResources' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorReconcileErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorNotReady' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorNodeLookupErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusOperatorListErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'PrometheusHighQueryLoad' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeStatefulSetReplicasMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:prometheus-prometheus-operator-prometheus)$'))) || (@.labels.alertname === 'KubeStatefulSetGenerationMismatch' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:prometheus-prometheus-operator-prometheus)$'))) || (@.labels.alertname === 'KubeStatefulSetUpdateNotRolledOut' && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning' && @.labels.statefulset.match(new RegExp('^(?:prometheus-prometheus-operator-prometheus)$'))) || (@.labels.alertname === 'KubeDeploymentReplicasMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-operator)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDeploymentGenerationMismatch' && @.labels.deployment.match(new RegExp('^(?:prometheus-operator-operator)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetNotScheduled' && @.labels.daemonset.match(new RegExp('^(?:prometheus-operator-prometheus-node-exporter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetMisScheduled' && @.labels.daemonset.match(new RegExp('^(?:prometheus-operator-prometheus-node-exporter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeDaemonSetRolloutStuck' && @.labels.daemonset.match(new RegExp('^(?:prometheus-operator-prometheus-node-exporter)$')) && @.labels.namespace.match(new RegExp('^(?:metalk8s-monitoring)$')) && @.labels.severity === 'warning'))]",
                            "summary": "The monitoring service is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.001310532,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.860270355Z",
                        "name": "MonitoringServiceDegraded",
                        "query": "sum(ALERTS{alertname=\"PrometheusLabelLimitHit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusTargetLimitHit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusTSDBReloadsFailing\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusTSDBCompactionsFailing\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusRemoteWriteDesiredShards\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOutOfOrderTimestamps\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusNotificationQueueRunningFull\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusNotIngestingSamples\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusNotConnectedToAlertmanagers\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusMissingRuleEvaluations\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusErrorSendingAlertsToSomeAlertmanagers\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusDuplicateTimestamps\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorWatchErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorSyncFailed\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorRejectedResources\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorReconcileErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorNotReady\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorNodeLookupErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusOperatorListErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"PrometheusHighQueryLoad\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeStatefulSetReplicasMismatch\",alertstate=\"firing\",namespace=~\"metalk8s-monitoring\",severity=\"warning\",statefulset=~\"prometheus-prometheus-operator-prometheus\"} or ALERTS{alertname=\"KubeStatefulSetGenerationMismatch\",alertstate=\"firing\",namespace=~\"metalk8s-monitoring\",severity=\"warning\",statefulset=~\"prometheus-prometheus-operator-prometheus\"} or ALERTS{alertname=\"KubeStatefulSetUpdateNotRolledOut\",alertstate=\"firing\",namespace=~\"metalk8s-monitoring\",severity=\"warning\",statefulset=~\"prometheus-prometheus-operator-prometheus\"} or ALERTS{alertname=\"KubeDeploymentReplicasMismatch\",alertstate=\"firing\",deployment=~\"prometheus-operator-operator\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDeploymentGenerationMismatch\",alertstate=\"firing\",deployment=~\"prometheus-operator-operator\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetNotScheduled\",alertstate=\"firing\",daemonset=~\"prometheus-operator-prometheus-node-exporter\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetMisScheduled\",alertstate=\"firing\",daemonset=~\"prometheus-operator-prometheus-node-exporter\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"} or ALERTS{alertname=\"KubeDaemonSetRolloutStuck\",alertstate=\"firing\",daemonset=~\"prometheus-operator-prometheus-node-exporter\",namespace=~\"metalk8s-monitoring\",severity=\"warning\"}) >= 1",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "NodeNetworkReceiveErrs{severity='warning'}, NodeHighNumberConntrackEntriesUsed{severity='warning'}, NodeNetworkTransmitErrs{severity='warning'}, NodeNetworkInterfaceFlapping{severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'NodeNetworkReceiveErrs' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeHighNumberConntrackEntriesUsed' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeNetworkTransmitErrs' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeNetworkInterfaceFlapping' && @.labels.severity === 'warning'))]",
                            "summary": "The network is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000183599,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.856674931Z",
                        "name": "NetworkDegraded",
                        "query": "sum(ALERTS{alertname=\"NodeNetworkReceiveErrs\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeHighNumberConntrackEntriesUsed\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeNetworkTransmitErrs\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeNetworkInterfaceFlapping\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubeNodeNotReady{severity='warning'}, KubeNodeReadinessFlapping{severity='warning'}, KubeNodeUnreachable{severity='warning'}, KubeletClientCertificateExpiration{severity='warning'}, KubeletClientCertificateRenewalErrors{severity='warning'}, KubeletPlegDurationHigh{severity='warning'}, KubeletPodStartUpLatencyHigh{severity='warning'}, KubeletServerCertificateExpiration{severity='warning'}, KubeletServerCertificateRenewalErrors{severity='warning'}, NodeClockNotSynchronising{severity='warning'}, NodeClockSkewDetected{severity='warning'}, NodeRAIDDiskFailure{severity='warning'}, NodeTextFileCollectorScrapeError{severity='warning'}, SystemPartitionDegraded{severity='warning'}",
                            "childrenJsonPath": "$[?(((@.labels.alertname === 'KubeNodeNotReady' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeNodeReadinessFlapping' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeNodeUnreachable' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeletClientCertificateExpiration' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeletClientCertificateRenewalErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeletPlegDurationHigh' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeletPodStartUpLatencyHigh' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeletServerCertificateExpiration' && @.labels.severity === 'warning') || (@.labels.alertname === 'KubeletServerCertificateRenewalErrors' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeClockNotSynchronising' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeClockSkewDetected' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeRAIDDiskFailure' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeTextFileCollectorScrapeError' && @.labels.severity === 'warning') || (@.labels.alertname === 'SystemPartitionDegraded' && @.labels.severity === 'warning')) && (@.labels.instance === '{{ $labels.instance }}'))]",
                            "summary": "The node {{ $labels.instance }} is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000676742,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.85686166Z",
                        "name": "NodeDegraded",
                        "query": "sum by (instance) (ALERTS{alertname=\"KubeNodeNotReady\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeNodeReadinessFlapping\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeNodeUnreachable\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeletClientCertificateExpiration\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeletClientCertificateRenewalErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeletPlegDurationHigh\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeletPodStartUpLatencyHigh\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeletServerCertificateExpiration\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"KubeletServerCertificateRenewalErrors\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeClockNotSynchronising\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeClockSkewDetected\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeRAIDDiskFailure\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeTextFileCollectorScrapeError\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"SystemPartitionDegraded\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T08:10:21.854583701Z",
                                "annotations": {
                                    "children": "MonitoringServiceDegraded{severity='warning'}, AlertingServiceDegraded{severity='warning'}, LoggingServiceDegraded{severity='warning'}, DashboardingServiceDegraded{severity='warning'}",
                                    "childrenJsonPath": "$[?((@.labels.alertname === 'MonitoringServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'AlertingServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'LoggingServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'DashboardingServiceDegraded' && @.labels.severity === 'warning'))]",
                                    "summary": "The observability services are degraded."
                                },
                                "labels": {
                                    "alertname": "ObservabilityServicesDegraded",
                                    "severity": "warning"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "children": "MonitoringServiceDegraded{severity='warning'}, AlertingServiceDegraded{severity='warning'}, LoggingServiceDegraded{severity='warning'}, DashboardingServiceDegraded{severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'MonitoringServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'AlertingServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'LoggingServiceDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'DashboardingServiceDegraded' && @.labels.severity === 'warning'))]",
                            "summary": "The observability services are degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.00032954,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.859936731Z",
                        "name": "ObservabilityServicesDegraded",
                        "query": "sum(ALERTS{alertname=\"MonitoringServiceDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"AlertingServiceDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"LoggingServiceDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"DashboardingServiceDegraded\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T08:11:51.854583701Z",
                                "annotations": {
                                    "children": "AccessServicesDegraded{severity='warning'}, CoreServicesDegraded{severity='warning'}, ObservabilityServicesDegraded{severity='warning'}",
                                    "childrenJsonPath": "$[?((@.labels.alertname === 'AccessServicesDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'CoreServicesDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'ObservabilityServicesDegraded' && @.labels.severity === 'warning'))]",
                                    "summary": "The Platform services are degraded."
                                },
                                "labels": {
                                    "alertname": "PlatformServicesDegraded",
                                    "severity": "warning"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "children": "AccessServicesDegraded{severity='warning'}, CoreServicesDegraded{severity='warning'}, ObservabilityServicesDegraded{severity='warning'}",
                            "childrenJsonPath": "$[?((@.labels.alertname === 'AccessServicesDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'CoreServicesDegraded' && @.labels.severity === 'warning') || (@.labels.alertname === 'ObservabilityServicesDegraded' && @.labels.severity === 'warning'))]",
                            "summary": "The Platform services are degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000323484,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.857729215Z",
                        "name": "PlatformServicesDegraded",
                        "query": "sum(ALERTS{alertname=\"AccessServicesDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"CoreServicesDegraded\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"ObservabilityServicesDegraded\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "NodeFileDescriptorLimit{severity='warning'}, NodeFilesystemAlmostOutOfSpace{severity='warning'}, NodeFilesystemAlmostOutOfFiles{severity='warning'}, NodeFilesystemFilesFillingUp{severity='warning'}, NodeFilesystemSpaceFillingUp{severity='warning'}",
                            "childrenJsonPath": "$[?(((@.labels.alertname === 'NodeFileDescriptorLimit' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeFilesystemAlmostOutOfSpace' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeFilesystemAlmostOutOfFiles' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeFilesystemFilesFillingUp' && @.labels.severity === 'warning') || (@.labels.alertname === 'NodeFilesystemSpaceFillingUp' && @.labels.severity === 'warning')) && (@.labels.mountpoint === '{{ $labels.mountpoint }}' && @.labels.instance === '{{ $labels.instance }}'))]",
                            "summary": "The system partition {{ $labels.mountpoint }} on node {{ $labels.instance }} is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000185642,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.857541266Z",
                        "name": "SystemPartitionDegraded",
                        "query": "sum by (mountpoint, instance) (ALERTS{alertname=\"NodeFileDescriptorLimit\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeFilesystemAlmostOutOfSpace\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeFilesystemAlmostOutOfFiles\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeFilesystemFilesFillingUp\",alertstate=\"firing\",severity=\"warning\"} or ALERTS{alertname=\"NodeFilesystemSpaceFillingUp\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "children": "KubePersistentVolumeFillingUp{severity='warning'}",
                            "childrenJsonPath": "$[?(((@.labels.alertname === 'KubePersistentVolumeFillingUp' && @.labels.severity === 'warning')) && (@.labels.persistentvolumeclaim === '{{ $labels.persistentvolumeclaim }}' && @.labels.namespace === '{{ $labels.namespace }}' && @.labels.instance === '{{ $labels.instance }}'))]",
                            "summary": "The volume {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} on node {{ $labels.instance }} is degraded."
                        },
                        "duration": 60,
                        "evaluationTime": 7.7074e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.862188366Z",
                        "name": "VolumeDegraded",
                        "query": "sum by (persistentvolumeclaim, namespace, instance) (ALERTS{alertname=\"KubePersistentVolumeFillingUp\",alertstate=\"firing\",severity=\"warning\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.00049743,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-metalk8s-kube-apps.rules-107d2e12-a517-4abc-8e56-ba84fcabbf48.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:10.123119624Z",
                "limit": 0,
                "name": "kubernetes-apps",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than 24 hours to complete.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted",
                            "summary": "Job did not complete in time"
                        },
                        "duration": 0,
                        "evaluationTime": 0.000466095,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:10.12314718Z",
                        "name": "KubeJobNotCompleted",
                        "query": "time() - max by (namespace, job_name, cluster) (kube_job_status_start_time{job=\"kube-state-metrics\",namespace=~\".*\"} and kube_job_status_active{job=\"kube-state-metrics\",namespace=~\".*\"} > 0) > (24 * 60 * 60)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.010529004,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-metalk8s-nodes.rules-9db727fb-1b05-43b1-b2ea-00b66b74b261.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:01.50150237Z",
                "limit": 0,
                "name": "node-exporter",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclocknotsynchronising",
                            "summary": "Clock not synchronising."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000167722,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511371905Z",
                        "name": "NodeClockNotSynchronising",
                        "query": "min_over_time(node_timex_sync_status[5m]) == 0 and node_timex_maxerror_seconds >= 16",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "message": "Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeclockskewdetected",
                            "summary": "Clock skew detected."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000200616,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511169048Z",
                        "name": "NodeClockSkewDetected",
                        "query": "(node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefiledescriptorlimit",
                            "summary": "Kernel is predicted to exhaust file descriptors limit soon."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000127943,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.51190134Z",
                        "name": "NodeFileDescriptorLimit",
                        "query": "(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 90)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefiledescriptorlimit",
                            "summary": "Kernel is predicted to exhaust file descriptors limit soon."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00013992,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511757852Z",
                        "name": "NodeFileDescriptorLimit",
                        "query": "(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 70)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles",
                            "summary": "Filesystem has less than 8% inodes left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000806546,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.509309251Z",
                        "name": "NodeFilesystemAlmostOutOfFiles",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 8 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles",
                            "summary": "Filesystem has less than 15% inodes left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000770948,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.508535582Z",
                        "name": "NodeFilesystemAlmostOutOfFiles",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 15 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace",
                            "summary": "Filesystem has less than 12% space left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000771077,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.505347088Z",
                        "name": "NodeFilesystemAlmostOutOfSpace",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 12 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace",
                            "summary": "Filesystem has less than 20% space left."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000754794,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.504589102Z",
                        "name": "NodeFilesystemAlmostOutOfSpace",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup",
                            "summary": "Filesystem is predicted to run out of inodes within the next 4 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001188041,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.507343911Z",
                        "name": "NodeFilesystemFilesFillingUp",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup",
                            "summary": "Filesystem is predicted to run out of inodes within the next 24 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001219513,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.506121424Z",
                        "name": "NodeFilesystemFilesFillingUp",
                        "query": "(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup",
                            "summary": "Filesystem is predicted to run out of space within the next 4 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001223586,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.503360668Z",
                        "name": "NodeFilesystemSpaceFillingUp",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup",
                            "summary": "Filesystem is predicted to run out of space within the next 24 hours."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.001819455,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.501537688Z",
                        "name": "NodeFilesystemSpaceFillingUp",
                        "query": "(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of conntrack entries are used",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodehighnumberconntrackentriesused",
                            "summary": "Number of conntrack are getting close to the limit"
                        },
                        "duration": 0,
                        "evaluationTime": 0.00012268,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511043931Z",
                        "name": "NodeHighNumberConntrackEntriesUsed",
                        "query": "(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs",
                            "summary": "Network interface is reporting many receive errors."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000460286,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.510118968Z",
                        "name": "NodeNetworkReceiveErrs",
                        "query": "increase(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs",
                            "summary": "Network interface is reporting many transmit errors."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000459077,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.510582057Z",
                        "name": "NodeNetworkTransmitErrs",
                        "query": "increase(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to 1 or more disks failures. Number of spare drives is insufficient to fix issue automatically.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-noderaiddegraded",
                            "summary": "RAID Array is degraded"
                        },
                        "duration": 900,
                        "evaluationTime": 8.5929e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511612938Z",
                        "name": "NodeRAIDDegraded",
                        "query": "node_md_disks_required - ignoring (state) (node_md_disks{state=\"active\"}) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "At least 1 device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-noderaiddiskfailure",
                            "summary": "Failed device in RAID array"
                        },
                        "duration": 0,
                        "evaluationTime": 5.453e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511701238Z",
                        "name": "NodeRAIDDiskFailure",
                        "query": "node_md_disks{state=\"failed\"} >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Node Exporter text file collector failed to scrape.",
                            "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodetextfilecollectorscrapeerror",
                            "summary": "Node Exporter text file collector failed to scrape."
                        },
                        "duration": 0,
                        "evaluationTime": 6.8425e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:01.511542162Z",
                        "name": "NodeTextFileCollectorScrapeError",
                        "query": "node_textfile_scrape_error{job=\"node-exporter\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.004557619,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-alertmanager.rules-7c620f65-6314-493c-a490-49c6a5d611d2.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:52.09448314Z",
                "limit": 0,
                "name": "alertmanager.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping",
                            "summary": "Half or more of the Alertmanager instances within the same cluster are crashlooping."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000312785,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.098725482Z",
                        "name": "AlertmanagerClusterCrashlooping",
                        "query": "(count by (namespace, service, cluster) (changes(process_start_time_seconds{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[10m]) > 4) / count by (namespace, service, cluster) (up{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"})) >= 0.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown",
                            "summary": "Half or more of the Alertmanager instances within the same cluster are down."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000301092,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.098420402Z",
                        "name": "AlertmanagerClusterDown",
                        "query": "(count by (namespace, service, cluster) (avg_over_time(up{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]) < 0.5) / count by (namespace, service, cluster) (up{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"})) >= 0.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts",
                            "summary": "All Alertmanager instances in a cluster failed to send notifications to a critical integration."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000745529,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.095876062Z",
                        "name": "AlertmanagerClusterFailedToSendAlerts",
                        "query": "min by (namespace, service, integration) (rate(alertmanager_notifications_failed_total{integration=~\".*\",job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]) / ignoring (reason) group_left () rate(alertmanager_notifications_total{integration=~\".*\",job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m])) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts",
                            "summary": "All Alertmanager instances in a cluster failed to send notifications to a non-critical integration."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000949338,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.096624679Z",
                        "name": "AlertmanagerClusterFailedToSendAlerts",
                        "query": "min by (namespace, service, integration) (rate(alertmanager_notifications_failed_total{integration!~\".*\",job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]) / ignoring (reason) group_left () rate(alertmanager_notifications_total{integration!~\".*\",job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m])) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Alertmanager instances within the {{$labels.job}} cluster have different configurations.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent",
                            "summary": "Alertmanager instances within the same cluster have different configurations."
                        },
                        "duration": 1200,
                        "evaluationTime": 0.000840755,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.097576611Z",
                        "name": "AlertmanagerConfigInconsistent",
                        "query": "count by (namespace, service, cluster) (count_values by (namespace, service, cluster) (\"config_hash\", alertmanager_config_hash{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"})) != 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload",
                            "summary": "Reloading an Alertmanager configuration has failed."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000421647,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.094505966Z",
                        "name": "AlertmanagerFailedReload",
                        "query": "max_over_time(alertmanager_config_last_reload_successful{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts",
                            "summary": "An Alertmanager instance failed to send notifications."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000697579,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.095175617Z",
                        "name": "AlertmanagerFailedToSendAlerts",
                        "query": "(rate(alertmanager_notifications_failed_total{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]) / ignoring (reason) group_left () rate(alertmanager_notifications_total{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m])) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent",
                            "summary": "A member of an Alertmanager cluster has not found all other cluster members."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000241586,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:52.094931302Z",
                        "name": "AlertmanagerMembersInconsistent",
                        "query": "max_over_time(alertmanager_cluster_members{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]) < on (namespace, service, cluster) group_left () count by (namespace, service, cluster) (max_over_time(alertmanager_cluster_members{job=\"prometheus-operator-alertmanager\",namespace=\"metalk8s-monitoring\"}[5m]))",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001390729,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-config-reloaders-72b78155-56ae-4018-866c-b3a33f360af3.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:41.633939902Z",
                "limit": 0,
                "name": "config-reloaders",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.\nAs a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors",
                            "summary": "config-reloader sidecar has not had a successful reload for 10m"
                        },
                        "duration": 600,
                        "evaluationTime": 0.001361414,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:41.633965388Z",
                        "name": "ConfigReloaderSidecarErrors",
                        "query": "max_over_time(reloader_last_reload_successful{namespace=~\".+\"}[5m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.012387348,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-etcd-2d3a31e6-f542-4120-acea-e8842b49cf6c.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:51.155551499Z",
                "limit": 0,
                "name": "etcd",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.",
                            "summary": "etcd cluster database is running full."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000208779,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.167537953Z",
                        "name": "etcdDatabaseQuotaLowSpace",
                        "query": "(last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~\".*etcd.*\"}[5m]) / last_over_time(etcd_server_quota_backend_bytes{job=~\".*etcd.*\"}[5m])) * 100 > 95",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive.",
                            "summary": "etcd cluster database growing very fast."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000187005,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.167749203Z",
                        "name": "etcdExcessiveDatabaseGrowth",
                        "query": "predict_linear(etcd_mvcc_db_total_size_in_bytes{job=~\".*etcd.*\"}[4h], 4 * 60 * 60) > etcd_server_quota_backend_bytes{job=~\".*etcd.*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": 99th percentile of gRPC requests is {{ $value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method }} method.",
                            "summary": "etcd grpc requests are slow"
                        },
                        "duration": 600,
                        "evaluationTime": 0.000177083,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.166371455Z",
                        "name": "etcdGRPCRequestsSlow",
                        "query": "histogram_quantile(0.99, sum without (grpc_type) (rate(grpc_server_handling_seconds_bucket{grpc_method!=\"Defragment\",grpc_type=\"unary\",job=~\".*etcd.*\"}[5m]))) > 0.15",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster 99th percentile commit durations are too high."
                        },
                        "duration": 1800,
                        "evaluationTime": 0.00024926,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.1672857Z",
                        "name": "etcdHighCommitDurations",
                        "query": "histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.25",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster 99th percentile fsync durations are too high."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000242945,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.167039867Z",
                        "name": "etcdHighFsyncDurations",
                        "query": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster 99th percentile fsync durations are too high."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000237573,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.166799771Z",
                        "name": "etcdHighFsyncDurations",
                        "query": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster has high number of failed grpc requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.004508928,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.161859308Z",
                        "name": "etcdHighNumberOfFailedGRPCRequests",
                        "query": "100 * sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=~\".*etcd.*\"}[5m])) / sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{job=~\".*etcd.*\"}[5m])) > 5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster has high number of failed grpc requests."
                        },
                        "duration": 600,
                        "evaluationTime": 0.004184277,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.157672238Z",
                        "name": "etcdHighNumberOfFailedGRPCRequests",
                        "query": "100 * sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=~\".*etcd.*\"}[5m])) / sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{job=~\".*etcd.*\"}[5m])) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster has high number of proposal failures."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000135004,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.166662296Z",
                        "name": "etcdHighNumberOfFailedProposals",
                        "query": "rate(etcd_server_proposals_failed_total{job=~\".*etcd.*\"}[15m]) > 5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": {{ $value }} leader changes within the last 15 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.",
                            "summary": "etcd cluster has high number of leader changes."
                        },
                        "duration": 300,
                        "evaluationTime": 0.001049861,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.156618953Z",
                        "name": "etcdHighNumberOfLeaderChanges",
                        "query": "increase((max without (instance, pod) (etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}) or 0 * absent(etcd_server_leader_changes_seen_total{job=~\".*etcd.*\"}))[15m:1m]) >= 4",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": insufficient members ({{ $value }}).",
                            "summary": "etcd cluster has insufficient number of members."
                        },
                        "duration": 180,
                        "evaluationTime": 0.000235757,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.156256115Z",
                        "name": "etcdInsufficientMembers",
                        "query": "sum without (instance, pod) (up{job=~\".*etcd.*\"} == bool 1) < ((count without (instance, pod) (up{job=~\".*etcd.*\"}) + 1) / 2)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.",
                            "summary": "etcd cluster member communication is slow."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000108971,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.166551005Z",
                        "name": "etcdMemberCommunicationSlow",
                        "query": "histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.15",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": members are down ({{ $value }}).",
                            "summary": "etcd cluster members are down."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000677377,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.155575294Z",
                        "name": "etcdMembersDown",
                        "query": "max without (endpoint) (sum without (instance, pod) (up{job=~\".*etcd.*\"} == bool 0) or count without (To) (sum without (instance, pod) (rate(etcd_network_peer_sent_failures_total{job=~\".*etcd.*\"}[2m])) > 0.01)) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "etcd cluster \"{{ $labels.job }}\": member {{ $labels.instance }} has no leader.",
                            "summary": "etcd cluster has no leader."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000121258,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.156495094Z",
                        "name": "etcdNoLeader",
                        "query": "etcd_server_has_leader{job=~\".*etcd.*\"} == 0",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001320506,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-general.rules-f39cb691-81b9-4ac3-9c87-c6bae9d83699.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:51.490425945Z",
                "limit": 0,
                "name": "general.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "This is an alert that is used to inhibit info alerts.\nBy themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with\nother alerts.\nThis alert fires whenever there's a severity=\"info\" alert, and stops firing when another alert with a\nseverity of 'warning' or 'critical' starts firing on the same namespace.\nThis alert should be routed to a null receiver and configured to inhibit alerts with severity=\"info\".",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor",
                            "summary": "Info-level alert inhibition."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000216738,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "none"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.49152653Z",
                        "name": "InfoInhibitor",
                        "query": "ALERTS{severity=\"info\"} == 1 unless on (namespace) ALERTS{alertname!=\"InfoInhibitor\",alertstate=\"firing\",severity=~\"warning|critical\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/targetdown",
                            "summary": "One or more targets are unreachable."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000753916,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.490449051Z",
                        "name": "TargetDown",
                        "query": "100 * (count by (cluster, job, namespace, service) (up == 0) / count by (cluster, job, namespace, service) (up)) > 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T07:57:21.484745363Z",
                                "annotations": {
                                    "description": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.",
                                    "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/watchdog",
                                    "summary": "An alert that should always be firing to certify that Alertmanager is working properly."
                                },
                                "labels": {
                                    "alertname": "Watchdog",
                                    "severity": "none"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "firing",
                                "value": "1e+00"
                            }
                        ],
                        "annotations": {
                            "description": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/watchdog",
                            "summary": "An alert that should always be firing to certify that Alertmanager is working properly."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000316568,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "none"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.491206376Z",
                        "name": "Watchdog",
                        "query": "vector(1)",
                        "state": "firing",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001979288,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.container-cpu-usage-seconds-total-83d07503-3650-4a60-9815-31e602826517.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:45.717261006Z",
                "limit": 0,
                "name": "k8s.rules.container_cpu_usage_seconds_total",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.001957569,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:45.717278442Z",
                        "name": "node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate",
                        "query": "sum by (cluster, namespace, pod, container) (irate(container_cpu_usage_seconds_total{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.00191853,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.container-memory-cache-ccbd4796-0877-4672-9678-66411e7fd50f.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:45.51032783Z",
                "limit": 0,
                "name": "k8s.rules.container_memory_cache",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.00189206,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:45.51035086Z",
                        "name": "node_namespace_pod_container:container_memory_cache",
                        "query": "container_memory_cache{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001786816,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.container-memory-rss-ca02a78e-4ab3-4f5e-a2e3-2863281565b3.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:07.480686027Z",
                "limit": 0,
                "name": "k8s.rules.container_memory_rss",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.001740157,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:07.480728732Z",
                        "name": "node_namespace_pod_container:container_memory_rss",
                        "query": "container_memory_rss{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.000742722,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.container-memory-swap-6ebcbb55-c6c4-472c-8c80-434ac7db5386.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:49.390893365Z",
                "limit": 0,
                "name": "k8s.rules.container_memory_swap",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000716671,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.390915771Z",
                        "name": "node_namespace_pod_container:container_memory_swap",
                        "query": "container_memory_swap{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001780551,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.container-memory-working-set-byte-d69b4047-dd94-458a-8a29-f3a001123f2b.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:57.427809803Z",
                "limit": 0,
                "name": "k8s.rules.container_memory_working_set_bytes",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.001756287,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:57.427828896Z",
                        "name": "node_namespace_pod_container:container_memory_working_set_bytes",
                        "query": "container_memory_working_set_bytes{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.005146018,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.container-resource-f5a67d43-6c51-48c7-9490-f6143c98eb16.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:09.318496115Z",
                "limit": 0,
                "name": "k8s.rules.container_resource",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000475357,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.322588763Z",
                        "name": "cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits",
                        "query": "kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.0006374,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.320119339Z",
                        "name": "cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests",
                        "query": "kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000583885,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.321381351Z",
                        "name": "cluster:namespace:pod_memory:active:kube_pod_container_resource_limits",
                        "query": "kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000922598,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.318513764Z",
                        "name": "cluster:namespace:pod_memory:active:kube_pod_container_resource_requests",
                        "query": "kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000572282,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.323067245Z",
                        "name": "namespace_cpu:kube_pod_container_resource_limits:sum",
                        "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00061831,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.32076024Z",
                        "name": "namespace_cpu:kube_pod_container_resource_requests:sum",
                        "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000616992,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.321968508Z",
                        "name": "namespace_memory:kube_pod_container_resource_limits:sum",
                        "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000675152,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:09.319441088Z",
                        "name": "namespace_memory:kube_pod_container_resource_requests:sum",
                        "query": "sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001911955,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-k8s.rules.pod-owner-2f1e118c-7bad-410d-8863-0e44e6d34120.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:54.772859402Z",
                "limit": 0,
                "name": "k8s.rules.pod_owner",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.00028165,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "workload_type": "daemonset"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.774173129Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"DaemonSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001285887,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "workload_type": "deployment"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.77288225Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicaSet\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on (replicaset, namespace) group_left (owner_name) topk by (replicaset, namespace) (1, max by (replicaset, namespace, owner_name) (kube_replicaset_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000137952,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "workload_type": "job"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.77463078Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"Job\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000169397,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "workload_type": "statefulset"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.774458376Z",
                        "name": "namespace_workload_pod:kube_pod_owner:relabel",
                        "query": "max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"StatefulSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.049139379,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver-availability.rules-89f9bb9e-6c8d-4264-ae18-61c2e7e6f898.yaml",
                "interval": 180,
                "lastEvaluation": "2024-10-18T08:22:50.587269897Z",
                "limit": 0,
                "name": "kube-apiserver-availability.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000823375,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "all"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.6304919Z",
                        "name": "apiserver_request:availability30d",
                        "query": "1 - ((sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"POST|PUT|PATCH|DELETE\"}) - sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"})) + (sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"LIST|GET\"}) - ((sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"1\",scope=~\"resource|\",verb=~\"LIST|GET\"}) or vector(0)) + sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"5\",scope=\"namespace\",verb=~\"LIST|GET\"}) + sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}))) + sum by (cluster) (code:apiserver_request_total:increase30d{code=~\"5..\"} or vector(0))) / sum by (cluster) (code:apiserver_request_total:increase30d)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00055562,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.631318548Z",
                        "name": "apiserver_request:availability30d",
                        "query": "1 - (sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"LIST|GET\"}) - ((sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"1\",scope=~\"resource|\",verb=~\"LIST|GET\"}) or vector(0)) + sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"5\",scope=\"namespace\",verb=~\"LIST|GET\"}) + sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"})) + sum by (cluster) (code:apiserver_request_total:increase30d{code=~\"5..\",verb=\"read\"} or vector(0))) / sum by (cluster) (code:apiserver_request_total:increase30d{verb=\"read\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000309861,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.631877534Z",
                        "name": "apiserver_request:availability30d",
                        "query": "1 - ((sum by (cluster) (cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d{verb=~\"POST|PUT|PATCH|DELETE\"}) - sum by (cluster) (cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"})) + sum by (cluster) (code:apiserver_request_total:increase30d{code=~\"5..\",verb=\"write\"} or vector(0))) / sum by (cluster) (code:apiserver_request_total:increase30d{verb=\"write\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.002457629,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.588499637Z",
                        "name": "cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase1h",
                        "query": "sum by (cluster, verb, scope) (increase(apiserver_request_sli_duration_seconds_count{job=\"apiserver\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00026604,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.590961413Z",
                        "name": "cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase30d",
                        "query": "sum by (cluster, verb, scope) (avg_over_time(cluster_verb_scope:apiserver_request_sli_duration_seconds_count:increase1h[30d]) * 24 * 30)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.037552125,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.59123106Z",
                        "name": "cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h",
                        "query": "sum by (cluster, verb, scope, le) (increase(apiserver_request_sli_duration_seconds_bucket[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00169556,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.628791678Z",
                        "name": "cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d",
                        "query": "sum by (cluster, verb, scope, le) (avg_over_time(cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase1h[30d]) * 24 * 30)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000159406,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.587930069Z",
                        "name": "code:apiserver_request_total:increase30d",
                        "query": "sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~\"LIST|GET\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000399432,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.588095654Z",
                        "name": "code:apiserver_request_total:increase30d",
                        "query": "sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~\"POST|PUT|PATCH|DELETE\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001284834,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.63219071Z",
                        "name": "code_resource:apiserver_request_total:rate5m",
                        "query": "sum by (cluster, code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000620043,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.633481085Z",
                        "name": "code_resource:apiserver_request_total:rate5m",
                        "query": "sum by (cluster, code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001683739,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.634104889Z",
                        "name": "code_verb:apiserver_request_total:increase1h",
                        "query": "sum by (cluster, code, verb) (increase(apiserver_request_total{code=~\"2..\",job=\"apiserver\",verb=~\"LIST|GET|POST|PUT|PATCH|DELETE\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000170008,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.635792967Z",
                        "name": "code_verb:apiserver_request_total:increase1h",
                        "query": "sum by (cluster, code, verb) (increase(apiserver_request_total{code=~\"3..\",job=\"apiserver\",verb=~\"LIST|GET|POST|PUT|PATCH|DELETE\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000249519,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.635966348Z",
                        "name": "code_verb:apiserver_request_total:increase1h",
                        "query": "sum by (cluster, code, verb) (increase(apiserver_request_total{code=~\"4..\",job=\"apiserver\",verb=~\"LIST|GET|POST|PUT|PATCH|DELETE\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000187044,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.636219161Z",
                        "name": "code_verb:apiserver_request_total:increase1h",
                        "query": "sum by (cluster, code, verb) (increase(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET|POST|PUT|PATCH|DELETE\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000616206,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:22:50.587309143Z",
                        "name": "code_verb:apiserver_request_total:increase30d",
                        "query": "avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 * 30",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.045159523,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver-burnrate.rules-e551e027-dca0-48f6-aceb-9eb64bba49ab.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:02.692693257Z",
                "limit": 0,
                "name": "kube-apiserver-burnrate.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.006131488,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.692712748Z",
                        "name": "apiserver_request:burnrate1d",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1d])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1d])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1d])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1d])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.002062048,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.725656076Z",
                        "name": "apiserver_request:burnrate1d",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004751817,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.698849878Z",
                        "name": "apiserver_request:burnrate1h",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1h])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1h])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1h])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[1h])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001775223,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.727730601Z",
                        "name": "apiserver_request:burnrate1h",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004665209,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.703605934Z",
                        "name": "apiserver_request:burnrate2h",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[2h])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[2h])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[2h])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[2h])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001805372,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.729509728Z",
                        "name": "apiserver_request:burnrate2h",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004567755,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.708275467Z",
                        "name": "apiserver_request:burnrate30m",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[30m])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[30m])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[30m])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[30m])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001812958,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.731318826Z",
                        "name": "apiserver_request:burnrate30m",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004584077,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.712846998Z",
                        "name": "apiserver_request:burnrate3d",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[3d])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[3d])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[3d])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[3d])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001745876,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.733137494Z",
                        "name": "apiserver_request:burnrate3d",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.003514227,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.717436904Z",
                        "name": "apiserver_request:burnrate5m",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[5m])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[5m])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[5m])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[5m])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001222955,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.73488706Z",
                        "name": "apiserver_request:burnrate5m",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.004696855,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.720955145Z",
                        "name": "apiserver_request:burnrate6h",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[6h])) - ((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=~\"resource|\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[6h])) or vector(0)) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[6h])) + sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[6h])))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001735603,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:24:02.73611418Z",
                        "name": "apiserver_request:burnrate6h",
                        "query": "((sum by (cluster) (rate(apiserver_request_sli_duration_seconds_count{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h])) - sum by (cluster) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",le=\"1\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) + sum by (cluster) (rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) / sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.036553436,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver-histogram.rules-4e37ca35-4e65-4331-83e3-9119aac5f4ae.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:43.237956839Z",
                "limit": 0,
                "name": "kube-apiserver-histogram.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.029282051,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.99",
                            "verb": "read"
                        },
                        "lastEvaluation": "2024-10-18T08:23:43.237979701Z",
                        "name": "cluster_quantile:apiserver_request_sli_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"LIST|GET\"}[5m]))) > 0",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.007230553,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.99",
                            "verb": "write"
                        },
                        "lastEvaluation": "2024-10-18T08:23:43.267269783Z",
                        "name": "cluster_quantile:apiserver_request_sli_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_sli_duration_seconds_bucket{job=\"apiserver\",subresource!~\"proxy|attach|log|exec|portforward\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) > 0",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001371029,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-apiserver-slos-38fe0be1-0bdc-49de-86ce-26e6e42856e0.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:56.067657019Z",
                "limit": 0,
                "name": "kube-apiserver-slos",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000201398,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "long": "1d",
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning",
                            "short": "2h"
                        },
                        "lastEvaluation": "2024-10-18T08:23:56.068377514Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum by (cluster) (apiserver_request:burnrate1d) > (3 * 0.01) and on (cluster) sum by (cluster) (apiserver_request:burnrate2h) > (3 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 120,
                        "evaluationTime": 0.000503346,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "long": "1h",
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical",
                            "short": "5m"
                        },
                        "lastEvaluation": "2024-10-18T08:23:56.067679513Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum by (cluster) (apiserver_request:burnrate1h) > (14.4 * 0.01) and on (cluster) sum by (cluster) (apiserver_request:burnrate5m) > (14.4 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T08:14:26.064299581Z",
                                "annotations": {
                                    "description": "The API server is burning too much error budget.",
                                    "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn",
                                    "summary": "The API server is burning too much error budget."
                                },
                                "labels": {
                                    "alertname": "KubeAPIErrorBudgetBurn",
                                    "long": "3d",
                                    "severity": "warning",
                                    "short": "6h"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "pending",
                                "value": "1.0374487706025396e-02"
                            }
                        ],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 10800,
                        "evaluationTime": 0.000436584,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "long": "3d",
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning",
                            "short": "6h"
                        },
                        "lastEvaluation": "2024-10-18T08:23:56.068588627Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum by (cluster) (apiserver_request:burnrate3d) > (1 * 0.01) and on (cluster) sum by (cluster) (apiserver_request:burnrate6h) > (1 * 0.01)",
                        "state": "pending",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The API server is burning too much error budget.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn",
                            "summary": "The API server is burning too much error budget."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000187472,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "long": "6h",
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical",
                            "short": "30m"
                        },
                        "lastEvaluation": "2024-10-18T08:23:56.068187245Z",
                        "name": "KubeAPIErrorBudgetBurn",
                        "query": "sum by (cluster) (apiserver_request:burnrate6h) > (6 * 0.01) and on (cluster) sum by (cluster) (apiserver_request:burnrate30m) > (6 * 0.01)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000928194,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-prometheus-general.rules-c772f89a-284b-40ca-aba2-5275a3b3e978.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:55.219213215Z",
                "limit": 0,
                "name": "kube-prometheus-general.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000183226,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.219955324Z",
                        "name": "count:up0",
                        "query": "count without (instance, pod, node) (up == 0)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000712542,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.219237519Z",
                        "name": "count:up1",
                        "query": "count without (instance, pod, node) (up == 1)",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.002307146,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-prometheus-node-recording.rules-9a841095-c91a-495d-bfaf-af585a254e11.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:42.06683374Z",
                "limit": 0,
                "name": "kube-prometheus-node-recording.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000270401,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:42.068866248Z",
                        "name": "cluster:node_cpu:ratio",
                        "query": "cluster:node_cpu:sum_rate5m / count(sum by (instance, cpu) (node_cpu_seconds_total))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000203352,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:42.068659326Z",
                        "name": "cluster:node_cpu:sum_rate5m",
                        "query": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000633551,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:42.06685411Z",
                        "name": "instance:node_cpu:rate:sum",
                        "query": "sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[3m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.0005712,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:42.068082619Z",
                        "name": "instance:node_cpu:ratio",
                        "query": "sum without (cpu, mode) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])) / on (instance) group_left () count by (instance) (sum by (instance, cpu) (node_cpu_seconds_total))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000297383,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:42.067492641Z",
                        "name": "instance:node_network_receive_bytes:rate:sum",
                        "query": "sum by (instance) (rate(node_network_receive_bytes_total[3m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000286022,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:42.067793571Z",
                        "name": "instance:node_network_transmit_bytes:rate:sum",
                        "query": "sum by (instance) (rate(node_network_transmit_bytes_total[3m]))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001644756,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-scheduler.rules-3df20a65-d050-4fb4-a894-673f4b3938ae.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:55.000664248Z",
                "limit": 0,
                "name": "kube-scheduler.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 9.9907e-05,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.002206483Z",
                        "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without (instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 8.6592e-05,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.0017799Z",
                        "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without (instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.00013823,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.001316449Z",
                        "name": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without (instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 8.9823e-05,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.001869052Z",
                        "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without (instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 9.2385e-05,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.001457357Z",
                        "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without (instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000286727,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.000684816Z",
                        "name": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without (instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000237045,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.001961671Z",
                        "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum without (instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000223978,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.00155259Z",
                        "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum without (instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000337335,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2024-10-18T08:23:55.000974755Z",
                        "name": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum without (instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"kube-scheduler\"}[5m])))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.001143765,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kube-state-metrics-28d3ec90-09ec-4f32-8ca6-6a1fc7dce772.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:48.482799968Z",
                "limit": 0,
                "name": "kube-state-metrics",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors",
                            "summary": "kube-state-metrics is experiencing errors in list operations."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000528819,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:48.482825518Z",
                        "name": "KubeStateMetricsListErrors",
                        "query": "(sum by (cluster) (rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum by (cluster) (rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch",
                            "summary": "kube-state-metrics sharding is misconfigured."
                        },
                        "duration": 900,
                        "evaluationTime": 7.6164e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:48.483572256Z",
                        "name": "KubeStateMetricsShardingMismatch",
                        "query": "stdvar by (cluster) (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) != 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing",
                            "summary": "kube-state-metrics shards are missing."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000290598,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:48.483650814Z",
                        "name": "KubeStateMetricsShardsMissing",
                        "query": "2 ^ max by (cluster) (kube_state_metrics_total_shards{job=\"kube-state-metrics\"}) - 1 - sum by (cluster) (2 ^ max by (cluster, shard_ordinal) (kube_state_metrics_shard_ordinal{job=\"kube-state-metrics\"})) != 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors",
                            "summary": "kube-state-metrics is experiencing errors in watch operations."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000211088,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:48.483357926Z",
                        "name": "KubeStateMetricsWatchErrors",
                        "query": "(sum by (cluster) (rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum by (cluster) (rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001343397,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubelet.rules-2b796694-7746-4efe-8eba-eb6e98b4617e.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:58.815518619Z",
                "limit": 0,
                "name": "kubelet.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000289597,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.5"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.816569282Z",
                        "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.5, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000355529,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.9"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.816209191Z",
                        "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.9, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000664242,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "quantile": "0.99"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.815538844Z",
                        "name": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile",
                        "query": "histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.005714779,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-apps-151d109c-c771-4efb-96cf-4b08417a1d4d.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:54.332447549Z",
                "limit": 0,
                "name": "kubernetes-apps",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting",
                            "summary": "Pod container waiting longer than 1 hour"
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000132827,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.337325485Z",
                        "name": "KubeContainerWaiting",
                        "query": "sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\".*\"}) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled",
                            "summary": "DaemonSet pods are misscheduled."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000106191,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.337636388Z",
                        "name": "KubeDaemonSetMisScheduled",
                        "query": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled",
                            "summary": "DaemonSet pods are not scheduled."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000166326,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.337467419Z",
                        "name": "KubeDaemonSetNotScheduled",
                        "query": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck",
                            "summary": "DaemonSet rollout is stuck."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000720748,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.336601182Z",
                        "name": "KubeDaemonSetRolloutStuck",
                        "query": "((kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}) or (kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != 0) or (kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}) or (kube_daemonset_status_number_available{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"})) and (changes(kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch",
                            "summary": "Deployment generation mismatch due to possible roll-back"
                        },
                        "duration": 900,
                        "evaluationTime": 0.000414466,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.33414938Z",
                        "name": "KubeDeploymentGenerationMismatch",
                        "query": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\".*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch",
                            "summary": "Deployment has not matched the expected number of replicas."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000649358,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.334579198Z",
                        "name": "KubeDeploymentReplicasMismatch",
                        "query": "(kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} > kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\".*\"}) and (changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[10m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck",
                            "summary": "Deployment rollout is not progressing."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000314069,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.335232852Z",
                        "name": "KubeDeploymentRolloutStuck",
                        "query": "kube_deployment_status_condition{condition=\"Progressing\",job=\"kube-state-metrics\",namespace=~\".*\",status=\"false\"} != 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout",
                            "summary": "HPA is running at max replicas"
                        },
                        "duration": 900,
                        "evaluationTime": 8.2919e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.33807679Z",
                        "name": "KubeHpaMaxedOut",
                        "query": "kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} == kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch",
                            "summary": "HPA has not matched desired number of replicas."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000258659,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.33781511Z",
                        "name": "KubeHpaReplicasMismatch",
                        "query": "(kube_horizontalpodautoscaler_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} > kube_horizontalpodautoscaler_spec_min_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} < kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and changes(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}[15m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed",
                            "summary": "Job failed to complete."
                        },
                        "duration": 900,
                        "evaluationTime": 6.734e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.337745335Z",
                        "name": "KubeJobFailed",
                        "query": "kube_job_failed{job=\"kube-state-metrics\",namespace=~\".*\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: \"CrashLoopBackOff\").",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping",
                            "summary": "Pod is crash looping."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000460043,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.332517905Z",
                        "name": "KubePodCrashLooping",
                        "query": "max_over_time(kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\".*\",reason=\"CrashLoopBackOff\"}[5m]) >= 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready",
                            "summary": "Pod has been in a non-ready state for more than 15 minutes."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00116535,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.332980974Z",
                        "name": "KubePodNotReady",
                        "query": "sum by (namespace, pod, cluster) (max by (namespace, pod, cluster) (kube_pod_status_phase{job=\"kube-state-metrics\",namespace=~\".*\",phase=~\"Pending|Unknown|Failed\"}) * on (namespace, pod, cluster) group_left (owner_kind) topk by (namespace, pod, cluster) (1, max by (namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!=\"Job\"}))) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch",
                            "summary": "StatefulSet generation mismatch due to possible roll-back"
                        },
                        "duration": 900,
                        "evaluationTime": 0.000179259,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.335912313Z",
                        "name": "KubeStatefulSetGenerationMismatch",
                        "query": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\".*\"}",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch",
                            "summary": "StatefulSet has not matched the expected number of replicas."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000357392,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.335551396Z",
                        "name": "KubeStatefulSetReplicasMismatch",
                        "query": "(kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\",namespace=~\".*\"}) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[10m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout",
                            "summary": "StatefulSet update has not been rolled out."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000500079,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:54.336095741Z",
                        "name": "KubeStatefulSetUpdateNotRolledOut",
                        "query": "(max by (namespace, statefulset, job, cluster) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\".*\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\".*\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\".*\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"})) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\".*\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002444651,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-resources-7e83c040-90e0-42da-80f4-dab277391622.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:49.615064877Z",
                "limit": 0,
                "name": "kubernetes-resources",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh",
                            "summary": "Processes experience elevated CPU throttling."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000384236,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "info"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.617122531Z",
                        "name": "CPUThrottlingHigh",
                        "query": "sum by (cluster, container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total{container!=\"\"}[5m])) / sum by (cluster, container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m])) > (25 / 100)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Cluster {{ $labels.cluster }} has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit",
                            "summary": "Cluster has overcommitted CPU resource requests."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000733189,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.615085897Z",
                        "name": "KubeCPUOvercommit",
                        "query": "sum by (cluster) (namespace_cpu:kube_pod_container_resource_requests:sum) - (sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) - max by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"})) > 0 and (sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) - max by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"})) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Cluster {{ $labels.cluster }}  has overcommitted CPU resource requests for Namespaces.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit",
                            "summary": "Cluster has overcommitted CPU resource requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000326255,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.61615852Z",
                        "name": "KubeCPUQuotaOvercommit",
                        "query": "sum by (cluster) (min without (resource) (kube_resourcequota{job=\"kube-state-metrics\",resource=~\"(cpu|requests.cpu)\",type=\"hard\"})) / sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) > 1.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit",
                            "summary": "Cluster has overcommitted memory resource requests."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000333668,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.615822257Z",
                        "name": "KubeMemoryOvercommit",
                        "query": "sum by (cluster) (namespace_memory:kube_pod_container_resource_requests:sum) - (sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"}) - max by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"})) > 0 and (sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"}) - max by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"})) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Cluster {{ $labels.cluster }}  has overcommitted memory resource requests for Namespaces.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit",
                            "summary": "Cluster has overcommitted memory resource requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000229767,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.616488817Z",
                        "name": "KubeMemoryQuotaOvercommit",
                        "query": "sum by (cluster) (min without (resource) (kube_resourcequota{job=\"kube-state-metrics\",resource=~\"(memory|requests.memory)\",type=\"hard\"})) / sum by (cluster) (kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"}) > 1.5",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull",
                            "summary": "Namespace quota is going to be full."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000155821,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "info"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.61672136Z",
                        "name": "KubeQuotaAlmostFull",
                        "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) > 0.9 < 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded",
                            "summary": "Namespace quota has exceeded the limits."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000124886,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.61699428Z",
                        "name": "KubeQuotaExceeded",
                        "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused",
                            "summary": "Namespace quota is fully used."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000111343,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "info"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.61687969Z",
                        "name": "KubeQuotaFullyUsed",
                        "query": "kube_resourcequota{job=\"kube-state-metrics\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",type=\"hard\"} > 0) == 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002189794,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-storage-b4ca43aa-582a-412c-9e94-423318a05f5f.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:58.950138136Z",
                "limit": 0,
                "name": "kubernetes-storage",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors",
                            "summary": "PersistentVolume is having issues with provisioning."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000110268,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.952215146Z",
                        "name": "KubePersistentVolumeErrors",
                        "query": "kube_persistentvolume_status_phase{job=\"kube-state-metrics\",phase=~\"Failed|Pending\"} > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup",
                            "summary": "PersistentVolume is filling up."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000686486,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.950157215Z",
                        "name": "KubePersistentVolumeFillingUp",
                        "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.03 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup",
                            "summary": "PersistentVolume is filling up."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000577149,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.950847472Z",
                        "name": "KubePersistentVolumeFillingUp",
                        "query": "(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.15 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}[6h], 4 * 24 * 3600) < 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup",
                            "summary": "PersistentVolumeInodes are filling up."
                        },
                        "duration": 60,
                        "evaluationTime": 0.000336702,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.951428334Z",
                        "name": "KubePersistentVolumeInodesFillingUp",
                        "query": "(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.03 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup",
                            "summary": "PersistentVolumeInodes are filling up."
                        },
                        "duration": 3600,
                        "evaluationTime": 0.000443287,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:58.951768829Z",
                        "name": "KubePersistentVolumeInodesFillingUp",
                        "query": "(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}) < 0.15 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"} > 0 and predict_linear(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\".*\"}[6h], 4 * 24 * 3600) < 0 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\"} == 1 unless on (cluster, namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.00088687,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-28fb41ba-c2eb-46cd-82aa-73152fbc2a40.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:07.56905518Z",
                "limit": 0,
                "name": "kubernetes-system",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors",
                            "summary": "Kubernetes API server client is experiencing errors."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000302017,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:07.569636356Z",
                        "name": "KubeClientErrors",
                        "query": "(sum by (cluster, instance, job, namespace) (rate(rest_client_requests_total{code=~\"5..\",job=\"apiserver\"}[5m])) / sum by (cluster, instance, job, namespace) (rate(rest_client_requests_total{job=\"apiserver\"}[5m]))) > 0.01",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "There are {{ $value }} different semantic versions of Kubernetes components running.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch",
                            "summary": "Different semantic versions of Kubernetes components running."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000558301,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:07.569074388Z",
                        "name": "KubeVersionMismatch",
                        "query": "count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~\"kube-dns|coredns\"}, \"git_version\", \"$1\", \"git_version\", \"(v[0-9]*.[0-9]*).*\"))) > 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.003696973,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-apiserver-b6bd4d53-96db-4aa1-bfe9-ac54c7bebc64.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:04.543009479Z",
                "limit": 0,
                "name": "kubernetes-system-apiserver",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeAPI has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000137343,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:04.544519337Z",
                        "name": "KubeAPIDown",
                        "query": "absent(up{job=\"apiserver\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests",
                            "summary": "The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests."
                        },
                        "duration": 300,
                        "evaluationTime": 0.002044271,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:04.544658982Z",
                        "name": "KubeAPITerminatedRequests",
                        "query": "sum by (cluster) (rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m])) / (sum by (cluster) (rate(apiserver_request_total{job=\"apiserver\"}[10m])) + sum by (cluster) (rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m]))) > 0.2",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown",
                            "summary": "Kubernetes aggregated API is down."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000397333,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:04.5441196Z",
                        "name": "KubeAggregatedAPIDown",
                        "query": "(1 - max by (name, namespace, cluster) (avg_over_time(aggregator_unavailable_apiservice{job=\"apiserver\"}[10m]))) * 100 < 85",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors",
                            "summary": "Kubernetes aggregated API has reported errors."
                        },
                        "duration": 0,
                        "evaluationTime": 0.000116154,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:04.544000944Z",
                        "name": "KubeAggregatedAPIErrors",
                        "query": "sum by (name, namespace, cluster) (increase(aggregator_unavailable_apiservice_total{job=\"apiserver\"}[10m])) > 4",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration",
                            "summary": "Client certificate is about to expire."
                        },
                        "duration": 300,
                        "evaluationTime": 0.00027824,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:24:04.543720213Z",
                        "name": "KubeClientCertificateExpiration",
                        "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on (cluster, job) histogram_quantile(0.01, sum by (cluster, job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 86400",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration",
                            "summary": "Client certificate is about to expire."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000683044,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:04.543033375Z",
                        "name": "KubeClientCertificateExpiration",
                        "query": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0 and on (cluster, job) histogram_quantile(0.01, sum by (cluster, job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 604800",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000422722,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-controller-manager-53945a3c-3e15-451a-954e-addc200ab1e5.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:46.59720358Z",
                "limit": 0,
                "name": "kubernetes-system-controller-manager",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeControllerManager has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000399095,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:46.597219519Z",
                        "name": "KubeControllerManagerDown",
                        "query": "absent(up{job=\"kube-controller-manager\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000457283,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-kube-proxy-f3b38537-7b85-467f-8050-2619f0095edf.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:57.219298874Z",
                "limit": 0,
                "name": "kubernetes-system-kube-proxy",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeProxy has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeproxydown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000431099,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:57.219322011Z",
                        "name": "KubeProxyDown",
                        "query": "absent(up{job=\"kube-proxy\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002951714,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-kubelet-fd9792ce-614a-4d6e-bd1a-e95b9fd0b9f0.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:47.870565289Z",
                "limit": 0,
                "name": "kubernetes-system-kubelet",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.node }} has been unready for more than 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready",
                            "summary": "Node is not ready."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000350464,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.870604068Z",
                        "name": "KubeNodeNotReady",
                        "query": "kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping",
                            "summary": "Node readiness status is flapping."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000159928,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.872174396Z",
                        "name": "KubeNodeReadinessFlapping",
                        "query": "sum by (cluster, node) (changes(kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"}[15m])) > 2",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $labels.node }} is unreachable and some workloads may be rescheduled.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable",
                            "summary": "Node is unreachable."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000320085,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.870958012Z",
                        "name": "KubeNodeUnreachable",
                        "query": "(kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} unless ignoring (key, value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration",
                            "summary": "Kubelet client certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 7.3963e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.873061211Z",
                        "name": "KubeletClientCertificateExpiration",
                        "query": "kubelet_certificate_manager_client_ttl_seconds < 86400",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration",
                            "summary": "Kubelet client certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 7.8709e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.872979994Z",
                        "name": "KubeletClientCertificateExpiration",
                        "query": "kubelet_certificate_manager_client_ttl_seconds < 604800",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors",
                            "summary": "Kubelet has failed to renew its client certificate."
                        },
                        "duration": 900,
                        "evaluationTime": 7.6493e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.873260814Z",
                        "name": "KubeletClientCertificateRenewalErrors",
                        "query": "increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 8.1594e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.873432863Z",
                        "name": "KubeletDown",
                        "query": "absent(up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh",
                            "summary": "Kubelet Pod Lifecycle Event Generator is taking too long to relist."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000127795,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.872337396Z",
                        "name": "KubeletPlegDurationHigh",
                        "query": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh",
                            "summary": "Kubelet Pod startup latency is too high."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000507699,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.872469339Z",
                        "name": "KubeletPodStartUpLatencyHigh",
                        "query": "histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m]))) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"} > 60",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration",
                            "summary": "Kubelet server certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 5.5351e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.8732033Z",
                        "name": "KubeletServerCertificateExpiration",
                        "query": "kubelet_certificate_manager_server_ttl_seconds < 86400",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $value | humanizeDuration }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration",
                            "summary": "Kubelet server certificate is about to expire."
                        },
                        "duration": 0,
                        "evaluationTime": 6.1868e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.873137869Z",
                        "name": "KubeletServerCertificateExpiration",
                        "query": "kubelet_certificate_manager_server_ttl_seconds < 604800",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors",
                            "summary": "Kubelet has failed to renew its server certificate."
                        },
                        "duration": 900,
                        "evaluationTime": 9.097e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.873339615Z",
                        "name": "KubeletServerCertificateRenewalErrors",
                        "query": "increase(kubelet_server_expiration_renew_errors[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods",
                            "summary": "Kubelet is running at capacity."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000889752,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "info"
                        },
                        "lastEvaluation": "2024-10-18T08:23:47.871281136Z",
                        "name": "KubeletTooManyPods",
                        "query": "count by (cluster, node) ((kube_pod_status_phase{job=\"kube-state-metrics\",phase=\"Running\"} == 1) * on (instance, pod, namespace, cluster) group_left (node) topk by (instance, pod, namespace, cluster) (1, kube_pod_info{job=\"kube-state-metrics\"})) / max by (cluster, node) (kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\"} != 1) > 0.95",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.000421205,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-kubernetes-system-scheduler-6f8b6834-b6fb-4493-9008-1865cfbb2ef2.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:59.144907277Z",
                "limit": 0,
                "name": "kubernetes-system-scheduler",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "KubeScheduler has disappeared from Prometheus target discovery.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown",
                            "summary": "Target disappeared from Prometheus target discovery."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000374363,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.144949248Z",
                        "name": "KubeSchedulerDown",
                        "query": "absent(up{job=\"kube-scheduler\"} == 1)",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001731952,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node-exporter-84bb637b-8acd-4808-aba3-9f1f59a7b592.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:49.104227233Z",
                "limit": 0,
                "name": "node-exporter",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Bonding interface {{ $labels.master }} on {{ $labels.instance }} is in degraded state due to one or more slave failures.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded",
                            "summary": "Bonding interface is degraded"
                        },
                        "duration": 300,
                        "evaluationTime": 8.8603e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.105867538Z",
                        "name": "NodeBondingDegraded",
                        "query": "(node_bonding_slaves - node_bonding_active) != 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}%.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage",
                            "summary": "High CPU usage."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000596527,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "info"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.104262254Z",
                        "name": "NodeCPUHighUsage",
                        "query": "sum without (mode) (avg without (cpu) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode!=\"idle\"}[2m]))) * 100 > 90",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis symptom might indicate disk saturation.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation",
                            "summary": "Disk IO queue is high."
                        },
                        "duration": 1800,
                        "evaluationTime": 0.000350094,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.105442615Z",
                        "name": "NodeDiskIOSaturation",
                        "query": "rate(node_disk_io_time_weighted_seconds_total{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"}[5m]) > 10",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}%.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization",
                            "summary": "Host is running out of memory."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000254057,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.105185478Z",
                        "name": "NodeMemoryHighUtilization",
                        "query": "100 - (node_memory_MemAvailable_bytes{job=\"node-exporter\"} / node_memory_MemTotal_bytes{job=\"node-exporter\"} * 100) > 90",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nPlease check that there is enough memory available at this instance.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults",
                            "summary": "Memory major page faults are occurring at very high rate."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000133807,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.105048693Z",
                        "name": "NodeMemoryMajorPagesFaults",
                        "query": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[5m]) > 500",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis might indicate this instance resources saturation and can cause it becoming unresponsive.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation",
                            "summary": "System saturated, load per core is very high."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000184063,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.104862065Z",
                        "name": "NodeSystemSaturation",
                        "query": "node_load1{job=\"node-exporter\"} / count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"}) > 2",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed",
                            "summary": "Systemd service has entered failed state."
                        },
                        "duration": 300,
                        "evaluationTime": 6.9229e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:49.105796149Z",
                        "name": "NodeSystemdServiceFailed",
                        "query": "node_systemd_unit_state{job=\"node-exporter\",state=\"failed\"} == 1",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002992421,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node-exporter.rules-6023a84e-d20b-48de-baaa-21ea75517808.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:51.344899333Z",
                "limit": 0,
                "name": "node-exporter.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000319342,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.345299213Z",
                        "name": "instance:node_cpu_utilisation:rate5m",
                        "query": "1 - avg without (cpu) (sum without (mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode=~\"idle|iowait|steal\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000120258,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.345621993Z",
                        "name": "instance:node_load1_per_cpu:ratio",
                        "query": "(node_load1{job=\"node-exporter\"} / instance:node_num_cpu:sum{job=\"node-exporter\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000375066,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.345745412Z",
                        "name": "instance:node_memory_utilisation:ratio",
                        "query": "1 - ((node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"})) / node_memory_MemTotal_bytes{job=\"node-exporter\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000260361,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.346851354Z",
                        "name": "instance:node_network_receive_bytes_excluding_lo:rate5m",
                        "query": "sum without (device) (rate(node_network_receive_bytes_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000220351,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.347415224Z",
                        "name": "instance:node_network_receive_drop_excluding_lo:rate5m",
                        "query": "sum without (device) (rate(node_network_receive_drop_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000296476,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.347114642Z",
                        "name": "instance:node_network_transmit_bytes_excluding_lo:rate5m",
                        "query": "sum without (device) (rate(node_network_transmit_bytes_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000248875,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.347638813Z",
                        "name": "instance:node_network_transmit_drop_excluding_lo:rate5m",
                        "query": "sum without (device) (rate(node_network_transmit_drop_total{device!=\"lo\",job=\"node-exporter\"}[5m]))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000355834,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.344935998Z",
                        "name": "instance:node_num_cpu:sum",
                        "query": "count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"})",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000115932,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.346124332Z",
                        "name": "instance:node_vmstat_pgmajfault:rate5m",
                        "query": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[5m])",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000335852,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.346243462Z",
                        "name": "instance_device:node_disk_io_time_seconds:rate5m",
                        "query": "rate(node_disk_io_time_seconds_total{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"}[5m])",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000262271,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:23:51.346583138Z",
                        "name": "instance_device:node_disk_io_time_weighted_seconds:rate5m",
                        "query": "rate(node_disk_io_time_weighted_seconds_total{device=~\"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)\",job=\"node-exporter\"}[5m])",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.000527283,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node-network-caa9d14a-9f7d-468b-af8f-c358b11101f8.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:06.342635374Z",
                "limit": 0,
                "name": "node-network",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Network interface \"{{ $labels.device }}\" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping",
                            "summary": "Network interface is often changing its status"
                        },
                        "duration": 120,
                        "evaluationTime": 0.000506556,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:06.342652323Z",
                        "name": "NodeNetworkInterfaceFlapping",
                        "query": "changes(node_network_up{device!~\"veth.+\",job=\"node-exporter\"}[2m]) > 2",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.002107271,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-node.rules-d3f5fe1d-47c3-46e5-8906-bad7131ad346.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:05.207906985Z",
                "limit": 0,
                "name": "node.rules",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "evaluationTime": 0.000299401,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.209365765Z",
                        "name": ":node_memory_MemAvailable_bytes:sum",
                        "query": "sum by (cluster) (node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 8.696e-05,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.209924895Z",
                        "name": "cluster:node_cpu:ratio_rate5m",
                        "query": "avg by (cluster) (node:node_cpu_utilization:ratio_rate5m)",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000253697,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.209668099Z",
                        "name": "node:node_cpu_utilization:ratio_rate5m",
                        "query": "avg by (cluster, node) (sum without (mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.000372514,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.208989754Z",
                        "name": "node:node_num_cpu:sum",
                        "query": "count by (cluster, node) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"} * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, node_namespace_pod:kube_pod_info:))",
                        "type": "recording"
                    },
                    {
                        "evaluationTime": 0.001012263,
                        "health": "ok",
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus"
                        },
                        "lastEvaluation": "2024-10-18T08:24:05.207971933Z",
                        "name": "node_namespace_pod:kube_pod_info:",
                        "query": "topk by (cluster, namespace, pod) (1, max by (cluster, node, namespace, pod) (label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")))",
                        "type": "recording"
                    }
                ]
            },
            {
                "evaluationTime": 0.004926792,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-prometheus-f21dcfb0-2619-41eb-9b84-be5836d0a648.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:23:59.84730581Z",
                "limit": 0,
                "name": "prometheus",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig",
                            "summary": "Failed Prometheus configuration reload."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000391504,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.84732377Z",
                        "name": "PrometheusBadConfig",
                        "query": "max_over_time(prometheus_config_last_reload_successful{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) == 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [
                            {
                                "activeAt": "2024-10-18T07:58:29.846198437Z",
                                "annotations": {
                                    "description": "Prometheus metalk8s-monitoring/prometheus-prometheus-operator-prometheus-0 is dropping 0.03333 samples/s with different values but duplicated timestamp.",
                                    "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps",
                                    "summary": "Prometheus is dropping samples with duplicate timestamps."
                                },
                                "labels": {
                                    "alertname": "PrometheusDuplicateTimestamps",
                                    "container": "prometheus",
                                    "endpoint": "http-web",
                                    "instance": "10.233.132.91:9090",
                                    "job": "prometheus-operator-prometheus",
                                    "namespace": "metalk8s-monitoring",
                                    "pod": "prometheus-prometheus-operator-prometheus-0",
                                    "service": "prometheus-operator-prometheus",
                                    "severity": "warning"
                                },
                                "partialResponseStrategy": "WARN",
                                "state": "firing",
                                "value": "3.333333333333333e-02"
                            }
                        ],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps",
                            "summary": "Prometheus is dropping samples with duplicate timestamps."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000399379,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.84936706Z",
                        "name": "PrometheusDuplicateTimestamps",
                        "query": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "firing",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ printf \"%.1f\" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager",
                            "summary": "Prometheus encounters more than 3% errors sending alerts to any Alertmanager."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000226245,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.852003527Z",
                        "name": "PrometheusErrorSendingAlertsToAnyAlertmanager",
                        "query": "min without (alertmanager) (rate(prometheus_notifications_errors_total{alertmanager!~\"\",job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) / rate(prometheus_notifications_sent_total{alertmanager!~\"\",job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) * 100 > 3",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers",
                            "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000194724,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.848177979Z",
                        "name": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                        "query": "(rate(prometheus_notifications_errors_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) / rate(prometheus_notifications_sent_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) * 100 > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheushighqueryload",
                            "summary": "Prometheus is reaching its maximum capacity serving concurrent requests."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000140712,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.851854852Z",
                        "name": "PrometheusHighQueryLoad",
                        "query": "avg_over_time(prometheus_engine_queries{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0.8",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Kubernetes service discovery of Prometheus {{$labels.namespace}}/{{$labels.pod}} is experiencing {{ printf \"%.0f\" $value }} failures with LIST/WATCH requests to the Kubernetes API in the last 5 minutes.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuskuberneteslistwatchfailures",
                            "summary": "Requests in Kubernetes SD are failing."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000118036,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.847819332Z",
                        "name": "PrometheusKubernetesListWatchFailures",
                        "query": "increase(prometheus_sd_kubernetes_failures_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit",
                            "summary": "Prometheus has dropped targets because some scrape configs have exceeded the labels limit."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000102861,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.851239587Z",
                        "name": "PrometheusLabelLimitHit",
                        "query": "increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations",
                            "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000262651,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.850882384Z",
                        "name": "PrometheusMissingRuleEvaluations",
                        "query": "increase(prometheus_rule_group_iterations_missed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers",
                            "summary": "Prometheus is not connected to any Alertmanagers."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000124583,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.848375162Z",
                        "name": "PrometheusNotConnectedToAlertmanagers",
                        "query": "max_over_time(prometheus_notifications_alertmanagers_discovered{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) < 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples",
                            "summary": "Prometheus is not ingesting samples."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000601377,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.848762624Z",
                        "name": "PrometheusNotIngestingSamples",
                        "query": "(sum without (type) (rate(prometheus_tsdb_head_samples_appended_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) <= 0 and (sum without (scrape_job) (prometheus_target_metadata_cache_entries{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}) > 0 or sum without (rule_group) (prometheus_rule_group_rules{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}) > 0))",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull",
                            "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000234328,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.847940027Z",
                        "name": "PrometheusNotificationQueueRunningFull",
                        "query": "(predict_linear(prometheus_notifications_queue_length{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]))",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps",
                            "summary": "Prometheus drops samples with out-of-order timestamps."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000136801,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.849770547Z",
                        "name": "PrometheusOutOfOrderTimestamps",
                        "query": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures",
                            "summary": "Prometheus fails to send samples to remote storage."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000355497,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.849909886Z",
                        "name": "PrometheusRemoteStorageFailures",
                        "query": "((rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) / ((rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) + (rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) or rate(prometheus_remote_storage_samples_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])))) * 100 > 1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind",
                            "summary": "Prometheus remote write is behind."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000182991,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.85026839Z",
                        "name": "PrometheusRemoteWriteBehind",
                        "query": "(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) - ignoring (remote_name, url) group_right () max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m])) > 120",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}` $labels.instance | query | first | value }}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards",
                            "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000138589,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.850454093Z",
                        "name": "PrometheusRemoteWriteDesiredShards",
                        "query": "(max_over_time(prometheus_remote_storage_shards_desired{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]))",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures",
                            "summary": "Prometheus is failing rule evaluations."
                        },
                        "duration": 900,
                        "evaluationTime": 0.00027994,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.850599796Z",
                        "name": "PrometheusRuleFailures",
                        "query": "increase(prometheus_rule_evaluation_failures_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to refresh SD with mechanism {{$labels.mechanism}}.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheussdrefreshfailure",
                            "summary": "Failed Prometheus SD refresh."
                        },
                        "duration": 1200,
                        "evaluationTime": 9.8013e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.847718974Z",
                        "name": "PrometheusSDRefreshFailure",
                        "query": "increase(prometheus_sd_refresh_failures_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[10m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit",
                            "summary": "Prometheus has dropped some targets that exceeded body size limit."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000101342,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.851345061Z",
                        "name": "PrometheusScrapeBodySizeLimitHit",
                        "query": "increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit",
                            "summary": "Prometheus has failed scrapes that have exceeded the configured sample limit."
                        },
                        "duration": 900,
                        "evaluationTime": 9.7442e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.851448501Z",
                        "name": "PrometheusScrapeSampleLimitHit",
                        "query": "increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing",
                            "summary": "Prometheus has issues compacting blocks."
                        },
                        "duration": 14400,
                        "evaluationTime": 0.000138712,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.848619812Z",
                        "name": "PrometheusTSDBCompactionsFailing",
                        "query": "increase(prometheus_tsdb_compactions_failed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[3h]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing",
                            "summary": "Prometheus has issues reloading blocks from disk."
                        },
                        "duration": 14400,
                        "evaluationTime": 0.000115044,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.848502461Z",
                        "name": "PrometheusTSDBReloadsFailing",
                        "query": "increase(prometheus_tsdb_reloads_failures_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[3h]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because the number of targets exceeded the configured target_limit.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit",
                            "summary": "Prometheus has dropped targets because some scrape configs have exceeded the targets limit."
                        },
                        "duration": 900,
                        "evaluationTime": 8.9715e-05,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.851147696Z",
                        "name": "PrometheusTargetLimitHit",
                        "query": "increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ printf \"%.0f\" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure",
                            "summary": "Prometheus has failed to sync targets."
                        },
                        "duration": 300,
                        "evaluationTime": 0.000303943,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "critical"
                        },
                        "lastEvaluation": "2024-10-18T08:23:59.85154851Z",
                        "name": "PrometheusTargetSyncFailure",
                        "query": "increase(prometheus_target_sync_failed_total{job=\"prometheus-operator-prometheus\",namespace=\"metalk8s-monitoring\"}[30m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            },
            {
                "evaluationTime": 0.001902454,
                "file": "/etc/prometheus/rules/prometheus-prometheus-operator-prometheus-rulefiles-0/metalk8s-monitoring-prometheus-operator-prometheus-operator-4b164bdd-5462-4cf7-9602-d1aff1c7740f.yaml",
                "interval": 30,
                "lastEvaluation": "2024-10-18T08:24:11.013825994Z",
                "limit": 0,
                "name": "prometheus-operator",
                "partialResponseStrategy": "ABORT",
                "rules": [
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors",
                            "summary": "Errors while performing list operations in controller."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000536396,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.013843378Z",
                        "name": "PrometheusOperatorListErrors",
                        "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[10m])) / sum by (cluster, controller, namespace) (rate(prometheus_operator_list_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[10m]))) > 0.4",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors",
                            "summary": "Errors while reconciling Prometheus."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000137694,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.015313312Z",
                        "name": "PrometheusOperatorNodeLookupErrors",
                        "query": "rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]) > 0.1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready",
                            "summary": "Prometheus operator not ready"
                        },
                        "duration": 300,
                        "evaluationTime": 0.000143107,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.015453055Z",
                        "name": "PrometheusOperatorNotReady",
                        "query": "min by (cluster, controller, namespace) (max_over_time(prometheus_operator_ready{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]) == 0)",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors",
                            "summary": "Errors while reconciling objects."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000258991,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.014783921Z",
                        "name": "PrometheusOperatorReconcileErrors",
                        "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) / (sum by (cluster, controller, namespace) (rate(prometheus_operator_reconcile_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) > 0.1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf \"%0.0f\" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources",
                            "summary": "Resources rejected by Prometheus operator"
                        },
                        "duration": 300,
                        "evaluationTime": 0.000127483,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.015598431Z",
                        "name": "PrometheusOperatorRejectedResources",
                        "query": "min_over_time(prometheus_operator_managed_resources{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\",state=\"rejected\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "{{ $value | humanizePercentage }} of status update operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorstatusupdateerrors",
                            "summary": "Errors while updating objects status."
                        },
                        "duration": 600,
                        "evaluationTime": 0.000264482,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.015046415Z",
                        "name": "PrometheusOperatorStatusUpdateErrors",
                        "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_status_update_errors_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) / (sum by (cluster, controller, namespace) (rate(prometheus_operator_status_update_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) > 0.1",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed",
                            "summary": "Last controller reconciliation failed"
                        },
                        "duration": 600,
                        "evaluationTime": 0.000128743,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.014652931Z",
                        "name": "PrometheusOperatorSyncFailed",
                        "query": "min_over_time(prometheus_operator_syncs{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\",status=\"failed\"}[5m]) > 0",
                        "state": "inactive",
                        "type": "alerting"
                    },
                    {
                        "alerts": [],
                        "annotations": {
                            "description": "Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
                            "runbook_url": "https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors",
                            "summary": "Errors while performing watch operations in controller."
                        },
                        "duration": 900,
                        "evaluationTime": 0.000267587,
                        "health": "ok",
                        "keepFiringFor": 0,
                        "labels": {
                            "prometheus": "metalk8s-monitoring/prometheus-operator-prometheus",
                            "severity": "warning"
                        },
                        "lastEvaluation": "2024-10-18T08:24:11.014383166Z",
                        "name": "PrometheusOperatorWatchErrors",
                        "query": "(sum by (cluster, controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m])) / sum by (cluster, controller, namespace) (rate(prometheus_operator_watch_operations_total{job=\"prometheus-operator-operator\",namespace=\"metalk8s-monitoring\"}[5m]))) > 0.4",
                        "state": "inactive",
                        "type": "alerting"
                    }
                ]
            }
        ]
    },
    "status": "success"
}